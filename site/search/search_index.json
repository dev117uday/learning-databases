{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Table of contents SQL Getting Started PgAdmin Tool Database Schema Data Types JSON User Defined Data Types Date/Time/Stamps Sequences Arrays Internal Functions Order of SQL Execution Table ORDER BY and DISTINCT WHERE Clause OPERATORS GROUP BY and HAVING Combining Tables Aggregation Constraints Useful Functions Views Common Table Expression Joins Inner Join Left and Right JOIN Full, Multiple & Self Joins Cross & Natural Joins Indexing SQL Indexes Unique Index GIN Index Custom Indexes Summarization Window Sub Queries Advance Tables Managing Tables Partitioning Tables Internals Pivotal or Cross-tab Tables Functions PL/pgSQL Stored Procedures Triggers More on Triggers Cursors MongoDB MongoDB MQL Mongo Administration MongoDB Aggregation Redis Redis Cassandra Cassandra : Scylla Data Modelling CQL","title":"Table of contents"},{"location":"#table-of-contents","text":"","title":"Table of contents"},{"location":"#sql","text":"Getting Started PgAdmin Tool Database Schema Data Types JSON User Defined Data Types Date/Time/Stamps Sequences Arrays Internal Functions Order of SQL Execution Table ORDER BY and DISTINCT WHERE Clause OPERATORS GROUP BY and HAVING Combining Tables Aggregation Constraints Useful Functions Views Common Table Expression Joins Inner Join Left and Right JOIN Full, Multiple & Self Joins Cross & Natural Joins Indexing SQL Indexes Unique Index GIN Index Custom Indexes Summarization Window Sub Queries Advance Tables Managing Tables Partitioning Tables Internals Pivotal or Cross-tab Tables Functions PL/pgSQL Stored Procedures Triggers More on Triggers Cursors","title":"SQL"},{"location":"#mongodb","text":"MongoDB MQL Mongo Administration MongoDB Aggregation","title":"MongoDB"},{"location":"#redis","text":"Redis","title":"Redis"},{"location":"#cassandra","text":"Cassandra : Scylla Data Modelling CQL","title":"Cassandra"},{"location":"cassandra/cassandra-scylla/","text":"Cassandra : Scylla Setup : Docker sudo docker run --name scyllaTest -d scylladb/scylla sudo docker exec -it scyllaTest nodetool status sudo docker exec -it scyllaTest cqlsh Scylla runs nodes in a hash ring. All nodes are equal: there are no master slave replica sets. Replication Factor The Replication Factor (RF) is equivalent to the number of nodes where data (rows and partitions) are replicated. Data is replicated to multiple (RF=N) nodes. An RF of one means there is only one copy of a row in a cluster, and there is no way to recover the data if the node is compromised or goes down. RF=2 means that there are two copies of a row in a cluster. An RF of at least three is used in most systems Consistency Level The Consistency Level (CL) determines how many replicas in a cluster must acknowledge a read or write operation before it is considered successful. Some of the most common Consistency Levels used are: ANY \u2013 A write must be written to at least one replica in the cluster. A read waits for a response from at least one replica. It provides the highest availability with the lowest consistency. QUORUM \u2013 When a majority of the replicas respond, the request is honored. If RF=3, then 2 replicas respond. QUORUM can be calculated using the formula (n/2 +1) where n is the Replication Factor. ONE \u2013 If one replica responds; the request is honored. LOCAL_ONE \u2013 At least one replica in the local data center responds. LOCAL_QUORUM \u2013 A quorum of replicas in the local datacenter responds. EACH_QUORUM \u2013 (unsupported for reads) \u2013 A quorum of replicas in ALL datacenters must be written to. ALL \u2013 A write must be written to all replicas in the cluster, a read waits for a response from all replicas. Provides the lowest availability with the highest consistency. Sharding in Scylla DB Each Scylla node consists of several independent shards, which contain their share of the node\u2019s total data. Scylla creates a one shard per core (technically, one shard per hyperthread , meaning some physical cores may have two or more virtual cores). Each shard operates on a shared-nothing architecture basis. This means each shard is assigned its RAM and its storage, manages its schedulers for the CPU and I/O, performs its compaction (more about compaction later on), and maintains its multi-queue network connection. Each shard runs as a single thread, and communicates asynchronously with its peers, without locking. Other Important Concepts Partition Key A Partition Key is one or more columns that are responsible for data distribution across the nodes. It determines in which nodes to store a given row. As we will see later on, typically, data is replicated, and copies are stored on multiple nodes. This means that even if one node goes down, the data will still be available. It ensures reliability and fault tolerance Node A Node is a unit of storage in Scylla. It is comprised of the Scylla database server software running on a computer server \u2014 a physical machine \u2014 and all its subsystems (CPUs, memory, storage, network interfaces and so on), or, in a visualized environment, a subset of a server\u2019s resources assigned to a container. Cluster A minimum Cluster typically consists of at least 3 nodes. Data is replicated across the cluster, depending on the Replication Factor Table A Table is how Scylla stores data and can be thought of as a set of rows and columns. Keyspace A Keyspace is a collection of tables with attributes that define how data is replicated on nodes. It defines several options that apply to all the tables it contains, most prominently of which is the replication strategy used by the Keyspace. It is generally encouraged to use one Keyspace per application, and thus a Cluster may define only one Keyspace. CQL A query language for interacting with the Scylla (or Cassandra) database. CQL Shell A command-line interface for interacting with Scylla through the Cassandra Query Language (CQL) Replication The process of replicating data across Nodes in a Cluster. Consistency Level A configurable setting which dictates how many replicas in a Cluster must acknowledge read or write operations. Tunable Consistency The possibility for unique, per-query, Consistency Level settings. These are incremental and override fixed database settings intended to enforce data consistency. Such settings may be set directly from a CQL statement when response speed for a given query or operation is more important. Replication Factor The total number of replica Nodes across a given Cluster. A Replication Factor of 1 means that the data will only exist on a single Node in the Cluster and this setup will not have any fault tolerance. The Replication Factor is set for each Keyspace. All replicas share equal priority; there are no primary or master replicas. CAP Theorem The CAP Theorem is a concept that states that a distributed database system can only have 2 of the 3: Consistency, Availability, and Partition Tolerance. Token Ranges Each node in a ring is assigned a range. The hash function computes a token for a given partition key. The hash function determines the placement of the data in the cluster . Without using Vnodes or virtual nodes, each node could only support one token range. By using vnodes, each node can support multiple, non-contiguous token ranges. By doing this, we can think of each physical node as hosting many virtual nodes. By default, each node has 256 virtual nodes. Gossips Scylla, like Apache Cassandra, uses a type of inter-node communication protocol called Gossip. For nodes to exchange information with each other. Gossip is decentralized, and there is no single point of failure. It\u2019s used for peer node discovery and metadata propagation. Gossip communication occurs periodically. Each node communicates with three other nodes. Eventually (within a few seconds), the information is propagated throughout the cluster . To see whether a node is communicating using Gossip, we use the statusgossip command: Cluster Level Read/Write Interaction So what happens when data is read or written at the cluster level? Note that what happens at the node level will be explained in another lesson. Since each node is equal in Scylla, any node can receive a read/write request. These are the main steps in the process: A client connects to a Scylla node using CQL shell and performs a CQL request The node the client connected to is now designated as the Coordinator Node. The Coordinator Node, based on hashing the data, using the partition key and on the Replication Strategy , sends the request to the applicable nodes. Inter-node messages are sent through a messaging queue in an asynchronous way. The Consistency Level determines the number of nodes the coordinator needs to hear back from, in order for the request to be successful. The client is notified if the request was successful.","title":"Cassandra : Scylla"},{"location":"cassandra/cassandra-scylla/#cassandra-scylla","text":"","title":"Cassandra : Scylla"},{"location":"cassandra/cassandra-scylla/#setup-docker","text":"sudo docker run --name scyllaTest -d scylladb/scylla sudo docker exec -it scyllaTest nodetool status sudo docker exec -it scyllaTest cqlsh Scylla runs nodes in a hash ring. All nodes are equal: there are no master slave replica sets.","title":"Setup : Docker"},{"location":"cassandra/cassandra-scylla/#replication-factor","text":"The Replication Factor (RF) is equivalent to the number of nodes where data (rows and partitions) are replicated. Data is replicated to multiple (RF=N) nodes. An RF of one means there is only one copy of a row in a cluster, and there is no way to recover the data if the node is compromised or goes down. RF=2 means that there are two copies of a row in a cluster. An RF of at least three is used in most systems","title":"Replication Factor"},{"location":"cassandra/cassandra-scylla/#consistency-level","text":"The Consistency Level (CL) determines how many replicas in a cluster must acknowledge a read or write operation before it is considered successful. Some of the most common Consistency Levels used are: ANY \u2013 A write must be written to at least one replica in the cluster. A read waits for a response from at least one replica. It provides the highest availability with the lowest consistency. QUORUM \u2013 When a majority of the replicas respond, the request is honored. If RF=3, then 2 replicas respond. QUORUM can be calculated using the formula (n/2 +1) where n is the Replication Factor. ONE \u2013 If one replica responds; the request is honored. LOCAL_ONE \u2013 At least one replica in the local data center responds. LOCAL_QUORUM \u2013 A quorum of replicas in the local datacenter responds. EACH_QUORUM \u2013 (unsupported for reads) \u2013 A quorum of replicas in ALL datacenters must be written to. ALL \u2013 A write must be written to all replicas in the cluster, a read waits for a response from all replicas. Provides the lowest availability with the highest consistency.","title":"Consistency Level"},{"location":"cassandra/cassandra-scylla/#sharding-in-scylla-db","text":"Each Scylla node consists of several independent shards, which contain their share of the node\u2019s total data. Scylla creates a one shard per core (technically, one shard per hyperthread , meaning some physical cores may have two or more virtual cores). Each shard operates on a shared-nothing architecture basis. This means each shard is assigned its RAM and its storage, manages its schedulers for the CPU and I/O, performs its compaction (more about compaction later on), and maintains its multi-queue network connection. Each shard runs as a single thread, and communicates asynchronously with its peers, without locking.","title":"Sharding in Scylla DB"},{"location":"cassandra/cassandra-scylla/#other-important-concepts","text":"","title":"Other Important Concepts"},{"location":"cassandra/cassandra-scylla/#partition-key","text":"A Partition Key is one or more columns that are responsible for data distribution across the nodes. It determines in which nodes to store a given row. As we will see later on, typically, data is replicated, and copies are stored on multiple nodes. This means that even if one node goes down, the data will still be available. It ensures reliability and fault tolerance","title":"Partition Key"},{"location":"cassandra/cassandra-scylla/#node","text":"A Node is a unit of storage in Scylla. It is comprised of the Scylla database server software running on a computer server \u2014 a physical machine \u2014 and all its subsystems (CPUs, memory, storage, network interfaces and so on), or, in a visualized environment, a subset of a server\u2019s resources assigned to a container.","title":"Node"},{"location":"cassandra/cassandra-scylla/#cluster","text":"A minimum Cluster typically consists of at least 3 nodes. Data is replicated across the cluster, depending on the Replication Factor","title":"Cluster"},{"location":"cassandra/cassandra-scylla/#table","text":"A Table is how Scylla stores data and can be thought of as a set of rows and columns.","title":"Table"},{"location":"cassandra/cassandra-scylla/#keyspace","text":"A Keyspace is a collection of tables with attributes that define how data is replicated on nodes. It defines several options that apply to all the tables it contains, most prominently of which is the replication strategy used by the Keyspace. It is generally encouraged to use one Keyspace per application, and thus a Cluster may define only one Keyspace.","title":"Keyspace"},{"location":"cassandra/cassandra-scylla/#cql","text":"A query language for interacting with the Scylla (or Cassandra) database.","title":"CQL"},{"location":"cassandra/cassandra-scylla/#cql-shell","text":"A command-line interface for interacting with Scylla through the Cassandra Query Language (CQL)","title":"CQL Shell"},{"location":"cassandra/cassandra-scylla/#replication","text":"The process of replicating data across Nodes in a Cluster.","title":"Replication"},{"location":"cassandra/cassandra-scylla/#consistency-level_1","text":"A configurable setting which dictates how many replicas in a Cluster must acknowledge read or write operations.","title":"Consistency Level"},{"location":"cassandra/cassandra-scylla/#tunable-consistency","text":"The possibility for unique, per-query, Consistency Level settings. These are incremental and override fixed database settings intended to enforce data consistency. Such settings may be set directly from a CQL statement when response speed for a given query or operation is more important.","title":"Tunable Consistency"},{"location":"cassandra/cassandra-scylla/#replication-factor_1","text":"The total number of replica Nodes across a given Cluster. A Replication Factor of 1 means that the data will only exist on a single Node in the Cluster and this setup will not have any fault tolerance. The Replication Factor is set for each Keyspace. All replicas share equal priority; there are no primary or master replicas.","title":"Replication Factor"},{"location":"cassandra/cassandra-scylla/#cap-theorem","text":"The CAP Theorem is a concept that states that a distributed database system can only have 2 of the 3: Consistency, Availability, and Partition Tolerance.","title":"CAP Theorem"},{"location":"cassandra/cassandra-scylla/#token-ranges","text":"Each node in a ring is assigned a range. The hash function computes a token for a given partition key. The hash function determines the placement of the data in the cluster . Without using Vnodes or virtual nodes, each node could only support one token range. By using vnodes, each node can support multiple, non-contiguous token ranges. By doing this, we can think of each physical node as hosting many virtual nodes. By default, each node has 256 virtual nodes.","title":"Token Ranges"},{"location":"cassandra/cassandra-scylla/#gossips","text":"Scylla, like Apache Cassandra, uses a type of inter-node communication protocol called Gossip. For nodes to exchange information with each other. Gossip is decentralized, and there is no single point of failure. It\u2019s used for peer node discovery and metadata propagation. Gossip communication occurs periodically. Each node communicates with three other nodes. Eventually (within a few seconds), the information is propagated throughout the cluster . To see whether a node is communicating using Gossip, we use the statusgossip command:","title":"Gossips"},{"location":"cassandra/cassandra-scylla/#cluster-level-readwrite-interaction","text":"So what happens when data is read or written at the cluster level? Note that what happens at the node level will be explained in another lesson. Since each node is equal in Scylla, any node can receive a read/write request. These are the main steps in the process: A client connects to a Scylla node using CQL shell and performs a CQL request The node the client connected to is now designated as the Coordinator Node. The Coordinator Node, based on hashing the data, using the partition key and on the Replication Strategy , sends the request to the applicable nodes. Inter-node messages are sent through a messaging queue in an asynchronous way. The Consistency Level determines the number of nodes the coordinator needs to hear back from, in order for the request to be successful. The client is notified if the request was successful.","title":"Cluster Level Read/Write Interaction"},{"location":"cassandra/cql/","text":"CQL Create KeySpace ( same as db in SQL ) CREATE KEYSPACE mykeyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1}; use mykeyspace; DESCRIBE KEYSPACE mykeyspace; Insert Create Table & Insert Data CREATE TABLE users ( user_id int, fname text, lname text, PRIMARY KEY((user_id))); insert into users(user_id, fname, lname) values (1, 'rick', 'sanchez'); insert into users(user_id, fname, lname) values (4, 'rust', 'cohle'); select * from users;","title":"CQL"},{"location":"cassandra/cql/#cql","text":"","title":"CQL"},{"location":"cassandra/cql/#create-keyspace","text":"( same as db in SQL ) CREATE KEYSPACE mykeyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1}; use mykeyspace; DESCRIBE KEYSPACE mykeyspace;","title":"Create KeySpace"},{"location":"cassandra/cql/#insert-create-table-insert-data","text":"CREATE TABLE users ( user_id int, fname text, lname text, PRIMARY KEY((user_id))); insert into users(user_id, fname, lname) values (1, 'rick', 'sanchez'); insert into users(user_id, fname, lname) values (4, 'rust', 'cohle'); select * from users;","title":"Insert Create Table &amp; Insert Data"},{"location":"cassandra/data-modelling/","text":"Data Modelling In Scylla, as opposed to relational databases, the data model is based around the queries and not just around the domain entities. When creating the data model, we take into account both the conceptual data model and the application workflow: which queries will be performed by which users and how often. Things to keep in mind when design Tables Even data distribution : data should be evenly spread across the cluster so that every node holds roughly the same amount of data. Scylla determines which node should store the data based on hashing the partition key. Therefore, choosing a suitable partition key is crucial. More on this later on. To minimize the number of partitions accessed in a read query : To make reads faster, we\u2019d ideally have all the data required in a read query stored in a single Table . Although it\u2019s fine to duplicate data across tables, in terms of performance, it\u2019s better if the data needed for a read query is in one table.","title":"Data Modelling"},{"location":"cassandra/data-modelling/#data-modelling","text":"In Scylla, as opposed to relational databases, the data model is based around the queries and not just around the domain entities. When creating the data model, we take into account both the conceptual data model and the application workflow: which queries will be performed by which users and how often. Things to keep in mind when design Tables Even data distribution : data should be evenly spread across the cluster so that every node holds roughly the same amount of data. Scylla determines which node should store the data based on hashing the partition key. Therefore, choosing a suitable partition key is crucial. More on this later on. To minimize the number of partitions accessed in a read query : To make reads faster, we\u2019d ideally have all the data required in a read query stored in a single Table . Although it\u2019s fine to duplicate data across tables, in terms of performance, it\u2019s better if the data needed for a read query is in one table.","title":"Data Modelling"},{"location":"mongodb/mongo-administration/","text":"Mongo Administration dbpath The dbpath is the directory where all the data files for your database are stored. The dbpath also contains journaling logs to provide durability in case of a crash. As we saw before, the default dbpath is /data/db; however, you can specify any directory that exists on your machine. The directory must have read/write permissions since database and journaling files will be written to the directory. port The port option allows us to specify the port on which mongod will listen for client connections. If we don't specify a port, it will default to 27017. Database clients should specify the same port to connect to mongod. auth auth enables authentication to control which users can access the database. When auth is specified, all database clients who want to connect to mongod first need to authenticate. Before any database users have been configured, a Mongo shell running on localhost will have access to the database. We can then configure users and their permission levels using the shell. Once one or more users have been configured, the shell will no longer have default access bind_ip The bind_ip option allows us to specify which IP addresses mongod should bind to. When mongod binds to an IP address, clients from that address are able to connect to mongod Sample Config File storage: dbPath: \"/data/db\" systemLog: path: \"/data/log/mongod.log\" destination: \"file\" replication: replSetName: M103 net: bindIp : \"127.0.0.1,192.168.103.100\" tls: mode: \"requireTLS\" certificateKeyFile: \"/etc/tls/tls.pem\" CAFile: \"/etc/tls/TLSCA.pem\" security: keyFile: \"/data/keyfile\" User management commands db.createUser() db.dropUser() # Collection management commands: db.<collection>.renameCollection() db.<collection>.createIndex() db.<collection>.drop() # Database management commands: db.dropDatabase() db.createCollection() # Database status command: db.serverStatus() # Creating index with Database Command: db.runCommand( { \"createIndexes\": <collection> }, { \"indexes\": [ { \"key\": { \"product\": 1 } }, { \"name\": \"name_index\" } ] } ) # Creating index with Shell Helper: db.<collection>.createIndex( { \"product\": 1 }, { \"name\": \"name_index\" } ) # Introspect a Shell Helper: db.<collection>.createIndex File Structure # List --dbpath directory: ls -l /data/db # List diagnostics data directory: ls -l /data/db/diagnostic.data # List journal directory: ls -l /data/db/journal # List socket file: ls /tmp/mongodb-27017.sock Create new user use admin db.createUser({ user: \"root\", pwd: \"root123\", roles : [ \"root\" ] }) Create security officer db.createUser( { user: \"security_officer\", pwd: \"h3ll0th3r3\", roles: [ { db: \"admin\", role: \"userAdmin\" } ] } ) # Create database administrator: db.createUser( { user: \"dba\", pwd: \"c1lynd3rs\", roles: [ { db: \"admin\", role: \"dbAdmin\" } ] } ) # Grant role to user: db.grantRolesToUser( \"dba\", [ { db: \"playground\", role: \"dbOwner\" } ] ) # Show role privileges db.runCommand( { rolesInfo: { role: \"dbOwner\", db: \"playground\" }, showP Basic Replication functions rs.add rs.initiate rs.remove rs.config rs.reconfig rs.isMaster rs.printReplicationInfo rs.status : reports health on replica set nodes uses data from heartbeats rs.isMaster : descibe's a node's role in a replica set rs.printReplicationInfo : only returns oplogs data relative to current node contains timestamps for first and oplog events oplog.rs : central point of replication keeps track of all statements getting replicated its is a capped collection : size of collection is limited by defualt, it takes 5% of the available disk it appends statements ( used in replication ), till the file cap is reached once full, it starts to over write operations from top replication windows is proportional to the system load size of oplog.rs will determine how much time a secondary node has to join in before it thorws itself in recovery mode # Display collections from the local database (this displays more collections from a replica set than from a standalone node): use local show collections # Query the oplog after connected to a replica set: use local db.oplog.rs.find() # Get information about the oplog (remember the oplog is a capped collection). # Store oplog stats as a variable called stats: var stats = db.oplog.rs.stats() # Verify that this collection is capped (it will grow to a pre-configured size before it starts to overwrite the oldest entries with newer ones): stats.capped # Get current size of the oplog: stats.size # Get size limit of the oplog: stats.maxSize # Get current oplog data (including first and last event times, and configured oplog size): rs.printReplicationInfo() Failover : Primary node is the first point of contact from client we first upgrade the secondary nodes then we step down the primary node to become secondary ( using rs.stepDown() ), once the election is complete, we can then safely upgrade the primary (which is now secondary) and connect it back to the replica set Elections : Happens when the primary node becomes unavailable or primary node wants to step down Next primary node will be elected keeping the following things in mind : Which ever node has the latest copy of data, it will run for election and automatically vote for itself If two node ( in a cluster of 3) has same recent data, the third node will cast a vote for any one of them to become primary. This becomes a problem in even node replica set Priority : likelihood that a node will become primary in case of election default priority is 1 priority 1 or higher will give the node a higher chance of winning the election set priority of node to 0 if we dont want node to become primary node that cannot become primary are known as passive nodes write concerns Commands under write concern insert update delete find or modify ACK mechanism added to write ops to provide stronger durability garuntee MAX : majority of nodes : roundup([num of nodes]/2) more durability requires more time to achieve write concerns level 0 : dont wait for ack 1 : wait for primary to ack =2 : wait for primary and one or more secondary to ack \"majority\" : wait for majority to ack write concern options wtimeout : time to wait before marking operation as failed j [true|false] : requires node to commit the write operation to the journal before returning the ack setting write concern higher make ops slower read concern / preference specifies durability during a read operation read concern level local : returns from the primary node available; majority; read preference allows you to redirect read operation to specific members of replica set read preference may return stale data; read preference modes primary : default primaryPreferred : can route read ops to secondary in case primary in not available secondary : routes read ops to sec. nodes only secondaryPreferred : if sec. not available, routes read ops to primary nearest : routes to least network latency from the host, ignores primary or secondary Sharding There is upper limit to vertical scaling Sharding means adding more machines and dividing the dataset into multiple pieces For each shard, we add more replica to make sure we dont lose data a sharded cluster contains a config server, that store the metadata about each shard. This config server are responsible to distributing the queries to the shard containing the data. to make config servers highly available, they are deployed in a replica set configuration We use mongos to route the queries to each shard When to shard a database if not economically viable to scale up the throughput, speed and volume scaling horizontally will add more cost to backup, restore and initial sync time, when not feasible, shard the database Max a server should contain 2tb to 5tb data (factor in CPU and RAM usage) when geographically distributed database is required when single threaded operation needs to be parallelised Sharding Architecture client do not connect to sharded cluster directly they connect to a process called mongos that routes queries to shards How mongos figures out where to route the queries Let's say we have 3 shard containing the data about football players shard 1 : A-J shard 2 : K-Q shard 3 : R-Z When you send a query to find details about player name Messi, it will know which shard contains the data about that player ( as all shards store data about specific player only ). The config server will contain the metadata, for example : shard 1 contains names from A-J, shard 2 contains names from K-Q, shard 3 contains name from R-Z which helps mongos the route the queries There can be one mongos process routing queries to 3 shards or there can be multiple mongos process routing queries to shards and takign request also from multiple clients If size of one shard grows more than other, the data needs to be moved from the bigger shard to the smaller ones to keep consist storage capacity. This update will also be reflected in config server. As not all collections in a database needs to be sharded, there is one shard in the cluster that will act as primary shard keeping all the non sharded collections If we query the database, lets say where age is in 28-30, then mongos will not be able to route the query to specific shard, rather it will send it to all shards to find out the data, then SHARD_MERGE stage takes place. This stage can take place on mongos or a randomly chosen shard in the cluster. The Config database maintained and used internally by mongodb, dont touch it i not necessary Switch to config DB: use config # Query config.databases: db.databases.find().pretty() # Query config.collections: db.collections.find().pretty() # Query config.shards: db.shards.find().pretty() # Query config.chunks: db.chunks.find().pretty() # Query config.mongos: db.mongos.find().pretty() Shard Key It is the indexed field that mongodb uses to partition data in a sharded collection and distribute it across the shards in your cluster You need to create index first before you can select your shard key. MongoDB uses these shard keys to distribute data across sharded clusters. This groupings are also known as chunks shard key should be present in every document in the collection (if not already) or in every new document that is inserted shard keys are immutable, cannot change shard key post-sharding you cannot change the values of shard key fields post-sharding sharded collections are irreversible, you cannot unshard a collection, once sharded. How to shard use sh.enableSharding(\"database\") to enable sharding for the specific database. use db.collections.createIndex() to create the index for your shard key field use sh.shardCollections(\"<database>.<collections>\",{shard_key}) to shard the collection Picking a Good shard key Cardinality High Cardinality = many possible unique shard key values. Low Frequency = low repetition of a given unique shard key value. Avoid shard keys that changes monotonically (keeping incrementing), choosing _id or timestamp or not a great options. Hashed Shard Keys shard key where the underlying index is hashed mongodb uses a hashing function to calculate the hash shard key and then you out where the data is located the data is not changed in the docuement, instead the underlying index backing the shard key itself is hashed As monotonically changind value like _id or timestamp can be hashed, because the output from the hash function can prevent hotspotting this can make data highly distribute, so in case where you need you cannot support geographically isolated read operations using zoned sharding use sh.enableSharding(\"database\") to enable sharding for the specified use db.collection.createIndex({\"field\":\"hashed\"}) to create the index for your shard key field use sh.shardCollection(\"<database>.<collection>\",{ shard_key : \"hashed\" }) to shard the collection Lab shard a collection mongoimport --drop /dataset/products.json --port 26000 -u \"m103-admin\" -p \"m103-pass\" --authenticationDatabase \"admin\" --db m103 --collection products mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin use m103 sh.enableSharding(\"m103\") db.products.createIndex({\"sku\":1}) sh.shardCollection(\"m103.products\",{ \"sku\" : 1 }) Chunks group of documents, who information is store in mongos determining which data belongs to which chunk and which shard contains it re balancing of chunks is preformed by primary of config server replica set Default Chunk Size : 64MB use config db.settings.save({_id: \"chunksize\", value: 2}) Targeted Queries vs Scatter Gather Each Shard contains chunks of sharded data, where each chunk represents a inclusive lower bound and upper bound. The config server replica set keeps maintains the primary record of where all the chunks are present. Mongos keeps a cached copy of the data chunks If the query contains the shard key, then mongos knows where to target the query. This is known as targeted query. targeted query are much faster and always used in query data request preformed by the customer. if the shard key isn't present in the query, them mongos will send the query to all shards and merge back the results from each shard. this is known as scatter gather. scatter gather query sometimes could be extremely slow, hence on admins performing analytics queries should be allowed to run them. In case of composite key example shard key : {\"sku\":1,\"type\":1,\"name\":1} Valid Targeted Queries db.products.find( { \"sku\": ... } ) db.products.find( { \"sku\": ... , \"type\": ... } ) db.products.find( { \"sku\": ... , \"type\": ... , \"name\": ... } ) Scatter Gather db.products.find( { \"type\": ... } ) db.products.find( { \"name\": ... } )","title":"Mongo Administration"},{"location":"mongodb/mongo-administration/#mongo-administration","text":"","title":"Mongo Administration"},{"location":"mongodb/mongo-administration/#dbpath","text":"The dbpath is the directory where all the data files for your database are stored. The dbpath also contains journaling logs to provide durability in case of a crash. As we saw before, the default dbpath is /data/db; however, you can specify any directory that exists on your machine. The directory must have read/write permissions since database and journaling files will be written to the directory.","title":"dbpath"},{"location":"mongodb/mongo-administration/#port","text":"The port option allows us to specify the port on which mongod will listen for client connections. If we don't specify a port, it will default to 27017. Database clients should specify the same port to connect to mongod.","title":"port"},{"location":"mongodb/mongo-administration/#auth","text":"auth enables authentication to control which users can access the database. When auth is specified, all database clients who want to connect to mongod first need to authenticate. Before any database users have been configured, a Mongo shell running on localhost will have access to the database. We can then configure users and their permission levels using the shell. Once one or more users have been configured, the shell will no longer have default access","title":"auth"},{"location":"mongodb/mongo-administration/#bind_ip","text":"The bind_ip option allows us to specify which IP addresses mongod should bind to. When mongod binds to an IP address, clients from that address are able to connect to mongod Sample Config File storage: dbPath: \"/data/db\" systemLog: path: \"/data/log/mongod.log\" destination: \"file\" replication: replSetName: M103 net: bindIp : \"127.0.0.1,192.168.103.100\" tls: mode: \"requireTLS\" certificateKeyFile: \"/etc/tls/tls.pem\" CAFile: \"/etc/tls/TLSCA.pem\" security: keyFile: \"/data/keyfile\"","title":"bind_ip"},{"location":"mongodb/mongo-administration/#user-management-commands","text":"db.createUser() db.dropUser() # Collection management commands: db.<collection>.renameCollection() db.<collection>.createIndex() db.<collection>.drop() # Database management commands: db.dropDatabase() db.createCollection() # Database status command: db.serverStatus() # Creating index with Database Command: db.runCommand( { \"createIndexes\": <collection> }, { \"indexes\": [ { \"key\": { \"product\": 1 } }, { \"name\": \"name_index\" } ] } ) # Creating index with Shell Helper: db.<collection>.createIndex( { \"product\": 1 }, { \"name\": \"name_index\" } ) # Introspect a Shell Helper: db.<collection>.createIndex","title":"User management commands"},{"location":"mongodb/mongo-administration/#file-structure","text":"# List --dbpath directory: ls -l /data/db # List diagnostics data directory: ls -l /data/db/diagnostic.data # List journal directory: ls -l /data/db/journal # List socket file: ls /tmp/mongodb-27017.sock","title":"File Structure"},{"location":"mongodb/mongo-administration/#create-new-user","text":"use admin db.createUser({ user: \"root\", pwd: \"root123\", roles : [ \"root\" ] })","title":"Create new user"},{"location":"mongodb/mongo-administration/#create-security-officer","text":"db.createUser( { user: \"security_officer\", pwd: \"h3ll0th3r3\", roles: [ { db: \"admin\", role: \"userAdmin\" } ] } ) # Create database administrator: db.createUser( { user: \"dba\", pwd: \"c1lynd3rs\", roles: [ { db: \"admin\", role: \"dbAdmin\" } ] } ) # Grant role to user: db.grantRolesToUser( \"dba\", [ { db: \"playground\", role: \"dbOwner\" } ] ) # Show role privileges db.runCommand( { rolesInfo: { role: \"dbOwner\", db: \"playground\" }, showP","title":"Create security officer"},{"location":"mongodb/mongo-administration/#basic-replication-functions","text":"rs.add rs.initiate rs.remove rs.config rs.reconfig rs.isMaster rs.printReplicationInfo rs.status : reports health on replica set nodes uses data from heartbeats rs.isMaster : descibe's a node's role in a replica set rs.printReplicationInfo : only returns oplogs data relative to current node contains timestamps for first and oplog events oplog.rs : central point of replication keeps track of all statements getting replicated its is a capped collection : size of collection is limited by defualt, it takes 5% of the available disk it appends statements ( used in replication ), till the file cap is reached once full, it starts to over write operations from top replication windows is proportional to the system load size of oplog.rs will determine how much time a secondary node has to join in before it thorws itself in recovery mode # Display collections from the local database (this displays more collections from a replica set than from a standalone node): use local show collections # Query the oplog after connected to a replica set: use local db.oplog.rs.find() # Get information about the oplog (remember the oplog is a capped collection). # Store oplog stats as a variable called stats: var stats = db.oplog.rs.stats() # Verify that this collection is capped (it will grow to a pre-configured size before it starts to overwrite the oldest entries with newer ones): stats.capped # Get current size of the oplog: stats.size # Get size limit of the oplog: stats.maxSize # Get current oplog data (including first and last event times, and configured oplog size): rs.printReplicationInfo()","title":"Basic Replication functions"},{"location":"mongodb/mongo-administration/#failover","text":"Primary node is the first point of contact from client we first upgrade the secondary nodes then we step down the primary node to become secondary ( using rs.stepDown() ), once the election is complete, we can then safely upgrade the primary (which is now secondary) and connect it back to the replica set Elections : Happens when the primary node becomes unavailable or primary node wants to step down Next primary node will be elected keeping the following things in mind : Which ever node has the latest copy of data, it will run for election and automatically vote for itself If two node ( in a cluster of 3) has same recent data, the third node will cast a vote for any one of them to become primary. This becomes a problem in even node replica set Priority : likelihood that a node will become primary in case of election default priority is 1 priority 1 or higher will give the node a higher chance of winning the election set priority of node to 0 if we dont want node to become primary node that cannot become primary are known as passive nodes","title":"Failover :"},{"location":"mongodb/mongo-administration/#write-concerns","text":"Commands under write concern insert update delete find or modify ACK mechanism added to write ops to provide stronger durability garuntee MAX : majority of nodes : roundup([num of nodes]/2) more durability requires more time to achieve write concerns level 0 : dont wait for ack 1 : wait for primary to ack =2 : wait for primary and one or more secondary to ack \"majority\" : wait for majority to ack write concern options wtimeout : time to wait before marking operation as failed j [true|false] : requires node to commit the write operation to the journal before returning the ack setting write concern higher make ops slower","title":"write concerns"},{"location":"mongodb/mongo-administration/#read-concern-preference","text":"specifies durability during a read operation read concern level local : returns from the primary node available; majority; read preference allows you to redirect read operation to specific members of replica set read preference may return stale data; read preference modes primary : default primaryPreferred : can route read ops to secondary in case primary in not available secondary : routes read ops to sec. nodes only secondaryPreferred : if sec. not available, routes read ops to primary nearest : routes to least network latency from the host, ignores primary or secondary","title":"read concern / preference"},{"location":"mongodb/mongo-administration/#sharding","text":"There is upper limit to vertical scaling Sharding means adding more machines and dividing the dataset into multiple pieces For each shard, we add more replica to make sure we dont lose data a sharded cluster contains a config server, that store the metadata about each shard. This config server are responsible to distributing the queries to the shard containing the data. to make config servers highly available, they are deployed in a replica set configuration We use mongos to route the queries to each shard","title":"Sharding"},{"location":"mongodb/mongo-administration/#when-to-shard-a-database","text":"if not economically viable to scale up the throughput, speed and volume scaling horizontally will add more cost to backup, restore and initial sync time, when not feasible, shard the database Max a server should contain 2tb to 5tb data (factor in CPU and RAM usage) when geographically distributed database is required when single threaded operation needs to be parallelised","title":"When to shard a database"},{"location":"mongodb/mongo-administration/#sharding-architecture","text":"client do not connect to sharded cluster directly they connect to a process called mongos that routes queries to shards","title":"Sharding Architecture"},{"location":"mongodb/mongo-administration/#how-mongos-figures-out-where-to-route-the-queries","text":"Let's say we have 3 shard containing the data about football players shard 1 : A-J shard 2 : K-Q shard 3 : R-Z When you send a query to find details about player name Messi, it will know which shard contains the data about that player ( as all shards store data about specific player only ). The config server will contain the metadata, for example : shard 1 contains names from A-J, shard 2 contains names from K-Q, shard 3 contains name from R-Z which helps mongos the route the queries There can be one mongos process routing queries to 3 shards or there can be multiple mongos process routing queries to shards and takign request also from multiple clients If size of one shard grows more than other, the data needs to be moved from the bigger shard to the smaller ones to keep consist storage capacity. This update will also be reflected in config server. As not all collections in a database needs to be sharded, there is one shard in the cluster that will act as primary shard keeping all the non sharded collections If we query the database, lets say where age is in 28-30, then mongos will not be able to route the query to specific shard, rather it will send it to all shards to find out the data, then SHARD_MERGE stage takes place. This stage can take place on mongos or a randomly chosen shard in the cluster.","title":"How mongos figures out where to route the queries"},{"location":"mongodb/mongo-administration/#the-config-database","text":"maintained and used internally by mongodb, dont touch it i not necessary Switch to config DB: use config # Query config.databases: db.databases.find().pretty() # Query config.collections: db.collections.find().pretty() # Query config.shards: db.shards.find().pretty() # Query config.chunks: db.chunks.find().pretty() # Query config.mongos: db.mongos.find().pretty()","title":"The Config database"},{"location":"mongodb/mongo-administration/#shard-key","text":"It is the indexed field that mongodb uses to partition data in a sharded collection and distribute it across the shards in your cluster You need to create index first before you can select your shard key. MongoDB uses these shard keys to distribute data across sharded clusters. This groupings are also known as chunks shard key should be present in every document in the collection (if not already) or in every new document that is inserted shard keys are immutable, cannot change shard key post-sharding you cannot change the values of shard key fields post-sharding sharded collections are irreversible, you cannot unshard a collection, once sharded.","title":"Shard Key"},{"location":"mongodb/mongo-administration/#how-to-shard","text":"use sh.enableSharding(\"database\") to enable sharding for the specific database. use db.collections.createIndex() to create the index for your shard key field use sh.shardCollections(\"<database>.<collections>\",{shard_key}) to shard the collection","title":"How to shard"},{"location":"mongodb/mongo-administration/#picking-a-good-shard-key","text":"Cardinality High Cardinality = many possible unique shard key values. Low Frequency = low repetition of a given unique shard key value. Avoid shard keys that changes monotonically (keeping incrementing), choosing _id or timestamp or not a great options.","title":"Picking a Good shard key"},{"location":"mongodb/mongo-administration/#hashed-shard-keys","text":"shard key where the underlying index is hashed mongodb uses a hashing function to calculate the hash shard key and then you out where the data is located the data is not changed in the docuement, instead the underlying index backing the shard key itself is hashed As monotonically changind value like _id or timestamp can be hashed, because the output from the hash function can prevent hotspotting this can make data highly distribute, so in case where you need you cannot support geographically isolated read operations using zoned sharding use sh.enableSharding(\"database\") to enable sharding for the specified use db.collection.createIndex({\"field\":\"hashed\"}) to create the index for your shard key field use sh.shardCollection(\"<database>.<collection>\",{ shard_key : \"hashed\" }) to shard the collection","title":"Hashed Shard Keys"},{"location":"mongodb/mongo-administration/#lab-shard-a-collection","text":"mongoimport --drop /dataset/products.json --port 26000 -u \"m103-admin\" -p \"m103-pass\" --authenticationDatabase \"admin\" --db m103 --collection products mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin use m103 sh.enableSharding(\"m103\") db.products.createIndex({\"sku\":1}) sh.shardCollection(\"m103.products\",{ \"sku\" : 1 })","title":"Lab shard a collection"},{"location":"mongodb/mongo-administration/#chunks","text":"group of documents, who information is store in mongos determining which data belongs to which chunk and which shard contains it re balancing of chunks is preformed by primary of config server replica set Default Chunk Size : 64MB use config db.settings.save({_id: \"chunksize\", value: 2})","title":"Chunks"},{"location":"mongodb/mongo-administration/#targeted-queries-vs-scatter-gather","text":"Each Shard contains chunks of sharded data, where each chunk represents a inclusive lower bound and upper bound. The config server replica set keeps maintains the primary record of where all the chunks are present. Mongos keeps a cached copy of the data chunks If the query contains the shard key, then mongos knows where to target the query. This is known as targeted query. targeted query are much faster and always used in query data request preformed by the customer. if the shard key isn't present in the query, them mongos will send the query to all shards and merge back the results from each shard. this is known as scatter gather. scatter gather query sometimes could be extremely slow, hence on admins performing analytics queries should be allowed to run them.","title":"Targeted Queries vs Scatter Gather"},{"location":"mongodb/mongo-administration/#in-case-of-composite-key","text":"example shard key : {\"sku\":1,\"type\":1,\"name\":1} Valid Targeted Queries db.products.find( { \"sku\": ... } ) db.products.find( { \"sku\": ... , \"type\": ... } ) db.products.find( { \"sku\": ... , \"type\": ... , \"name\": ... } ) Scatter Gather db.products.find( { \"type\": ... } ) db.products.find( { \"name\": ... } )","title":"In case of composite key"},{"location":"mongodb/mongodb-aggregation/","text":"MongoDB Aggregation Aggregate Framework Queries are written inside [] operator, denoting the order in which hey execute $group : An operator that takes in multiple streams of data and distributes it into multiple reservoirs # MQL Query db.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": 1, \"_id\": 0 }).pretty() # MQL Query ith aggregation framework db.listingsAndReviews.aggregate( [ { \"$match\": { \"amenities\": \"Wifi\" } }, { \"$project\": { \"price\": 1, \"address\": 1, \"_id\": 0 }} ]).pretty() # Find one document in the collection # and only include the address field in the resulting cursor. db.listingsAndReviews.findOne({ },{ \"address\": 1, \"_id\": 0 }) # Project only the address field value for each document, # then group all documents into one document per address.country value. db.listingsAndReviews.aggregate( [ { \"$project\": { \"address\": 1, \"_id\": 0 }}, { \"$group\": { \"_id\": \"$address.country\" }} ]) # Project only the address field value for each document, # then group all documents into one document per address.country value, # and count one for each document in each group. db.listingsAndReviews.aggregate( [ { \"$project\": { \"address\": 1, \"_id\": 0 }}, { \"$group\": { \"_id\": \"$address.country\", \"count\": { \"$sum\": 1 } } } ])","title":"MongoDB Aggregation"},{"location":"mongodb/mongodb-aggregation/#mongodb-aggregation","text":"","title":"MongoDB Aggregation"},{"location":"mongodb/mongodb-aggregation/#aggregate-framework","text":"Queries are written inside [] operator, denoting the order in which hey execute $group : An operator that takes in multiple streams of data and distributes it into multiple reservoirs # MQL Query db.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": 1, \"_id\": 0 }).pretty() # MQL Query ith aggregation framework db.listingsAndReviews.aggregate( [ { \"$match\": { \"amenities\": \"Wifi\" } }, { \"$project\": { \"price\": 1, \"address\": 1, \"_id\": 0 }} ]).pretty() # Find one document in the collection # and only include the address field in the resulting cursor. db.listingsAndReviews.findOne({ },{ \"address\": 1, \"_id\": 0 }) # Project only the address field value for each document, # then group all documents into one document per address.country value. db.listingsAndReviews.aggregate( [ { \"$project\": { \"address\": 1, \"_id\": 0 }}, { \"$group\": { \"_id\": \"$address.country\" }} ]) # Project only the address field value for each document, # then group all documents into one document per address.country value, # and count one for each document in each group. db.listingsAndReviews.aggregate( [ { \"$project\": { \"address\": 1, \"_id\": 0 }}, { \"$group\": { \"_id\": \"$address.country\", \"count\": { \"$sum\": 1 } } } ])","title":"Aggregate Framework"},{"location":"mongodb/mongodb/","text":"MongoDB MongoDB native To connect to database use the connect settings in mongodb atlas Starting local database server After installing, you can start the mongod by sudo systemctl start mongod if you receive an error : Failed to start mongod.service: Unit mongod.service not found. Run the following command first: sudo systemctl daemon-reload Verify that MongoDB has started successfully. sudo systemctl status mongod You can optionally ensure that MongoDB will start following a system reboot by issuing the following command: sudo systemctl enable mongod Stop MongoDB. As needed, you can stop the mongod process by issuing the following command: sudo systemctl stop mongod Restart MongoDB. You can restart the mongod process by issuing the following command: sudo systemctl restart mongod MongoDB setup Docker sudo docker run --name mongo --network mongonet -d \\ -p 27017:27017 \\ -e MONGO_INITDB_ROOT_USERNAME=admin \\ -e MONGO_INITDB_ROOT_PASSWORD=pass \\ mongo sudo docker exec -it some-mongo sh Begin using MongoDB. Start a mongo shell on the same host machine as the mongod . You can run the mongo shell without any command-line options to connect to a mongod that is running on your localhost with default port 27017: mongo \"mongodb+srv://<username>:<password>@<url>:<port>/<db>\" To show collections show dbs use <name of db> show collections Basic Commands List all databases : show dbs to switch to db : use <name_of_db> to run a query : db.<name_of_collection>.[function name] to iterate over many results : it add : .pretty() to see json better to find any one document from collection, just use .findOne() to create new collection : db.createCollection(\"employees\") To shutdown db server use admin db.shutdownServer() exit To delete drop collection db.inspection.drop()","title":"MongoDB"},{"location":"mongodb/mongodb/#mongodb","text":"","title":"MongoDB"},{"location":"mongodb/mongodb/#mongodb-native","text":"To connect to database use the connect settings in mongodb atlas Starting local database server After installing, you can start the mongod by sudo systemctl start mongod if you receive an error : Failed to start mongod.service: Unit mongod.service not found. Run the following command first: sudo systemctl daemon-reload Verify that MongoDB has started successfully. sudo systemctl status mongod You can optionally ensure that MongoDB will start following a system reboot by issuing the following command: sudo systemctl enable mongod Stop MongoDB. As needed, you can stop the mongod process by issuing the following command: sudo systemctl stop mongod Restart MongoDB. You can restart the mongod process by issuing the following command: sudo systemctl restart mongod","title":"MongoDB native"},{"location":"mongodb/mongodb/#mongodb-setup-docker","text":"sudo docker run --name mongo --network mongonet -d \\ -p 27017:27017 \\ -e MONGO_INITDB_ROOT_USERNAME=admin \\ -e MONGO_INITDB_ROOT_PASSWORD=pass \\ mongo sudo docker exec -it some-mongo sh Begin using MongoDB. Start a mongo shell on the same host machine as the mongod . You can run the mongo shell without any command-line options to connect to a mongod that is running on your localhost with default port 27017: mongo \"mongodb+srv://<username>:<password>@<url>:<port>/<db>\" To show collections show dbs use <name of db> show collections","title":"MongoDB setup Docker"},{"location":"mongodb/mongodb/#basic-commands","text":"List all databases : show dbs to switch to db : use <name_of_db> to run a query : db.<name_of_collection>.[function name] to iterate over many results : it add : .pretty() to see json better to find any one document from collection, just use .findOne() to create new collection : db.createCollection(\"employees\") To shutdown db server use admin db.shutdownServer() exit","title":"Basic Commands"},{"location":"mongodb/mongodb/#to-delete","text":"drop collection db.inspection.drop()","title":"To delete"},{"location":"mongodb/mql/","text":"MQL MongoDB Data is stored in documents Documents are stored in Collections Document here refers to JSON Redundant copies of data are stored in replica set JSON is stored as BSON internally in MongoDB # FOR BSON, use dump (backup) and restore (restore backup) mongodump --uri \"mongodb+srv://<your username>:<your password>@<your cluster>.mongodb.net/<database name>\" # drop will delete the stuff already in and create the new object from restore mongorestore --uri \"mongodb+srv://<your username>:<your password>@<your cluster>.mongodb.net/<database name>\" --drop dump # FOR JSON, use export (backup) and import (import backup) # collection to specify which collection # out to specify the file name to export to mongoexport --uri=\"mongodb+srv://<your username>:<your password>@<your cluster>.mongodb.net/<database name>\" --collection=sales --out=sales.json mongoimport --uri=\"mongodb+srv://<your username>:<your password>@<your cluster>.mongodb.net/<database name>\" --drop sales.json # to look at all databases available show dbs # to connect to a database use sample_training # to look at collections inside a database show collections Queries Find db.zips.find({\"state\": \"NY\"}) # Use 'it' : iterates through a cursor # cursor : A pointer to a result set of query # pointer : A direct address of memory location db.zips.find({\"state\": \"NY\"}).count() db.zips.find({\"state\": \"NY\", \"city\": \"ALBANY\"}) db.zips.find({\"state\": \"NY\", \"city\": \"ALBANY\"}).pretty() # get random one db.inspections.findOne() Each Document has a unique object _id which is set by default if not specfied Insert db.inspections.insert({ \"_id\" : ObjectId(\"56d61033a378eccde8a8354f\"), \"id\" : \"10021-2015-ENFO\", \"certificate_number\" : 9278806, \"business_name\" : \"ATLIXCO DELI GROCERY INC.\", \"date\" : \"Feb 20 2015\", \"result\" : \"No Violation Issued\", \"sector\" : \"Cigarette Retail Dealer - 127\", \"address\" : { \"city\" : \"RIDGEWOOD\", \"zip\" : 11385, \"street\" : \"MENAHAN ST\", \"number\" : 1712 } }) db.inspections.insert({ \"id\" : \"10021-2015-ENFO\", \"certificate_number\" : 9278806, \"business_name\" : \"ATLIXCO DELI GROCERY INC.\", \"date\" : \"Feb 20 2015\", \"result\" : \"No Violation Issued\", \"sector\" : \"Cigarette Retail Dealer - 127\", \"address\" : { \"city\" : \"RIDGEWOOD\", \"zip\" : 11385, \"street\" : \"MENAHAN ST\", \"number\" : 1712 } }) db.inspections.find( {\"id\" : \"10021-2015-ENFO\", \"certificate_number\" : 9278806} ).pretty() Insert conflicts # Insert three test documents db.inspections.insert([ { \"test\": 1 }, { \"test\": 2 }, { \"test\": 3 } ]) # Insert three test documents but specify the _id values # Error in 2 docs # Insert operation halts when an error is in-countered db.inspections.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 1, \"test\": 2 }, { \"_id\": 3, \"test\": 3 }]) # Find the documents with _id: 1 db.inspections.find({ \"_id\": 1 }) # Insert multiple documents specifying the _id values, # and using the \"ordered\": false option # Ordered False will allow to insert all docs where id doesnt match, # and give errors for those which failed db.inspections.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 1, \"test\": 2 }, { \"_id\": 3, \"test\": 3 }],{ \"ordered\": false }) # Insert multiple documents with _id: 1 with the default \"ordered\": true setting db.inspection.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 3, \"test\": 3 }]) Updates https://docs.mongodb.com/manual/reference/operator/update/#id1 # Find all documents in the zips collection # where the zip field is equal to \"12434\". db.zips.find({ \"zip\": \"12534\" }).pretty() # Find all documents in the zips collection # where the city field is equal to \"HUDSON\". db.zips.find({ \"city\": \"HUDSON\" }).pretty() # Update all documents in the zips collection # where the city field is equal to \"HUDSON\" # by adding 10 to the current value of the \"pop\" field. # Increment Operation db.zips.updateMany({ \"city\": \"HUDSON\" }, { \"$inc\": { \"pop\": 10 } }) # Update a single document in the zips # collection where the zip field is # equal to \"12534\" by setting the value # of the \"pop\" field to 17630. # Update / Set operation db.zips.updateOne({ \"zip\": \"12534\" }, { \"$set\": { \"pop\": 17630 } }) # Update a single document in the zips # collection where the zip field is equal # to \"12534\" by setting the value of # the \"popupation\" field to 17630. # Update / Set operation db.zips.updateOne({ \"zip\": \"12534\" }, { \"$set\": { \"population\": 17630 } }) # Find all documents in the grades # collection where the student_id # field is 151 , and the class_id field is 339. db.grades.find({ \"student_id\": 151, \"class_id\": 339 }).pretty() # Update one document in the grades # collection where the student_id is # `250` *, and the class_id field is 339, # by adding a document element to the \"scores\" array. db.grades.updateOne({ \"student_id\": 250, \"class_id\": 339 }, { \"$push\": { \"scores\": { \"type\": \"extra credit\", \"score\": 100 } } }) db.grades.find({ \"student_id\": 250, \"class_id\": 339 }) Upsert db.iot.updateOne({ \"sensor\": r.sensor, \"date\": r.date, \"valcount\": { \"$lt\": 48 } }, { \"$push\": { \"readings\": { \"v\": r.value, \"t\": r.time } }, \"$inc\": { \"valcount\": 1, \"total\": r.value } }, { \"upsert\": true }) Delete # Look at all the docs that have test field equal to 1. db.inspections.find({ \"test\": 1 }).pretty() # Delete all the documents that have test field equal to 1. db.inspections.deleteMany({ \"test\": 1 }) # Delete one document that has test field equal to 3. db.inspections.deleteOne({ \"test\": 3 }) # Inspect what is left of the inspection collection. db.inspection.find().pretty() # View what collections are present in the sample_training collection. show collections # Drop the inspection collection. db.inspection.drop() Operators # Find all documents where the tripduration # was less than or equal to 70 seconds and # the usertype was not Subscriber: # LESS THAN EQUAL # NOT EQUAL db.trips.find({ \"tripduration\": { \"$lte\" : 70 }, \"usertype\": { \"$ne\": \"Subscriber\" } }).pretty() # Find all documents where the tripduration # was less than or equal to 70 seconds and # the usertype was Customer using a redundant equality operator: # LESS THAN EQUAL # EQUAL db.trips.find({ \"tripduration\": { \"$lte\" : 70 }, \"usertype\": { \"$eq\": \"Customer\" }}).pretty() # Find all documents where the tripduration # was less than or equal to 70 seconds and # the usertype was Customer using the implicit equality operator: # LESS THAN EQUAL db.trips.find({ \"tripduration\": { \"$lte\" : 70 }, \"usertype\": \"Customer\" }).pretty() # Find all documents where airplanes CR2 or A81 # left or landed in the KZN airport: # AND, OR operator db.routes.find({ \"$and\": [ { \"$or\" :[ { \"dst_airport\": \"KZN\" }, { \"src_airport\": \"KZN\" } ] }, { \"$or\" :[ { \"airplane\": \"CR2\" }, { \"airplane\": \"A81\" } ] } ]}).pretty() AND operator is present in your qureies when not specified EXPR # Find all documents where the trip started # and ended at the same station: # here $ denotes the value of the field specified db.trips.find( { \"$expr\": { \"$eq\": [ \"$end station id\", \"$start station id\"] } }).count() # replacing id with name db.trips.find( { \"$expr\": { \"$eq\": [ \"$end station name\", \"$start station name\"]} }).count() # Find all documents where the trip lasted # longer than 1200 seconds, and started # and ended at the same station: db.trips.find({ \"$expr\": { \"$and\": [ { \"$gt\": [ \"$tripduration\", 1200 ]}, { \"$eq\": [ \"$end station id\", \"$start station id\" ]} ]}}).count() Array # using ALL operator db.listingsAndReviews.find( { \"amenities\": { \"$size\": 20, \"$all\": [ \"Internet\", \"Wifi\", \"Kitchen\", \"Heating\", \"Family/kid friendly\", \"Washer\", \"Dryer\", \"Essentials\", \"Shampoo\", \"Hangers\", \"Hair dryer\", \"Iron\", \"Laptop friendly workspace\" ] } }).pretty() Array operators and Projection # Find all documents with exactly 20 amenities which include # all the amenities listed in the query array, # and display their price and address: db.listingsAndReviews.find({ \"amenities\": { \"$size\": 20, \"$all\": [ \"Internet\", \"Wifi\", \"Kitchen\", \"Heating\", \"Family/kid friendly\", \"Washer\", \"Dryer\", \"Essentials\", \"Shampoo\", \"Hangers\", \"Hair dryer\", \"Iron\", \"Laptop friendly workspace\" ] } }, {\"price\": 1, \"address\": 1}).pretty() # Find all documents that have Wifi as one of the amenities # only include price and address in the resulting cursor: db.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": 1, \"_id\": 0 }).pretty() # Find all documents that have Wifi as one of the amenities # only include price and address in the resulting cursor, # also exclude ``\"maximum_nights\"``. **This will be an error:* db.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": 1, \"_id\": 0, \"maximum_nights\":0 }).pretty() # nested projection db.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": { \"country\" : 1 }, \"_id\": 0 }).pretty() # Switch to this database: use sample_training # Get one document from the collection: db.grades.findOne() # Elematch Example # Find all documents where the student in class 431 received # a grade higher than 85 for any type of assignment: db.grades.find({ \"class_id\": 431 }, { \"scores\": { \"$elemMatch\": { \"score\": { \"$gt\": 85 } } } }).pretty() # Find all documents where the student had an extra credit score: db.grades.find({ \"scores\": { \"$elemMatch\": { \"type\": \"extra credit\" } } }).pretty() Elematch : matches documents that contains an array field with at least one element that matches the specified query creteria Elematch : Projects only the array elements with at least one element that matches the specified criteria Array Operators and Sub-Documents use sample_training db.trips.findOne({ \"start station location.type\": \"Point\" }) db.companies.find({ \"relationships.0.person.last_name\": \"Zuckerberg\" }, { \"name\": 1 }).pretty() db.companies.find({ \"relationships.0.person.first_name\": \"Mark\", \"relationships.0.title\": { \"$regex\": \"CEO\" } }, { \"name\": 1 }).count() db.companies.find({ \"relationships.0.person.first_name\": \"Mark\", \"relationships.0.title\": {\"$regex\": \"CEO\" } }, { \"name\": 1 }).pretty() db.companies.find({ \"relationships\": { \"$elemMatch\": { \"is_past\": true, \"person.first_name\": \"Mark\" } } }, { \"name\": 1 }).pretty() db.companies.find({ \"relationships\": { \"$elemMatch\": { \"is_past\": true, \"person.first_name\": \"Mark\" } } }, { \"name\": 1 }).count() Sort and Limit Use sort before limit always db.zips.find().sort({ \"pop\": 1 }).limit(1) db.zips.find({ \"pop\": 0 }).count() db.zips.find().sort({ \"pop\": -1 }).limit(1) db.zips.find().sort({ \"pop\": -1 }).limit(10) db.zips.find().sort({ \"pop\": 1, \"city\": -1 })","title":"MQL"},{"location":"mongodb/mql/#mql","text":"","title":"MQL"},{"location":"mongodb/mql/#mongodb","text":"Data is stored in documents Documents are stored in Collections Document here refers to JSON Redundant copies of data are stored in replica set JSON is stored as BSON internally in MongoDB # FOR BSON, use dump (backup) and restore (restore backup) mongodump --uri \"mongodb+srv://<your username>:<your password>@<your cluster>.mongodb.net/<database name>\" # drop will delete the stuff already in and create the new object from restore mongorestore --uri \"mongodb+srv://<your username>:<your password>@<your cluster>.mongodb.net/<database name>\" --drop dump # FOR JSON, use export (backup) and import (import backup) # collection to specify which collection # out to specify the file name to export to mongoexport --uri=\"mongodb+srv://<your username>:<your password>@<your cluster>.mongodb.net/<database name>\" --collection=sales --out=sales.json mongoimport --uri=\"mongodb+srv://<your username>:<your password>@<your cluster>.mongodb.net/<database name>\" --drop sales.json # to look at all databases available show dbs # to connect to a database use sample_training # to look at collections inside a database show collections","title":"MongoDB"},{"location":"mongodb/mql/#queries","text":"","title":"Queries"},{"location":"mongodb/mql/#find","text":"db.zips.find({\"state\": \"NY\"}) # Use 'it' : iterates through a cursor # cursor : A pointer to a result set of query # pointer : A direct address of memory location db.zips.find({\"state\": \"NY\"}).count() db.zips.find({\"state\": \"NY\", \"city\": \"ALBANY\"}) db.zips.find({\"state\": \"NY\", \"city\": \"ALBANY\"}).pretty() # get random one db.inspections.findOne() Each Document has a unique object _id which is set by default if not specfied","title":"Find"},{"location":"mongodb/mql/#insert","text":"db.inspections.insert({ \"_id\" : ObjectId(\"56d61033a378eccde8a8354f\"), \"id\" : \"10021-2015-ENFO\", \"certificate_number\" : 9278806, \"business_name\" : \"ATLIXCO DELI GROCERY INC.\", \"date\" : \"Feb 20 2015\", \"result\" : \"No Violation Issued\", \"sector\" : \"Cigarette Retail Dealer - 127\", \"address\" : { \"city\" : \"RIDGEWOOD\", \"zip\" : 11385, \"street\" : \"MENAHAN ST\", \"number\" : 1712 } }) db.inspections.insert({ \"id\" : \"10021-2015-ENFO\", \"certificate_number\" : 9278806, \"business_name\" : \"ATLIXCO DELI GROCERY INC.\", \"date\" : \"Feb 20 2015\", \"result\" : \"No Violation Issued\", \"sector\" : \"Cigarette Retail Dealer - 127\", \"address\" : { \"city\" : \"RIDGEWOOD\", \"zip\" : 11385, \"street\" : \"MENAHAN ST\", \"number\" : 1712 } }) db.inspections.find( {\"id\" : \"10021-2015-ENFO\", \"certificate_number\" : 9278806} ).pretty()","title":"Insert"},{"location":"mongodb/mql/#insert-conflicts","text":"# Insert three test documents db.inspections.insert([ { \"test\": 1 }, { \"test\": 2 }, { \"test\": 3 } ]) # Insert three test documents but specify the _id values # Error in 2 docs # Insert operation halts when an error is in-countered db.inspections.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 1, \"test\": 2 }, { \"_id\": 3, \"test\": 3 }]) # Find the documents with _id: 1 db.inspections.find({ \"_id\": 1 }) # Insert multiple documents specifying the _id values, # and using the \"ordered\": false option # Ordered False will allow to insert all docs where id doesnt match, # and give errors for those which failed db.inspections.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 1, \"test\": 2 }, { \"_id\": 3, \"test\": 3 }],{ \"ordered\": false }) # Insert multiple documents with _id: 1 with the default \"ordered\": true setting db.inspection.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 3, \"test\": 3 }])","title":"Insert conflicts"},{"location":"mongodb/mql/#updates","text":"https://docs.mongodb.com/manual/reference/operator/update/#id1 # Find all documents in the zips collection # where the zip field is equal to \"12434\". db.zips.find({ \"zip\": \"12534\" }).pretty() # Find all documents in the zips collection # where the city field is equal to \"HUDSON\". db.zips.find({ \"city\": \"HUDSON\" }).pretty() # Update all documents in the zips collection # where the city field is equal to \"HUDSON\" # by adding 10 to the current value of the \"pop\" field. # Increment Operation db.zips.updateMany({ \"city\": \"HUDSON\" }, { \"$inc\": { \"pop\": 10 } }) # Update a single document in the zips # collection where the zip field is # equal to \"12534\" by setting the value # of the \"pop\" field to 17630. # Update / Set operation db.zips.updateOne({ \"zip\": \"12534\" }, { \"$set\": { \"pop\": 17630 } }) # Update a single document in the zips # collection where the zip field is equal # to \"12534\" by setting the value of # the \"popupation\" field to 17630. # Update / Set operation db.zips.updateOne({ \"zip\": \"12534\" }, { \"$set\": { \"population\": 17630 } }) # Find all documents in the grades # collection where the student_id # field is 151 , and the class_id field is 339. db.grades.find({ \"student_id\": 151, \"class_id\": 339 }).pretty() # Update one document in the grades # collection where the student_id is # `250` *, and the class_id field is 339, # by adding a document element to the \"scores\" array. db.grades.updateOne({ \"student_id\": 250, \"class_id\": 339 }, { \"$push\": { \"scores\": { \"type\": \"extra credit\", \"score\": 100 } } }) db.grades.find({ \"student_id\": 250, \"class_id\": 339 })","title":"Updates"},{"location":"mongodb/mql/#upsert","text":"db.iot.updateOne({ \"sensor\": r.sensor, \"date\": r.date, \"valcount\": { \"$lt\": 48 } }, { \"$push\": { \"readings\": { \"v\": r.value, \"t\": r.time } }, \"$inc\": { \"valcount\": 1, \"total\": r.value } }, { \"upsert\": true })","title":"Upsert"},{"location":"mongodb/mql/#delete","text":"# Look at all the docs that have test field equal to 1. db.inspections.find({ \"test\": 1 }).pretty() # Delete all the documents that have test field equal to 1. db.inspections.deleteMany({ \"test\": 1 }) # Delete one document that has test field equal to 3. db.inspections.deleteOne({ \"test\": 3 }) # Inspect what is left of the inspection collection. db.inspection.find().pretty() # View what collections are present in the sample_training collection. show collections # Drop the inspection collection. db.inspection.drop()","title":"Delete"},{"location":"mongodb/mql/#operators","text":"# Find all documents where the tripduration # was less than or equal to 70 seconds and # the usertype was not Subscriber: # LESS THAN EQUAL # NOT EQUAL db.trips.find({ \"tripduration\": { \"$lte\" : 70 }, \"usertype\": { \"$ne\": \"Subscriber\" } }).pretty() # Find all documents where the tripduration # was less than or equal to 70 seconds and # the usertype was Customer using a redundant equality operator: # LESS THAN EQUAL # EQUAL db.trips.find({ \"tripduration\": { \"$lte\" : 70 }, \"usertype\": { \"$eq\": \"Customer\" }}).pretty() # Find all documents where the tripduration # was less than or equal to 70 seconds and # the usertype was Customer using the implicit equality operator: # LESS THAN EQUAL db.trips.find({ \"tripduration\": { \"$lte\" : 70 }, \"usertype\": \"Customer\" }).pretty() # Find all documents where airplanes CR2 or A81 # left or landed in the KZN airport: # AND, OR operator db.routes.find({ \"$and\": [ { \"$or\" :[ { \"dst_airport\": \"KZN\" }, { \"src_airport\": \"KZN\" } ] }, { \"$or\" :[ { \"airplane\": \"CR2\" }, { \"airplane\": \"A81\" } ] } ]}).pretty() AND operator is present in your qureies when not specified","title":"Operators"},{"location":"mongodb/mql/#expr","text":"# Find all documents where the trip started # and ended at the same station: # here $ denotes the value of the field specified db.trips.find( { \"$expr\": { \"$eq\": [ \"$end station id\", \"$start station id\"] } }).count() # replacing id with name db.trips.find( { \"$expr\": { \"$eq\": [ \"$end station name\", \"$start station name\"]} }).count() # Find all documents where the trip lasted # longer than 1200 seconds, and started # and ended at the same station: db.trips.find({ \"$expr\": { \"$and\": [ { \"$gt\": [ \"$tripduration\", 1200 ]}, { \"$eq\": [ \"$end station id\", \"$start station id\" ]} ]}}).count()","title":"EXPR"},{"location":"mongodb/mql/#array","text":"# using ALL operator db.listingsAndReviews.find( { \"amenities\": { \"$size\": 20, \"$all\": [ \"Internet\", \"Wifi\", \"Kitchen\", \"Heating\", \"Family/kid friendly\", \"Washer\", \"Dryer\", \"Essentials\", \"Shampoo\", \"Hangers\", \"Hair dryer\", \"Iron\", \"Laptop friendly workspace\" ] } }).pretty()","title":"Array"},{"location":"mongodb/mql/#array-operators-and-projection","text":"# Find all documents with exactly 20 amenities which include # all the amenities listed in the query array, # and display their price and address: db.listingsAndReviews.find({ \"amenities\": { \"$size\": 20, \"$all\": [ \"Internet\", \"Wifi\", \"Kitchen\", \"Heating\", \"Family/kid friendly\", \"Washer\", \"Dryer\", \"Essentials\", \"Shampoo\", \"Hangers\", \"Hair dryer\", \"Iron\", \"Laptop friendly workspace\" ] } }, {\"price\": 1, \"address\": 1}).pretty() # Find all documents that have Wifi as one of the amenities # only include price and address in the resulting cursor: db.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": 1, \"_id\": 0 }).pretty() # Find all documents that have Wifi as one of the amenities # only include price and address in the resulting cursor, # also exclude ``\"maximum_nights\"``. **This will be an error:* db.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": 1, \"_id\": 0, \"maximum_nights\":0 }).pretty() # nested projection db.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": { \"country\" : 1 }, \"_id\": 0 }).pretty() # Switch to this database: use sample_training # Get one document from the collection: db.grades.findOne() # Elematch Example # Find all documents where the student in class 431 received # a grade higher than 85 for any type of assignment: db.grades.find({ \"class_id\": 431 }, { \"scores\": { \"$elemMatch\": { \"score\": { \"$gt\": 85 } } } }).pretty() # Find all documents where the student had an extra credit score: db.grades.find({ \"scores\": { \"$elemMatch\": { \"type\": \"extra credit\" } } }).pretty() Elematch : matches documents that contains an array field with at least one element that matches the specified query creteria Elematch : Projects only the array elements with at least one element that matches the specified criteria","title":"Array operators and Projection"},{"location":"mongodb/mql/#array-operators-and-sub-documents","text":"use sample_training db.trips.findOne({ \"start station location.type\": \"Point\" }) db.companies.find({ \"relationships.0.person.last_name\": \"Zuckerberg\" }, { \"name\": 1 }).pretty() db.companies.find({ \"relationships.0.person.first_name\": \"Mark\", \"relationships.0.title\": { \"$regex\": \"CEO\" } }, { \"name\": 1 }).count() db.companies.find({ \"relationships.0.person.first_name\": \"Mark\", \"relationships.0.title\": {\"$regex\": \"CEO\" } }, { \"name\": 1 }).pretty() db.companies.find({ \"relationships\": { \"$elemMatch\": { \"is_past\": true, \"person.first_name\": \"Mark\" } } }, { \"name\": 1 }).pretty() db.companies.find({ \"relationships\": { \"$elemMatch\": { \"is_past\": true, \"person.first_name\": \"Mark\" } } }, { \"name\": 1 }).count()","title":"Array Operators and Sub-Documents"},{"location":"mongodb/mql/#sort-and-limit","text":"Use sort before limit always db.zips.find().sort({ \"pop\": 1 }).limit(1) db.zips.find({ \"pop\": 0 }).count() db.zips.find().sort({ \"pop\": -1 }).limit(1) db.zips.find().sort({ \"pop\": -1 }).limit(10) db.zips.find().sort({ \"pop\": 1, \"city\": -1 })","title":"Sort and Limit"},{"location":"redis/redis/","text":"Redis Setup Docker # pull the image from docker hub $ docker run --name redis-learn -p 6370:6370 -d redis # connect to container and redis shell $ docker exec -it redis-learn redis-cli To benchmark docker exec -it redis-learn redis-benchmark -n 1000 -d 10000 # -d for bytes of data redis-benchmark -n 1000 -d 10000 To set max memory limit > config set maxmemory 128M Set & Get value set key value get key value check whether key exists, returns integer 0 - false | 1 - true > set name \"uday\" OK > get name \"uday\" > exists name (integer) 1 Key related ops Get all keys Delete all keys ( sync | async ) > keys * 1) \"name\" > flushall # options : async|sync OK Set key with expiry time # After 5 seconds, this key will be deleted > set key \"value\" EX [expiry time in seconds] > get key > exists key # expire after X seconds > set key \"value\" > get key > expire key X # to check time remaining > ttl key # Another way, to expire after 10 seconds > setex key 10 \"value\" Delete Key > del name (integer) 1 Set & Get multiple values > mset first \"uday\" last \"yadav\" > mget first last 1) \"uday\" 2) \"yadav\" Miscellaneous > set name \"uday yadav\" # Specifies the range : from 0th char to 3rd char > getrange name 0 3 \"uda\" > strlen name 10 # Append to key > set name \"uday\" > append name \" yadav\" > get name \"uday yadav\" Maths operations > set count 1 > incr count (integer) 2 > incrby count 10 (integer) 12 > decr count (integer) 11 > decrby count 2 (integer) 9 > set pi 3.14 > incrbyfloat pi 0.1 \"3.24\" Lists in Redis # lpush adds value infront > lpush country india > lpush country USA > lpush country UK # get the element at index, start from 0 > lindex country 2 # to add values to the bottom > rpush country Australia # to get all values in list > lrange country 0 -1 1) \"UK\" 2) \"USA\" 3) \"india\" 4) \"Australia\" # to get first 2 values > lrange country 0 1 # get list length > llen country # use lpop and rpop to remove the data from top and bottom # and it returns the element > lpop country > rpop country # to change a key's value > lset country 0 germany # to insert values before and after > linsert country before \"germany\" \"new zealand\" > linsert country after \"germany\" \"UAE\" # use lpushx and rpushx to add key to list only if it exists # else returns 0 Sorting List # Alpha is needed only for strings, nothing for integers > sort country ALPHA > sort country desc ALPHA Sets in Redis > sadd tech golang (integer) 1 # when you do it again > sadd tech golang (integer) 0 > sadd tech postgis python aws > sadd tech1 aws python mysql nodejs # to see all members > smembers tech # to get the length of set > scard tech # to search the set > sismember tech aws 1 # to get the diff between to sets > sdiff tech tech1 # to store the difference btw 2 sets to a new set > sdiffstore tech tech1 [more set] # to check intersection > sinter tech tech1 Sorted Set Redis # add key values > zadd users 10 uday > zadd users 5 uday1 8 uday2 # to get all users > zrange users 0 -1 > zrange users 0 -1 withscores # in reverse order > zrevrange users 0 -1 # to get the length of the string > zcard users 3 # to get key's value over a range > zcount users 0 2 # to remove member > zrem users uday Hashes in Redis # add keys to a set > hset myhash name uday > hset myhash email dev117uday@gmail.com # get all keys from hashset > hkeys myhash # to get all values > hvals myhash # get value > hget myhash name # to check for keys > hexists myhash name # check the length > hlen myhash # to set multiple values at once > hmset myhash country india phone_no 9810039759 age 24 # to get multiple values > hmget myhash country name email # increment some value > hincrby myhash age 2 # to delete key from set > hdel myhash age # to avoid over writting the values > hsetnx myhash name Uday Transaction # to go in transaction mode > multi (TX)> set key1 uday (TX)> set key2 yadav (TX)> set key3 dev117uday (TX)> exec 1) OK 2) OK 3) OK # to discard transaction discard Pub/Sub # To listen to a channel > subscribe my-chat # To publish to a channel > publish my-chat \"hello world\" # to subscribe to channel with patterns in name > psubscribe chats* > psubscribe h?llo If no one is sub to the channel you specify in publish, it returns 0 GeoSpatial Data # add geo spatial data in long : lat > GEOADD maps 77.216721 28.644800 delhi > GEOADD maps 72.877426 19.076090 mumbai # data is tored in sorted set data structure > zrange maps 0 -1 1) \"mumbai\" 2) \"delhi\" # to get the geohash of city > GEOHASH maps delhi 1) \"ttnfvnes010\" # to get long:lat > GEOPOS maps delhi 1) 1) \"77.21672326326370239\" 2) \"28.64479890853065314\" # to get distance, in meter (default) > GEODIST maps delhi mumbai \"1151873.1929\" > GEODIST maps delhi mumbai km \"1151.8732\" # within distance > GEORADIUS maps 77.216721 28.644800 2000 km 1) \"delhi\" 2) \"mumbai > GEORADIUS maps 77.216721 28.644800 2000 km withcoord 1) 1) \"delhi\" 2) 1) \"77.21672326326370239\" 2) \"28.64479890853065314\" 2) 1) \"mumbai\" 2) 1) \"72.87742406129837036\" 2) \"19.07608965708350723\" > GEORADIUS maps 77.216721 28.644800 2000 km withcoord withdist 1) 1) \"delhi\" 2) \"0.0003\" 3) 1) \"77.21672326326370239\" 2) \"28.64479890853065314\" 2) 1) \"mumbai\" 2) \"1151.8732\" 3) 1) \"72.87742406129837036\" 2) \"19.07608965708350723\" > GEORADIUSBYMEMBER maps delhi 1300 km 1) \"delhi\" 2) \"mumbai\" > GEORADIUSBYMEMBER maps delhi 1300 km withcoord withdist desc|asc 1) 1) \"mumbai\" 2) \"1151.8732\" 3) 1) \"72.87742406129837036\" 2) \"19.07608965708350723\" 2) 1) \"delhi\" 2) \"0.0000\" 3) 1) \"77.21672326326370239\" 2) \"28.64479890853065314\"","title":"Redis"},{"location":"redis/redis/#redis","text":"","title":"Redis"},{"location":"redis/redis/#setup","text":"","title":"Setup"},{"location":"redis/redis/#docker","text":"# pull the image from docker hub $ docker run --name redis-learn -p 6370:6370 -d redis # connect to container and redis shell $ docker exec -it redis-learn redis-cli","title":"Docker"},{"location":"redis/redis/#to-benchmark","text":"docker exec -it redis-learn redis-benchmark -n 1000 -d 10000 # -d for bytes of data redis-benchmark -n 1000 -d 10000","title":"To benchmark"},{"location":"redis/redis/#to-set-max-memory-limit","text":"> config set maxmemory 128M","title":"To set max memory limit"},{"location":"redis/redis/#set-get-value","text":"set key value get key value check whether key exists, returns integer 0 - false | 1 - true > set name \"uday\" OK > get name \"uday\" > exists name (integer) 1","title":"Set &amp; Get value"},{"location":"redis/redis/#key-related-ops","text":"Get all keys Delete all keys ( sync | async ) > keys * 1) \"name\" > flushall # options : async|sync OK","title":"Key related ops"},{"location":"redis/redis/#set-key-with-expiry-time","text":"# After 5 seconds, this key will be deleted > set key \"value\" EX [expiry time in seconds] > get key > exists key # expire after X seconds > set key \"value\" > get key > expire key X # to check time remaining > ttl key # Another way, to expire after 10 seconds > setex key 10 \"value\"","title":"Set key with expiry time"},{"location":"redis/redis/#delete-key","text":"> del name (integer) 1","title":"Delete Key"},{"location":"redis/redis/#set-get-multiple-values","text":"> mset first \"uday\" last \"yadav\" > mget first last 1) \"uday\" 2) \"yadav\"","title":"Set &amp; Get multiple values"},{"location":"redis/redis/#miscellaneous","text":"> set name \"uday yadav\" # Specifies the range : from 0th char to 3rd char > getrange name 0 3 \"uda\" > strlen name 10 # Append to key > set name \"uday\" > append name \" yadav\" > get name \"uday yadav\"","title":"Miscellaneous"},{"location":"redis/redis/#maths-operations","text":"> set count 1 > incr count (integer) 2 > incrby count 10 (integer) 12 > decr count (integer) 11 > decrby count 2 (integer) 9 > set pi 3.14 > incrbyfloat pi 0.1 \"3.24\"","title":"Maths operations"},{"location":"redis/redis/#lists-in-redis","text":"# lpush adds value infront > lpush country india > lpush country USA > lpush country UK # get the element at index, start from 0 > lindex country 2 # to add values to the bottom > rpush country Australia # to get all values in list > lrange country 0 -1 1) \"UK\" 2) \"USA\" 3) \"india\" 4) \"Australia\" # to get first 2 values > lrange country 0 1 # get list length > llen country # use lpop and rpop to remove the data from top and bottom # and it returns the element > lpop country > rpop country # to change a key's value > lset country 0 germany # to insert values before and after > linsert country before \"germany\" \"new zealand\" > linsert country after \"germany\" \"UAE\" # use lpushx and rpushx to add key to list only if it exists # else returns 0","title":"Lists in Redis"},{"location":"redis/redis/#sorting-list","text":"# Alpha is needed only for strings, nothing for integers > sort country ALPHA > sort country desc ALPHA","title":"Sorting List"},{"location":"redis/redis/#sets-in-redis","text":"> sadd tech golang (integer) 1 # when you do it again > sadd tech golang (integer) 0 > sadd tech postgis python aws > sadd tech1 aws python mysql nodejs # to see all members > smembers tech # to get the length of set > scard tech # to search the set > sismember tech aws 1 # to get the diff between to sets > sdiff tech tech1 # to store the difference btw 2 sets to a new set > sdiffstore tech tech1 [more set] # to check intersection > sinter tech tech1","title":"Sets in Redis"},{"location":"redis/redis/#sorted-set-redis","text":"# add key values > zadd users 10 uday > zadd users 5 uday1 8 uday2 # to get all users > zrange users 0 -1 > zrange users 0 -1 withscores # in reverse order > zrevrange users 0 -1 # to get the length of the string > zcard users 3 # to get key's value over a range > zcount users 0 2 # to remove member > zrem users uday","title":"Sorted Set Redis"},{"location":"redis/redis/#hashes-in-redis","text":"# add keys to a set > hset myhash name uday > hset myhash email dev117uday@gmail.com # get all keys from hashset > hkeys myhash # to get all values > hvals myhash # get value > hget myhash name # to check for keys > hexists myhash name # check the length > hlen myhash # to set multiple values at once > hmset myhash country india phone_no 9810039759 age 24 # to get multiple values > hmget myhash country name email # increment some value > hincrby myhash age 2 # to delete key from set > hdel myhash age # to avoid over writting the values > hsetnx myhash name Uday","title":"Hashes in Redis"},{"location":"redis/redis/#transaction","text":"# to go in transaction mode > multi (TX)> set key1 uday (TX)> set key2 yadav (TX)> set key3 dev117uday (TX)> exec 1) OK 2) OK 3) OK # to discard transaction discard Pub/Sub # To listen to a channel > subscribe my-chat # To publish to a channel > publish my-chat \"hello world\" # to subscribe to channel with patterns in name > psubscribe chats* > psubscribe h?llo If no one is sub to the channel you specify in publish, it returns 0","title":"Transaction"},{"location":"redis/redis/#geospatial-data","text":"# add geo spatial data in long : lat > GEOADD maps 77.216721 28.644800 delhi > GEOADD maps 72.877426 19.076090 mumbai # data is tored in sorted set data structure > zrange maps 0 -1 1) \"mumbai\" 2) \"delhi\" # to get the geohash of city > GEOHASH maps delhi 1) \"ttnfvnes010\" # to get long:lat > GEOPOS maps delhi 1) 1) \"77.21672326326370239\" 2) \"28.64479890853065314\" # to get distance, in meter (default) > GEODIST maps delhi mumbai \"1151873.1929\" > GEODIST maps delhi mumbai km \"1151.8732\" # within distance > GEORADIUS maps 77.216721 28.644800 2000 km 1) \"delhi\" 2) \"mumbai > GEORADIUS maps 77.216721 28.644800 2000 km withcoord 1) 1) \"delhi\" 2) 1) \"77.21672326326370239\" 2) \"28.64479890853065314\" 2) 1) \"mumbai\" 2) 1) \"72.87742406129837036\" 2) \"19.07608965708350723\" > GEORADIUS maps 77.216721 28.644800 2000 km withcoord withdist 1) 1) \"delhi\" 2) \"0.0003\" 3) 1) \"77.21672326326370239\" 2) \"28.64479890853065314\" 2) 1) \"mumbai\" 2) \"1151.8732\" 3) 1) \"72.87742406129837036\" 2) \"19.07608965708350723\" > GEORADIUSBYMEMBER maps delhi 1300 km 1) \"delhi\" 2) \"mumbai\" > GEORADIUSBYMEMBER maps delhi 1300 km withcoord withdist desc|asc 1) 1) \"mumbai\" 2) \"1151.8732\" 3) 1) \"72.87742406129837036\" 2) \"19.07608965708350723\" 2) 1) \"delhi\" 2) \"0.0000\" 3) 1) \"77.21672326326370239\" 2) \"28.64479890853065314\"","title":"GeoSpatial Data"},{"location":"sql/order-of-sql-execution/","text":"Order of SQL Execution 1. FROM and JOIN s The FROM clause, and subsequent JOIN s are first executed to determine the total working set of data that is being queried. This includes sub-queries in this clause, and can cause temporary tables to be created under the hood containing all the columns and rows of the tables being joined. 2. WHERE Once we have the total working set of data, the first-pass WHERE constraints are applied to the individual rows, and rows that do not satisfy the constraint are discarded. Each of the constraints can only access columns directly from the tables requested in the FROM clause. Aliases in the SELECT part of the query are not accessible in most databases since they may include expressions dependent on parts of the query that have not yet executed. 3. GROUP BY The remaining rows after the WHERE constraints are applied are then grouped based on common values in the column specified in the GROUP BY clause. As a result of the grouping, there will only be as many rows as there are unique values in that column. Implicitly, this means that you should only need to use this when you have aggregate functions in your query. 4. HAVING If the query has a GROUP BY clause, then the constraints in the HAVING clause are then applied to the grouped rows, discard the grouped rows that don't satisfy the constraint. Like the WHERE clause, aliases are also not accessible from this step in most databases. 5. SELECT Any expressions in the SELECT part of the query are finally computed. 6. DISTINCT Of the remaining rows, rows with duplicate values in the column marked as DISTINCT will be discarded. 7. ORDER BY If an order is specified by the ORDER BY clause, the rows are then sorted by the specified data in either ascending or descending order. Since all the expressions in the SELECT part of the query have been computed, you can reference aliases in this clause. 8. LIMIT / OFFSET Finally, the rows that fall outside the range specified by the LIMIT and OFFSET are discarded, leaving the final set of rows to be returned from the query.","title":"Order of SQL Execution"},{"location":"sql/order-of-sql-execution/#order-of-sql-execution","text":"","title":"Order of SQL Execution"},{"location":"sql/order-of-sql-execution/#1-from-and-joins","text":"The FROM clause, and subsequent JOIN s are first executed to determine the total working set of data that is being queried. This includes sub-queries in this clause, and can cause temporary tables to be created under the hood containing all the columns and rows of the tables being joined.","title":"1. FROM and JOINs"},{"location":"sql/order-of-sql-execution/#2-where","text":"Once we have the total working set of data, the first-pass WHERE constraints are applied to the individual rows, and rows that do not satisfy the constraint are discarded. Each of the constraints can only access columns directly from the tables requested in the FROM clause. Aliases in the SELECT part of the query are not accessible in most databases since they may include expressions dependent on parts of the query that have not yet executed.","title":"2. WHERE"},{"location":"sql/order-of-sql-execution/#3-group-by","text":"The remaining rows after the WHERE constraints are applied are then grouped based on common values in the column specified in the GROUP BY clause. As a result of the grouping, there will only be as many rows as there are unique values in that column. Implicitly, this means that you should only need to use this when you have aggregate functions in your query.","title":"3. GROUP BY"},{"location":"sql/order-of-sql-execution/#4-having","text":"If the query has a GROUP BY clause, then the constraints in the HAVING clause are then applied to the grouped rows, discard the grouped rows that don't satisfy the constraint. Like the WHERE clause, aliases are also not accessible from this step in most databases.","title":"4. HAVING"},{"location":"sql/order-of-sql-execution/#5-select","text":"Any expressions in the SELECT part of the query are finally computed.","title":"5. SELECT"},{"location":"sql/order-of-sql-execution/#6-distinct","text":"Of the remaining rows, rows with duplicate values in the column marked as DISTINCT will be discarded.","title":"6. DISTINCT"},{"location":"sql/order-of-sql-execution/#7-order-by","text":"If an order is specified by the ORDER BY clause, the rows are then sorted by the specified data in either ascending or descending order. Since all the expressions in the SELECT part of the query have been computed, you can reference aliases in this clause.","title":"7. ORDER BY"},{"location":"sql/order-of-sql-execution/#8-limit-offset","text":"Finally, the rows that fall outside the range specified by the LIMIT and OFFSET are discarded, leaving the final set of rows to be returned from the query.","title":"8. LIMIT / OFFSET"},{"location":"sql/advance-tables/internals/","text":"Advance Tables Generated Columns faster than triggers create table if not exists area ( w real, h real, area real GENERATED ALWAYS AS ( w*h ) STORED ); INSERT INTO area (w, h) values (2,3),(4,7); select * from area; update area set w = 10 where w = 4 select * from area; Internals -- size of database SELECT datname as db_name, pg_size_pretty(pg_database_size(datname)) as database_size FROM pg_database ORDER BY pg_database_size(datname) DESC; -- list all databases and schema SELECT catalog_name as \"Database Name\" FROM information_schema.information_schema_catalog_name; -- list all schemas SELECT catalog_name, schema_name, schema_owner FROM information_schema.schemata; -- list all schema starting with pg_... SELECT * FROM information_schema.schemata WHERE schema_name LIKE 'pg%'; -- list all tables SELECT * FROM information_schema.tables WHERE table_schema = 'public' -- list all views SELECT * FROM information_schema.views WHERE table_schema = 'public' -- views from information_schema SELECT * FROM information_schema.views WHERE table_schema = 'information_schema' -- list all columns SELECT * FROM information_schema.columns WHERE table_name = 'orders' -- look at system metadata SELECT CURRENT_CATALOG, CURRENT_DATABASE(), CURRENT_SCHEMA, CURRENT_USER, SESSION_USER; -- LOOK AT DATABASE VERSION SELECT VERSION(); SELECT has_database_privilege('learning','CREATE') has_schema_privilege('public','USAGE'), has_table_privilege('orders','INSERT'), has_any_column_privilege('orders','SELECT'); SELECT current_setting('timezone'); -- show all running queries SELECT pid, age(clock_timestamp(),query_start), usename as run_by_user_name, query as running FROM pg_stat_activity WHERE query != '<IDLE>' AND query NOT ILIKE '%pg_stat_activity%' ORDER BY query_start DESC; -- show all idle query SELECT pid, age(clock_timestamp(),query_start), usename as run_by_user_name, query as running FROM pg_stat_activity WHERE query = '<IDLE>' ORDER BY query_start DESC; -- KILL running query SELECT pg_cancel_backend(pid); -- get live and dead rows in table SELECT relname, n_live_tup, n_dead_tup FROM pg_stat_user_tables; -- show location of postgres data directory show data_directory; -- show files of a table is located SELECT pg_relation_filepath('orders');","title":"Advance Tables"},{"location":"sql/advance-tables/internals/#advance-tables","text":"","title":"Advance Tables"},{"location":"sql/advance-tables/internals/#generated-columns","text":"faster than triggers create table if not exists area ( w real, h real, area real GENERATED ALWAYS AS ( w*h ) STORED ); INSERT INTO area (w, h) values (2,3),(4,7); select * from area; update area set w = 10 where w = 4 select * from area;","title":"Generated Columns"},{"location":"sql/advance-tables/internals/#internals","text":"-- size of database SELECT datname as db_name, pg_size_pretty(pg_database_size(datname)) as database_size FROM pg_database ORDER BY pg_database_size(datname) DESC; -- list all databases and schema SELECT catalog_name as \"Database Name\" FROM information_schema.information_schema_catalog_name; -- list all schemas SELECT catalog_name, schema_name, schema_owner FROM information_schema.schemata; -- list all schema starting with pg_... SELECT * FROM information_schema.schemata WHERE schema_name LIKE 'pg%'; -- list all tables SELECT * FROM information_schema.tables WHERE table_schema = 'public' -- list all views SELECT * FROM information_schema.views WHERE table_schema = 'public' -- views from information_schema SELECT * FROM information_schema.views WHERE table_schema = 'information_schema' -- list all columns SELECT * FROM information_schema.columns WHERE table_name = 'orders' -- look at system metadata SELECT CURRENT_CATALOG, CURRENT_DATABASE(), CURRENT_SCHEMA, CURRENT_USER, SESSION_USER; -- LOOK AT DATABASE VERSION SELECT VERSION(); SELECT has_database_privilege('learning','CREATE') has_schema_privilege('public','USAGE'), has_table_privilege('orders','INSERT'), has_any_column_privilege('orders','SELECT'); SELECT current_setting('timezone'); -- show all running queries SELECT pid, age(clock_timestamp(),query_start), usename as run_by_user_name, query as running FROM pg_stat_activity WHERE query != '<IDLE>' AND query NOT ILIKE '%pg_stat_activity%' ORDER BY query_start DESC; -- show all idle query SELECT pid, age(clock_timestamp(),query_start), usename as run_by_user_name, query as running FROM pg_stat_activity WHERE query = '<IDLE>' ORDER BY query_start DESC; -- KILL running query SELECT pg_cancel_backend(pid); -- get live and dead rows in table SELECT relname, n_live_tup, n_dead_tup FROM pg_stat_user_tables; -- show location of postgres data directory show data_directory; -- show files of a table is located SELECT pg_relation_filepath('orders');","title":"Internals"},{"location":"sql/advance-tables/managing-tables/","text":"Managing Tables Creating new tables with INTO SELECT * INTO emp3 FROM orders WHERE employee_id = 3; SELECT * FROM emp3; select employee_id, ship_name from emp3 limit 5; employee_id | ship_name -------------+------------------------ 3 | Victuailles en stock 3 | Hanari Carnes 3 | Wellington Importadora 3 | Wartian Herkku 3 | QUICK-Stop Create table with NOT DATA Just copying the table structure CREATE TABLE emp1 as (SELECT * FROM orders where employee_id = 1) WITH NO DATA; SELECT * FROM emp1; Tables are Fraud When you UPDATE value in database, it doesnt update the original row When you DELETE a row in table, it doesnt show you the new row, but it still remains in db drop table table_vacuum; CREATE TABLE table_vacuum ( id integer ); select pg_total_relation_size('table_vacuum'), pg_size_pretty(pg_total_relation_size('table_vacuum')); -- output : 0 bytes INSERT INTO table_vacuum SELECT * FROM generate_series(1, 400000); select pg_total_relation_size('table_vacuum'), pg_size_pretty(pg_total_relation_size('table_vacuum')); -- output : 14MB SELECT * FROM table_vacuum limit 5; update table_vacuum set id = id + 2; select pg_total_relation_size('table_vacuum'), pg_size_pretty(pg_total_relation_size('table_vacuum')); -- output : 28MB SELECT * FROM table_vacuum limit 5; -- look at autovacuum process SELECT relname, last_vacuum, last_autovacuum, last_analyze, vacuum_count, autovacuum_count FROM pg_stat_all_tables WHERE relname = 'table_vacuum'; VACUUM FULL VERBOSE table_vacuum; select pg_total_relation_size('table_vacuum'), pg_size_pretty(pg_total_relation_size('table_vacuum')); -- output : 14MB","title":"Managing Tables"},{"location":"sql/advance-tables/managing-tables/#managing-tables","text":"","title":"Managing Tables"},{"location":"sql/advance-tables/managing-tables/#creating-new-tables-with-into","text":"SELECT * INTO emp3 FROM orders WHERE employee_id = 3; SELECT * FROM emp3; select employee_id, ship_name from emp3 limit 5; employee_id | ship_name -------------+------------------------ 3 | Victuailles en stock 3 | Hanari Carnes 3 | Wellington Importadora 3 | Wartian Herkku 3 | QUICK-Stop","title":"Creating new tables with INTO"},{"location":"sql/advance-tables/managing-tables/#create-table-with-not-data","text":"Just copying the table structure CREATE TABLE emp1 as (SELECT * FROM orders where employee_id = 1) WITH NO DATA; SELECT * FROM emp1;","title":"Create table with NOT DATA"},{"location":"sql/advance-tables/managing-tables/#tables-are-fraud","text":"When you UPDATE value in database, it doesnt update the original row When you DELETE a row in table, it doesnt show you the new row, but it still remains in db drop table table_vacuum; CREATE TABLE table_vacuum ( id integer ); select pg_total_relation_size('table_vacuum'), pg_size_pretty(pg_total_relation_size('table_vacuum')); -- output : 0 bytes INSERT INTO table_vacuum SELECT * FROM generate_series(1, 400000); select pg_total_relation_size('table_vacuum'), pg_size_pretty(pg_total_relation_size('table_vacuum')); -- output : 14MB SELECT * FROM table_vacuum limit 5; update table_vacuum set id = id + 2; select pg_total_relation_size('table_vacuum'), pg_size_pretty(pg_total_relation_size('table_vacuum')); -- output : 28MB SELECT * FROM table_vacuum limit 5; -- look at autovacuum process SELECT relname, last_vacuum, last_autovacuum, last_analyze, vacuum_count, autovacuum_count FROM pg_stat_all_tables WHERE relname = 'table_vacuum'; VACUUM FULL VERBOSE table_vacuum; select pg_total_relation_size('table_vacuum'), pg_size_pretty(pg_total_relation_size('table_vacuum')); -- output : 14MB","title":"Tables are Fraud"},{"location":"sql/advance-tables/paritioning-tables/","text":"Partitioning Tables It is splitting table into logical division multiple smaller pieces more manageable pieces table Partition leads to a huge performance boost. Types of ranges Range : The table is partitioned into \"range\" defined by a key column or set of columns, with no overlap between the ranges of values assigned to different partitions List : According to a key in table, ex : country, sales Hash : The partition specifying a modulus and a reminder for each partition. Table Inheritance create table master ( pk INTEGER primary key , tag text, parent integer ); create table master_child() inherits (master); ALTER TABLE public.master_child ADD PRIMARY KEY (pk); select * from master; select * from master_child; insert into master (pk, tag, parent) values (1,'pencil',0); insert into master_child (pk, tag, parent) values (2,'pen',0); update master set tag = 'monitor' where pk = 2; select * from only master; select * from only master_child; -- error drop table master; drop table master cascade ; Range Partitioning create table employees_range ( id bigserial, birth_date DATE NOT NULL , country_code VARCHAR(2) NOT NULL ) PARTITION BY RANGE (birth_date); CREATE TABLE employee_range_y2000 PARTITION of employees_range for values from ('2000-01-01') to ('2001-01-01'); CREATE TABLE employee_range_y2001 PARTITION of employees_range for values from ('2001-01-01') to ('2002-01-01'); insert into employees_range (birth_date, country_code) values ('2000-01-01','US'), ('2000-01-02','US'), ('2000-12-31','US'), ('2000-01-01','US'), ('2001-01-01','US'), ('2001-01-02','US'), ('2001-12-31','US'), ('2001-01-01','US'); select * from employees_range; select * from only employees_range; -- you will get nothing select * from employee_range_y2000; select * from employee_range_y2001; explain analyze select * from employees_range where birth_date = '2001-01-01'; List Partitioning create table employee_list ( id bigserial, birth_date DATE NOT NULL , country_code VARCHAR(2) not null ) PARTITION BY LIST ( country_code ); create table employee_list_eu PARTITION of employee_list for values in ('UK','DE'); create table employee_list_us PARTITION of employee_list for values in ('US','BZ'); Hash Partitioning create table employee_hash ( id bigserial, birth_date DATE NOT NULL, country_code varchar(2) not null ) PARTITION BY HASH (id); create table employee_hash_0 partition of employee_hash for values with (modulus 3, remainder 0); create table employee_hash_1 partition of employee_hash for values with (modulus 3, remainder 1); create table employee_hash_2 partition of employee_hash for values with (modulus 3, remainder 2); Default Partitioning create table employee_default_part partition of employee_list default; Multilevel Partitioning create table employee_list_master ( id bigserial, birth_date DATE NOT NULL , country_code VARCHAR(2) not null ) PARTITION BY LIST ( country_code ); create table employee_list_eu_m PARTITION of employee_list_master for values in ('UK','DE') partition by hash (id); create table employee_list_us_m PARTITION of employee_list_master for values in ('US','BZ'); create table employee_hash_0_m partition of employee_list_eu_m for values with (modulus 3, remainder 0); create table employee_hash_1_m partition of employee_list_eu_m for values with (modulus 3, remainder 1); create table employee_hash_2_m partition of employee_list_eu_m for values with (modulus 3, remainder 2); Attach and DeAttach Partitions create table employees_list_sp partition of employee_list for values in ('SP'); insert into employee_list (birth_date, country_code) values ('2001-01-01','SP'); create table employees_list_in partition of employee_list for values in ('IN'); create table employees_list_default partition of employee_list default ALTER TABLE employee_list detach partition employees_list_in; Altering Partitions create table t1 ( a int, b int ) partition by range (a); create table t1p1 partition of t1 for values from (0) to (1000); create table t1p2 partition of t1 for values from (2000) to (3000); insert into t1 (a, b) values (1, 1); -- detach -- alter -- attach BEGIN TRANSACTION ; ALTER TABLE T1 DETACH PARTITION t1p1; ALTER TABLE t1 attach partition t1p1 for values FROM (0) to (200); commit TRANSACTION; Partition Indexes Creating an index on the master/parent table will automatically create same indexes to every attached table partition PostgreSQL doesnt allow a way to create a single index covering every partition of the parent table. You have to create indexes for each table create unique index idx_u_employee_list_id_country_code on employee_list (id); -- error create unique index idx_u_employee_list_id_country_code on employee_list (id,country_code); -- with paritition key","title":"Partitioning Tables"},{"location":"sql/advance-tables/paritioning-tables/#partitioning-tables","text":"It is splitting table into logical division multiple smaller pieces more manageable pieces table Partition leads to a huge performance boost. Types of ranges Range : The table is partitioned into \"range\" defined by a key column or set of columns, with no overlap between the ranges of values assigned to different partitions List : According to a key in table, ex : country, sales Hash : The partition specifying a modulus and a reminder for each partition.","title":"Partitioning Tables"},{"location":"sql/advance-tables/paritioning-tables/#table-inheritance","text":"create table master ( pk INTEGER primary key , tag text, parent integer ); create table master_child() inherits (master); ALTER TABLE public.master_child ADD PRIMARY KEY (pk); select * from master; select * from master_child; insert into master (pk, tag, parent) values (1,'pencil',0); insert into master_child (pk, tag, parent) values (2,'pen',0); update master set tag = 'monitor' where pk = 2; select * from only master; select * from only master_child; -- error drop table master; drop table master cascade ;","title":"Table Inheritance"},{"location":"sql/advance-tables/paritioning-tables/#range-partitioning","text":"create table employees_range ( id bigserial, birth_date DATE NOT NULL , country_code VARCHAR(2) NOT NULL ) PARTITION BY RANGE (birth_date); CREATE TABLE employee_range_y2000 PARTITION of employees_range for values from ('2000-01-01') to ('2001-01-01'); CREATE TABLE employee_range_y2001 PARTITION of employees_range for values from ('2001-01-01') to ('2002-01-01'); insert into employees_range (birth_date, country_code) values ('2000-01-01','US'), ('2000-01-02','US'), ('2000-12-31','US'), ('2000-01-01','US'), ('2001-01-01','US'), ('2001-01-02','US'), ('2001-12-31','US'), ('2001-01-01','US'); select * from employees_range; select * from only employees_range; -- you will get nothing select * from employee_range_y2000; select * from employee_range_y2001; explain analyze select * from employees_range where birth_date = '2001-01-01';","title":"Range Partitioning"},{"location":"sql/advance-tables/paritioning-tables/#list-partitioning","text":"create table employee_list ( id bigserial, birth_date DATE NOT NULL , country_code VARCHAR(2) not null ) PARTITION BY LIST ( country_code ); create table employee_list_eu PARTITION of employee_list for values in ('UK','DE'); create table employee_list_us PARTITION of employee_list for values in ('US','BZ');","title":"List Partitioning"},{"location":"sql/advance-tables/paritioning-tables/#hash-partitioning","text":"create table employee_hash ( id bigserial, birth_date DATE NOT NULL, country_code varchar(2) not null ) PARTITION BY HASH (id); create table employee_hash_0 partition of employee_hash for values with (modulus 3, remainder 0); create table employee_hash_1 partition of employee_hash for values with (modulus 3, remainder 1); create table employee_hash_2 partition of employee_hash for values with (modulus 3, remainder 2);","title":"Hash Partitioning"},{"location":"sql/advance-tables/paritioning-tables/#default-partitioning","text":"create table employee_default_part partition of employee_list default;","title":"Default Partitioning"},{"location":"sql/advance-tables/paritioning-tables/#multilevel-partitioning","text":"create table employee_list_master ( id bigserial, birth_date DATE NOT NULL , country_code VARCHAR(2) not null ) PARTITION BY LIST ( country_code ); create table employee_list_eu_m PARTITION of employee_list_master for values in ('UK','DE') partition by hash (id); create table employee_list_us_m PARTITION of employee_list_master for values in ('US','BZ'); create table employee_hash_0_m partition of employee_list_eu_m for values with (modulus 3, remainder 0); create table employee_hash_1_m partition of employee_list_eu_m for values with (modulus 3, remainder 1); create table employee_hash_2_m partition of employee_list_eu_m for values with (modulus 3, remainder 2);","title":"Multilevel Partitioning"},{"location":"sql/advance-tables/paritioning-tables/#attach-and-deattach-partitions","text":"create table employees_list_sp partition of employee_list for values in ('SP'); insert into employee_list (birth_date, country_code) values ('2001-01-01','SP'); create table employees_list_in partition of employee_list for values in ('IN'); create table employees_list_default partition of employee_list default ALTER TABLE employee_list detach partition employees_list_in;","title":"Attach and DeAttach Partitions"},{"location":"sql/advance-tables/paritioning-tables/#altering-partitions","text":"create table t1 ( a int, b int ) partition by range (a); create table t1p1 partition of t1 for values from (0) to (1000); create table t1p2 partition of t1 for values from (2000) to (3000); insert into t1 (a, b) values (1, 1); -- detach -- alter -- attach BEGIN TRANSACTION ; ALTER TABLE T1 DETACH PARTITION t1p1; ALTER TABLE t1 attach partition t1p1 for values FROM (0) to (200); commit TRANSACTION;","title":"Altering Partitions"},{"location":"sql/advance-tables/paritioning-tables/#partition-indexes","text":"Creating an index on the master/parent table will automatically create same indexes to every attached table partition PostgreSQL doesnt allow a way to create a single index covering every partition of the parent table. You have to create indexes for each table create unique index idx_u_employee_list_id_country_code on employee_list (id); -- error create unique index idx_u_employee_list_id_country_code on employee_list (id,country_code); -- with paritition key","title":"Partition Indexes"},{"location":"sql/advance-tables/pivotal-or-crosstab-tables/","text":"Pivotal or Crosstab Tables something about cross tab CREATE EXTENSION IF NOT EXISTS tablefunc; CREATE TABLE ct(id SERIAL, rowid TEXT, attribute TEXT, value TEXT); INSERT INTO ct(rowid, attribute, value) VALUES('test1','att1','val1'); INSERT INTO ct(rowid, attribute, value) VALUES('test1','att2','val2'); INSERT INTO ct(rowid, attribute, value) VALUES('test1','att3','val3'); INSERT INTO ct(rowid, attribute, value) VALUES('test1','att4','val4'); INSERT INTO ct(rowid, attribute, value) VALUES('test2','att1','val5'); INSERT INTO ct(rowid, attribute, value) VALUES('test2','att2','val6'); INSERT INTO ct(rowid, attribute, value) VALUES('test2','att3','val7'); INSERT INTO ct(rowid, attribute, value) VALUES('test2','att4','val8'); SELECT * FROM ct; SELECT * FROM crosstab( 'select rowid, attribute, value from ct where attribute = ''att2'' or attribute = ''att3'' order by 1,2') AS ct(row_name text, category_1 text, category_2 text); SELECT * FROM crosstab ( ' SELECT location, year, SUM(raindays)::int FROM rainfalls GROUP BY location, year ORDER BY location, year ' ) AS ct ( \"LOCATION\" TEXT, \"2012\" INT, \"2013\" INT, \"2014\" INT, \"2015\" INT, \"2016\" INT, \"2017\" INT )","title":"Pivotal or Crosstab Tables"},{"location":"sql/advance-tables/pivotal-or-crosstab-tables/#pivotal-or-crosstab-tables","text":"something about cross tab CREATE EXTENSION IF NOT EXISTS tablefunc; CREATE TABLE ct(id SERIAL, rowid TEXT, attribute TEXT, value TEXT); INSERT INTO ct(rowid, attribute, value) VALUES('test1','att1','val1'); INSERT INTO ct(rowid, attribute, value) VALUES('test1','att2','val2'); INSERT INTO ct(rowid, attribute, value) VALUES('test1','att3','val3'); INSERT INTO ct(rowid, attribute, value) VALUES('test1','att4','val4'); INSERT INTO ct(rowid, attribute, value) VALUES('test2','att1','val5'); INSERT INTO ct(rowid, attribute, value) VALUES('test2','att2','val6'); INSERT INTO ct(rowid, attribute, value) VALUES('test2','att3','val7'); INSERT INTO ct(rowid, attribute, value) VALUES('test2','att4','val8'); SELECT * FROM ct; SELECT * FROM crosstab( 'select rowid, attribute, value from ct where attribute = ''att2'' or attribute = ''att3'' order by 1,2') AS ct(row_name text, category_1 text, category_2 text); SELECT * FROM crosstab ( ' SELECT location, year, SUM(raindays)::int FROM rainfalls GROUP BY location, year ORDER BY location, year ' ) AS ct ( \"LOCATION\" TEXT, \"2012\" INT, \"2013\" INT, \"2014\" INT, \"2015\" INT, \"2016\" INT, \"2017\" INT )","title":"Pivotal or Crosstab Tables"},{"location":"sql/data-types/","text":"Data Types Data Types Boolean Data TRUE FALSE NULL TRUE FALSE TRUE FALSE 'true' 'false' 't' 'f' 'yes' 'no' 'y' 'n' '1' '0' CREATE TABLE booltable ( id SERIAL PRIMARY KEY , is_enable BOOLEAN NOT NULL ); INSERT INTO booltable (is_enable) VALUES (TRUE), ('true'), ('y') , ('yes'), ('t'), ('1'); INSERT INTO booltable (is_enable) VALUES (FALSE), ('false'), ('n') , ('no'), ('f'), ('0'); SELECT * FROM booltable; SELECT * FROM booltable WHERE is_enable = 'y'; SELECT * FROM booltable WHERE NOT is_enable; Character Data Character Type Notes CHARACTER (N), CHAR (N) fixed-length, blank padded CHARACTER VARYING (N), VARCHAR(N) variable length with length limit TEXT, VARCHAR variable unlimited length, max 1GB n is default to 1 -- INPUT SELECT CAST('Uday' as character(10)) as \"name\"; -- OUTPUT \"Uday \" -- INPUT SELECT 'Uday'::character(10) as \"name\"; -- OUTPUT \"Uday \" -- INPUT SELECT 'uday'::varchar(10); -- OUTPUT \"uday\" -- INPUT SELECT 'lorem ipsum'::text; -- OUTPUT \"lorem ipsum\" Numeric Data Types Notes Integers whole number, +ve and -ve Fixed-point, floating point for fractions of whole nu type size (bytes) min max smallint 2 -32678 32767 integer 4 -2,147,483,648 2,147,483,647 bigint 8 -9223372036854775808 9223372036854775807 type size range smallserial 2 1 to 32767 serial 4 1 to 2147483647 bigserial 8 1 to 9223372036854775807 Fixed Point Data numeric ( precision , scale ) | decimal ( precision , scale ) precision : max number of digits to the left and right of the decimal point scale : number of digits allowable on the right of the decimal point Floating Point Data Type Notes Real allows precision to six decimal digits Double precision allows precision to 15 digits points of precision type size storage type Range numeric, decimal variable fixed point 131072 digits before decimal point and 16383 digits after the decimal point real 4 floating point 6 decimal digits precision double precision 8 floating point 15 decimal digits precision CREATE TABLE table_numbers ( col_numeric numeric(20,5), col_real real, col_double double precision ); INSERT INTO table_numbers (col_numeric,col_real,col_double) VALUES (.9,.9,.9), (3.34675,3.34675,3.34675), (4.2345678910,4.2345678910,4.2345678910); SELECT * FROM table_numbers; -- OUTPUT learning=# select * from table_numbers ; col_numeric | col_real | col_double -------------+----------+------------- 0.90000 | 0.9 | 0.9 3.34675 | 3.34675 | 3.34675 4.23457 | 4.234568 | 4.234567891 (3 rows) Hierarchical order to SELECT best type : numeric > decimal > float Date Time Data type stores low high Date date only 4713 BC 294276 AD Time time only 4713 BC 5874897 AD Timestamp date and time 4713 BC 294276 AD Timestampz date, time and timezone 4713 BC 294276 AD Interval difference btw time Date type CREATE TABLE table_dates ( id serial primary key, employee_name varchar(100) not null, hire_date DATE NOT NULL, add_date DATE DEFAULT CURRENT_DATE ); INSERT INTO table_dates (employee_name, hire_date) VALUES ('uday','2020-02-02'),('another uday','2020-02-01'); SELECT * FROM table_dates; SELECT NOW(); Time type CREATE TABLE table_time ( id serial primary key , class_name varchar(10) not null , start_time time not null , end_time time not null ); INSERT INTO table_time (class_name, start_time, end_time) VALUES ('maths','08:00:00','08:55:00'), ('chemistry','08:55:00','09:00:00'); SELECT * FROM table_time; -- OUTPUT id | class_name | start_time | end_time ----+------------+------------+---------- 1 | maths | 08:00:00 | 08:55:00 2 | chemistry | 08:55:00 | 09:00:00 (2 rows) SELECT CURRENT_TIME; current_time -------------------- 07:21:00.163354+00 (1 row) SELECT CURRENT_TIME(2); current_time ---------------- 07:21:14.96+00 (1 row) SELECT LOCALTIME; localtime ----------------- 07:21:36.717509 (1 row) SELECT time '12:10' - time '04:30' as RESULT; result ---------- 07:40:00 (1 row) -- format : interval 'n type' -- n = number -- type : second, minute, hours, day, month, year .... SELECT CURRENT_TIME , CURRENT_TIME + INTERVAL '2 hours' as RESULT; current_time | result --------------------+-------------------- 07:22:06.241919+00 | 09:22:06.241919+00 (1 row) SELECT CURRENT_TIME , CURRENT_TIME + INTERVAL '-2 hours' as RESULT; current_time | result --------------------+-------------------- 07:22:16.644727+00 | 05:22:16.644727+00 (1 row) Timestamp and Timezone timestamp : stores time without time zone timestamptz : timestamp with time zone , stored using UTC format adding timestamp to timestamptz without mentioning the zone will result in server automatically assumes timezone to system's timezone Internally, PostgreSQL will store the timezoneaccurately but then OUTPUTting the data, will it be converted according to your timezone SELECT name FROM pg_timezone_names where name = 'posix/Asia/Calcutta'; SET TIMEZONE='Asia/Calcutta'; SELECT NOW()::TIMESTAMP; now ---------------------------- 2021-08-12 12:53:03.971433 (1 row) CREATE TABLE table_time_tz ( ts timestamp, tstz timestamptz ); INSERT INTO table_time_tz (ts, tstz) VALUES ('2020-12-22 10:10:10', '2020-12-22 10:10:10.009+05:30'); SELECT * FROM table_time_tz; ts | tstz ---------------------+------------------------------- 2020-12-22 10:10:10 | 2020-12-22 10:10:10.009+05:30 (1 row) SELECT CURRENT_TIMESTAMP; current_timestamp --------------------------------- 2021-08-12 12:53:29.54762+05:30 (1 row) SELECT timezone('Asia/Singapore','2020-01-01 00:00:00') timezone --------------------- 2020-01-01 02:30:00 (1 row) UUID UUID : Universal Unique Identifier PostgreSQL doesn't provide internal function to generate UUID's, use uuid-ossp CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"; SELECT uuid_generate_v1(); uuid_generate_v1 -------------------------------------- 4d459e0c-fb3e-11eb-a638-0242ac110002 -- pure randomness SELECT uuid_generate_v4(); uuid_generate_v4 -------------------------------------- 418f39e5-8a46-4da2-8cea-884904f45d6f CREATE TABLE products_uuid ( id uuid default uuid_generate_v1(), product_name varchar(100) not null ); INSERT INTO products_uuid (product_name) VALUES ('ice cream'),('cake'),('candies'); SELECT * FROM products_uuid; id | product_name --------------------------------------+-------------- 5cf1dbe0-fb3e-11eb-a638-0242ac110002 | ice cream 5cf1df28-fb3e-11eb-a638-0242ac110002 | cake 5cf1df46-fb3e-11eb-a638-0242ac110002 | candies CREATE TABLE products_uuid_v4 ( id uuid default uuid_generate_v4(), product_name varchar(100) not null ); INSERT INTO products_uuid_v4 (product_name) VALUES ('ice cream'),('cake'),('candies'); SELECT * FROM products_uuid_v4; learning=# SELECT * FROM products_uuid_v4; id | product_name --------------------------------------+-------------- 83b74bed-2cf8-4e26-80b0-c7c7b2e5f3e7 | ice cream ac563251-7a95-408d-966b-ed5ecc1f228d | cake 1079f6d3-b0c3-40ef-bd2e-da4467b63432 | candies HSTORE stores data in key-value pairs key and VALUES are text string only CREATE EXTENSION IF NOT EXISTS hstore; CREATE TABLE table_hstore ( id SERIAL PRIMARY KEY , title varchar(100) not null, book_info hstore ); INSERT INTO table_hstore (title, book_info) VALUES ( 'Title 1', ' \"publisher\" => \"ABC publisher\" , \"paper_cost\" => \"100\" , \"e_cost\" => \"5.85\" ' ); SELECT * FROM table_hstore; id | title | book_info 1 | Title 1 | \"e_cost\"=>\"5.85\", \"publisher\"=>\"ABC publisher\", \"paper_cost\"=>\"100\" SELECT book_info -> 'publisher' as publisher FROM table_hstore; publisher --------------- ABC publisher Json PostgreSQL supports both JSON BSON or JSONB ( Binary JSON ) JSONB has full support for indexing CREATE TABLE table_json ( id SERIAL PRIMARY KEY , docs json ); INSERT INTO table_json (docs) VALUES ('[1,2,3,4,5,6]'),('{\"key\":\"value\"}'); INSERT INTO table_json (docs) VALUES ('[{\"key\":\"value\"},{\"key2\":\"value2\"}]'); SELECT * FROM table_json; id | docs ----+------------------------------------- 1 | [1,2,3,4,5,6] 2 | {\"key\":\"value\"} 3 | [{\"key\":\"value\"},{\"key2\":\"value2\"}] ALTER TABLE table_json alter column docs type jsonb; SELECT * FROM table_json where docs @> '2'; id | docs ----+-------------------- 1 | [1, 2, 3, 4, 5, 6] CREATE index on table_json USING GIN (docs jsonb_path_ops); Network Address Data Types Name Storage Size Notes cidr 7 or 19 bytes IPv4 and IPv6 networks inet 7 or 19 bytes IPv4 and IPv6 hosts and networks macaddr 6 bytes MAC addresses macaddr8 8 bytes MAC addresses ( EUI 64-bit ) It is better to use these types instead of plain text types of store network address, because these types offer input error checking and specialised operators and functions Supports indexing and advance operations CREATE TABLE table_netaddr ( id SERIAL PRIMARY KEY , ip inet ); INSERT INTO table_netaddr (ip) VALUES ('148.77.50.74'), ('110.158.172.66'), ('176.103.251.175'), ('84.84.14.58'), ('141.122.225.161'), ('78.44.113.33'), ('81.236.254.9'), ('82.116.85.21'), ('54.64.79.223'), ('162.240.78.253'); SELECT * FROM table_netaddr LIMIT 5; id | ip ----+----------------- 1 | 148.77.50.74 2 | 110.158.172.66 3 | 176.103.251.175 4 | 84.84.14.58 5 | 141.122.225.161 SELECT ip, set_masklen(ip,24) as inet_24, set_masklen(ip::cidr,24) as cidr_24 , set_masklen(ip::cidr,27) as cidr_27, set_masklen(ip::cidr,28) as cidr_28 FROM table_netaddr LIMIT 2; ip | inet_24 | cidr_24 | cidr_27 | cidr_28 148.77.50.74 | 148.77.50.74/24 | 148.77.50.0/24 | 148.77.50.64/27 | 148.77.50.64/28 110.158.172.66 | 110.158.172.66/24 | 110.158.172.0/24 | 110.158.172.64/27 | 110.158.172.64/28","title":"Data Types"},{"location":"sql/data-types/#data-types","text":"","title":"Data Types"},{"location":"sql/data-types/#data-types_1","text":"","title":"Data Types"},{"location":"sql/data-types/#boolean-data","text":"TRUE FALSE NULL TRUE FALSE TRUE FALSE 'true' 'false' 't' 'f' 'yes' 'no' 'y' 'n' '1' '0' CREATE TABLE booltable ( id SERIAL PRIMARY KEY , is_enable BOOLEAN NOT NULL ); INSERT INTO booltable (is_enable) VALUES (TRUE), ('true'), ('y') , ('yes'), ('t'), ('1'); INSERT INTO booltable (is_enable) VALUES (FALSE), ('false'), ('n') , ('no'), ('f'), ('0'); SELECT * FROM booltable; SELECT * FROM booltable WHERE is_enable = 'y'; SELECT * FROM booltable WHERE NOT is_enable;","title":"Boolean Data"},{"location":"sql/data-types/#character-data","text":"Character Type Notes CHARACTER (N), CHAR (N) fixed-length, blank padded CHARACTER VARYING (N), VARCHAR(N) variable length with length limit TEXT, VARCHAR variable unlimited length, max 1GB n is default to 1 -- INPUT SELECT CAST('Uday' as character(10)) as \"name\"; -- OUTPUT \"Uday \" -- INPUT SELECT 'Uday'::character(10) as \"name\"; -- OUTPUT \"Uday \" -- INPUT SELECT 'uday'::varchar(10); -- OUTPUT \"uday\" -- INPUT SELECT 'lorem ipsum'::text; -- OUTPUT \"lorem ipsum\"","title":"Character Data"},{"location":"sql/data-types/#numeric-data","text":"Types Notes Integers whole number, +ve and -ve Fixed-point, floating point for fractions of whole nu type size (bytes) min max smallint 2 -32678 32767 integer 4 -2,147,483,648 2,147,483,647 bigint 8 -9223372036854775808 9223372036854775807 type size range smallserial 2 1 to 32767 serial 4 1 to 2147483647 bigserial 8 1 to 9223372036854775807","title":"Numeric Data"},{"location":"sql/data-types/#fixed-point-data","text":"numeric ( precision , scale ) | decimal ( precision , scale ) precision : max number of digits to the left and right of the decimal point scale : number of digits allowable on the right of the decimal point","title":"Fixed Point Data"},{"location":"sql/data-types/#floating-point-data","text":"Type Notes Real allows precision to six decimal digits Double precision allows precision to 15 digits points of precision type size storage type Range numeric, decimal variable fixed point 131072 digits before decimal point and 16383 digits after the decimal point real 4 floating point 6 decimal digits precision double precision 8 floating point 15 decimal digits precision CREATE TABLE table_numbers ( col_numeric numeric(20,5), col_real real, col_double double precision ); INSERT INTO table_numbers (col_numeric,col_real,col_double) VALUES (.9,.9,.9), (3.34675,3.34675,3.34675), (4.2345678910,4.2345678910,4.2345678910); SELECT * FROM table_numbers; -- OUTPUT learning=# select * from table_numbers ; col_numeric | col_real | col_double -------------+----------+------------- 0.90000 | 0.9 | 0.9 3.34675 | 3.34675 | 3.34675 4.23457 | 4.234568 | 4.234567891 (3 rows) Hierarchical order to SELECT best type : numeric > decimal > float","title":"Floating Point Data"},{"location":"sql/data-types/#date-time-data","text":"type stores low high Date date only 4713 BC 294276 AD Time time only 4713 BC 5874897 AD Timestamp date and time 4713 BC 294276 AD Timestampz date, time and timezone 4713 BC 294276 AD Interval difference btw time","title":"Date Time Data"},{"location":"sql/data-types/#date-type","text":"CREATE TABLE table_dates ( id serial primary key, employee_name varchar(100) not null, hire_date DATE NOT NULL, add_date DATE DEFAULT CURRENT_DATE ); INSERT INTO table_dates (employee_name, hire_date) VALUES ('uday','2020-02-02'),('another uday','2020-02-01'); SELECT * FROM table_dates; SELECT NOW();","title":"Date type"},{"location":"sql/data-types/#time-type","text":"CREATE TABLE table_time ( id serial primary key , class_name varchar(10) not null , start_time time not null , end_time time not null ); INSERT INTO table_time (class_name, start_time, end_time) VALUES ('maths','08:00:00','08:55:00'), ('chemistry','08:55:00','09:00:00'); SELECT * FROM table_time; -- OUTPUT id | class_name | start_time | end_time ----+------------+------------+---------- 1 | maths | 08:00:00 | 08:55:00 2 | chemistry | 08:55:00 | 09:00:00 (2 rows) SELECT CURRENT_TIME; current_time -------------------- 07:21:00.163354+00 (1 row) SELECT CURRENT_TIME(2); current_time ---------------- 07:21:14.96+00 (1 row) SELECT LOCALTIME; localtime ----------------- 07:21:36.717509 (1 row) SELECT time '12:10' - time '04:30' as RESULT; result ---------- 07:40:00 (1 row) -- format : interval 'n type' -- n = number -- type : second, minute, hours, day, month, year .... SELECT CURRENT_TIME , CURRENT_TIME + INTERVAL '2 hours' as RESULT; current_time | result --------------------+-------------------- 07:22:06.241919+00 | 09:22:06.241919+00 (1 row) SELECT CURRENT_TIME , CURRENT_TIME + INTERVAL '-2 hours' as RESULT; current_time | result --------------------+-------------------- 07:22:16.644727+00 | 05:22:16.644727+00 (1 row)","title":"Time type"},{"location":"sql/data-types/#timestamp-and-timezone","text":"timestamp : stores time without time zone timestamptz : timestamp with time zone , stored using UTC format adding timestamp to timestamptz without mentioning the zone will result in server automatically assumes timezone to system's timezone Internally, PostgreSQL will store the timezoneaccurately but then OUTPUTting the data, will it be converted according to your timezone SELECT name FROM pg_timezone_names where name = 'posix/Asia/Calcutta'; SET TIMEZONE='Asia/Calcutta'; SELECT NOW()::TIMESTAMP; now ---------------------------- 2021-08-12 12:53:03.971433 (1 row) CREATE TABLE table_time_tz ( ts timestamp, tstz timestamptz ); INSERT INTO table_time_tz (ts, tstz) VALUES ('2020-12-22 10:10:10', '2020-12-22 10:10:10.009+05:30'); SELECT * FROM table_time_tz; ts | tstz ---------------------+------------------------------- 2020-12-22 10:10:10 | 2020-12-22 10:10:10.009+05:30 (1 row) SELECT CURRENT_TIMESTAMP; current_timestamp --------------------------------- 2021-08-12 12:53:29.54762+05:30 (1 row) SELECT timezone('Asia/Singapore','2020-01-01 00:00:00') timezone --------------------- 2020-01-01 02:30:00 (1 row)","title":"Timestamp and Timezone"},{"location":"sql/data-types/#uuid","text":"UUID : Universal Unique Identifier PostgreSQL doesn't provide internal function to generate UUID's, use uuid-ossp CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"; SELECT uuid_generate_v1(); uuid_generate_v1 -------------------------------------- 4d459e0c-fb3e-11eb-a638-0242ac110002 -- pure randomness SELECT uuid_generate_v4(); uuid_generate_v4 -------------------------------------- 418f39e5-8a46-4da2-8cea-884904f45d6f CREATE TABLE products_uuid ( id uuid default uuid_generate_v1(), product_name varchar(100) not null ); INSERT INTO products_uuid (product_name) VALUES ('ice cream'),('cake'),('candies'); SELECT * FROM products_uuid; id | product_name --------------------------------------+-------------- 5cf1dbe0-fb3e-11eb-a638-0242ac110002 | ice cream 5cf1df28-fb3e-11eb-a638-0242ac110002 | cake 5cf1df46-fb3e-11eb-a638-0242ac110002 | candies CREATE TABLE products_uuid_v4 ( id uuid default uuid_generate_v4(), product_name varchar(100) not null ); INSERT INTO products_uuid_v4 (product_name) VALUES ('ice cream'),('cake'),('candies'); SELECT * FROM products_uuid_v4; learning=# SELECT * FROM products_uuid_v4; id | product_name --------------------------------------+-------------- 83b74bed-2cf8-4e26-80b0-c7c7b2e5f3e7 | ice cream ac563251-7a95-408d-966b-ed5ecc1f228d | cake 1079f6d3-b0c3-40ef-bd2e-da4467b63432 | candies","title":"UUID"},{"location":"sql/data-types/#hstore","text":"stores data in key-value pairs key and VALUES are text string only CREATE EXTENSION IF NOT EXISTS hstore; CREATE TABLE table_hstore ( id SERIAL PRIMARY KEY , title varchar(100) not null, book_info hstore ); INSERT INTO table_hstore (title, book_info) VALUES ( 'Title 1', ' \"publisher\" => \"ABC publisher\" , \"paper_cost\" => \"100\" , \"e_cost\" => \"5.85\" ' ); SELECT * FROM table_hstore; id | title | book_info 1 | Title 1 | \"e_cost\"=>\"5.85\", \"publisher\"=>\"ABC publisher\", \"paper_cost\"=>\"100\" SELECT book_info -> 'publisher' as publisher FROM table_hstore; publisher --------------- ABC publisher","title":"HSTORE"},{"location":"sql/data-types/#json","text":"PostgreSQL supports both JSON BSON or JSONB ( Binary JSON ) JSONB has full support for indexing CREATE TABLE table_json ( id SERIAL PRIMARY KEY , docs json ); INSERT INTO table_json (docs) VALUES ('[1,2,3,4,5,6]'),('{\"key\":\"value\"}'); INSERT INTO table_json (docs) VALUES ('[{\"key\":\"value\"},{\"key2\":\"value2\"}]'); SELECT * FROM table_json; id | docs ----+------------------------------------- 1 | [1,2,3,4,5,6] 2 | {\"key\":\"value\"} 3 | [{\"key\":\"value\"},{\"key2\":\"value2\"}] ALTER TABLE table_json alter column docs type jsonb; SELECT * FROM table_json where docs @> '2'; id | docs ----+-------------------- 1 | [1, 2, 3, 4, 5, 6] CREATE index on table_json USING GIN (docs jsonb_path_ops);","title":"Json"},{"location":"sql/data-types/#network-address-data-types","text":"Name Storage Size Notes cidr 7 or 19 bytes IPv4 and IPv6 networks inet 7 or 19 bytes IPv4 and IPv6 hosts and networks macaddr 6 bytes MAC addresses macaddr8 8 bytes MAC addresses ( EUI 64-bit ) It is better to use these types instead of plain text types of store network address, because these types offer input error checking and specialised operators and functions Supports indexing and advance operations CREATE TABLE table_netaddr ( id SERIAL PRIMARY KEY , ip inet ); INSERT INTO table_netaddr (ip) VALUES ('148.77.50.74'), ('110.158.172.66'), ('176.103.251.175'), ('84.84.14.58'), ('141.122.225.161'), ('78.44.113.33'), ('81.236.254.9'), ('82.116.85.21'), ('54.64.79.223'), ('162.240.78.253'); SELECT * FROM table_netaddr LIMIT 5; id | ip ----+----------------- 1 | 148.77.50.74 2 | 110.158.172.66 3 | 176.103.251.175 4 | 84.84.14.58 5 | 141.122.225.161 SELECT ip, set_masklen(ip,24) as inet_24, set_masklen(ip::cidr,24) as cidr_24 , set_masklen(ip::cidr,27) as cidr_27, set_masklen(ip::cidr,28) as cidr_28 FROM table_netaddr LIMIT 2; ip | inet_24 | cidr_24 | cidr_27 | cidr_28 148.77.50.74 | 148.77.50.74/24 | 148.77.50.0/24 | 148.77.50.64/27 | 148.77.50.64/28 110.158.172.66 | 110.158.172.66/24 | 110.158.172.0/24 | 110.158.172.64/27 | 110.158.172.64/28","title":"Network Address Data Types"},{"location":"sql/data-types/arrays/","text":"Arrays Arrays Original Documentation : here Syntax column_name DATATYPE[] {CONSTRAINT} CREATE TABLE table_array ( id SERIAL, name varchar(100), grades text[] ); INSERT INTO table_array (name, grades) VALUES ('person 1', array ['100','45']); INSERT INTO table_array (name, grades) VALUES ('person 2', array ['100','90']); INSERT INTO table_array (name, grades) VALUES ('person 3', array ['100','97']); INSERT INTO table_array (name, grades) VALUES ('person 4', array ['100','94']); SELECT name, grades[1] FROM table_array; name | grades ----------+-------- person 1 | 100 person 2 | 100 person 3 | 100 person 4 | 100 Array in Tables Insert for non text data , use {value1,value2} or array ['value1','value2'] for text data , use {\"value1\",\"value2\"} or array [value1,value2] CREATE TABLE teachers ( id serial primary key, class text[] ); CREATE TABLE IF NOT EXISTS teachers ( id serial primary key, class text array ); INSERT INTO teachers (class) VALUES (array ['english','maths']); id | class ----+----------------- 1 | {english,maths} Query SELECT class[1] FROM teachers; class --------- english SELECT * FROM teachers WHERE class[1] = 'english'; id | class ----+----------------- 1 | {english,maths} SELECT * FROM teachers WHERE 'english' = any (class); id | class ----+----------------- 1 | {english,maths} Update update teachers set class[1] = 'dutch' WHERE id = 1; id | class ----+--------------- 1 | {dutch,maths} Update teachers set class[3] = 'science' WHERE id = 1; id | class ----+----------------------- 1 | {dutch,maths,science} Dimensionless CREATE TABLE teacher2 ( id serial primary key, class text array[1] ); INSERT INTO teacher2 (class) VALUES (array ['english']); id | class ----+----------- 1 | {english} -- dimensions doesnt matter INSERT INTO teacher2 (class) VALUES (array ['english','hindi']); id | class ----+----------------- 1 | {english} 2 | {english,hindi} Unnest SELECT id, class, unnest(class) FROM teacher2; id | class | unnest ----+-----------------+--------- 1 | {english} | english 2 | {english,hindi} | english 2 | {english,hindi} | hindi Multi Dimensional Array CREATE TABLE students ( id serial primary key, name varchar(50) not null, grade integer[][] ); INSERT INTO students (name, grade) VALUES ('s1', '{90,2020}'), ('s1', '{70,2020}'), ('s1', '{60,2020}'); SELECT * FROM students; id | name | grade ----+------+----------- 1 | s1 | {90,2020} 2 | s1 | {70,2020} 3 | s1 | {60,2020} SELECT * FROM students WHERE grade @> '{90}'; id | name | grade ----+------+----------- 1 | s1 | {90,2020} SELECT * FROM students WHERE '2020' = any (grade); id | name | grade ----+------+----------- 1 | s1 | {90,2020} 2 | s1 | {70,2020} 3 | s1 | {60,2020} SELECT * FROM students WHERE grade[1] < 80; id | name | grade ----+------+----------- 2 | s1 | {70,2020} 3 | s1 | {60,2020} Array vs JSONB Advantages to Array It's pretty easy to setup Requires less storage than jsonb It has multi dimensional support Indexing through GIN, greatly speeds up query The PostgreSQL planner is likely to make better decisions with PostgreSQL array, as it collects statistics on its content, but not with JSONB. Disadvantages to Array Its main advantages is that you are limited to one data type Have to follow strict order of the array data input. Advantages to JSONB Provides additional operators for querying Support for indexing Disadvantages to JSONB Has to parse the json data to binary format slow in writing, but faster in reading Doesn't maintain order Ranges SELECT INT4RANGE(1, 6) AS \"DEFAULT [(\", NUMRANGE(1.432, 6.238, '[]') AS \"[]\", DATERANGE('20200101', '20201222', '()') AS \"DATES ()\", TSRANGE(LOCALTIMESTAMP, LOCALTIMESTAMP + INTERVAL '8 DAYS', '(]') AS \"OPENED CLOSED\"; DEFAULT [( | [] | DATES () | OPENED CLOSED ------------+---------------+-------------------------+----------------------------------------------------------- [1,6) | [1.432,6.238] | [2020-01-02,2020-12-22) | (\"2021-08-24 05:22:13.03625\",\"2021-09-01 05:22:13.03625\"] SELECT ARRAY [1,2,3] AS \"INT ARRAYS\", ARRAY [2.123::FLOAT] AS \"FLOATING NUMBERS\", ARRAY [CURRENT_DATE, CURRENT_DATE + 5]; INT ARRAYS | FLOATING NUMBERS | array ------------+------------------+------------------------- {1,2,3} | {2.123} | {2021-08-24,2021-08-29} SELECT ARRAY [1,2,3,4] = ARRAY [1,2,3,4], ARRAY [1,2,3,4] = ARRAY [1,1,3,4], ARRAY [1,2,3,4] <> ARRAY [1,2,3,4], ARRAY [1,2,3,4] < ARRAY [1,5,3,4], ARRAY [1,2,3,4] <= ARRAY [1,3,3,4], ARRAY [1,2,3,4] > ARRAY [1,2,3,4]; ?column? | ?column? | ?column? | ?column? | ?column? | ?column? ----------+----------+----------+----------+----------+---------- t | f | f | t | t | f Inclusion Operators SELECT ARRAY [1,2,3,4] @> ARRAY [2,3,4] AS \"CONTAINS\", ARRAY ['A','B'] <@ ARRAY ['A','B','C'] AS \"CONTAINED BY\", ARRAY [1,2,3,4] && ARRAY [2,3,4] AS \"IS OVERLAP\"; CONTAINS | CONTAINED BY | IS OVERLAP ----------+--------------+------------ t | t | t Length and Dimensions SELECT ARRAY [1,2,3] || ARRAY [4,5,6] AS \"COMBINED ARRAY\"; COMBINED ARRAY ---------------- {1,2,3,4,5,6} SELECT ARRAY_CAT(ARRAY [1,2,3], ARRAY [4,5,6]) AS \"COMBINED ARRAY VIA CAT\"; COMBINED ARRAY VIA CAT ------------------------ {1,2,3,4,5,6} SELECT 4 || ARRAY [1,2,3] AS \"ADDING TO ARRAY\"; ADDING TO ARRAY ----------------- {4,1,2,3} SELECT ARRAY [1,2,3] || 4 AS \"ADDING TO ARRAY\"; ADDING TO ARRAY ----------------- {1,2,3,4} SELECT ARRAY_APPEND(ARRAY [1,2,3], 4) AS \"USING APPEND\"; USING APPEND -------------- {1,2,3,4} SELECT ARRAY_PREPEND(4, ARRAY [1,2,3]) AS \"USING APPEND\"; USING APPEND -------------- {4,1,2,3} SELECT ARRAY_NDIMS(ARRAY [[1,2,3,4],[1,2,3,4],[1,2,3,4]]) AS \"DIMENSIONS\", ARRAY_DIMS(ARRAY [1,2,3,4,2,3,4]) AS \"DIMENSIONS\"; DIMENSIONS | DIMENSIONS ------------+------------ 2 | [1:7] SELECT ARRAY_LENGTH(ARRAY [-111,2,3,4], 1); array_length -------------- 4 SELECT ARRAY_UPPER(ARRAY [1,2,3,4000], 1), ARRAY_LOWER(ARRAY [-100,2,3,4], 1); array_upper | array_lower -------------+------------- 4 | 1 Positions SELECT array_position(array ['jan','feb','mar'], 'feb'); array_position ---------------- 2 SELECT array_position(array [1,2,2,3,4], 2, 3); array_position ---------------- 3 SELECT array_positions(array [1,2,2,3,4], 2); array_positions ----------------- {2,3} Search, Replace, Remove SELECT array_cat(array [1,2], array [3,4]); array_cat ----------- {1,2,3,4} SELECT array_append(array [1,2,3], 4); array_append -------------- {1,2,3,4} SELECT array_remove(array [1,2,3,4,4,4], 4); array_remove -------------- {1,2,3} SELECT array_replace(array [1,2,3,4,4,4], 4, 5); array_replace --------------- {1,2,3,5,5,5} IN, NOT IN, ANY SELECT 20 in (1, 2, 3, 20) as \"result\"; -- t SELECT 25 in (1, 2, 3, 20) as \"result\"; -- f SELECT 25 not in (1, 2, 3, 20) as \"result\"; -- t SELECT 20 = all (Array [20,22]), 20 = all (array [20,20]); -- f SELECT 20 = any (Array [1,2,25]) as \"result\"; -- f STRING TO Array SELECT string_to_array('1,2,3,4,5', ','); string_to_array ----------------- {1,2,3,4,5} SELECT string_to_array('1,2,3,4,5,ABC', ',', 'ABC'); string_to_array ------------------ {1,2,3,4,5,NULL} SELECT string_to_array('1,2,3,4,,6', ',', ''); string_to_array ------------------ {1,2,3,4,NULL,6} SELECT array_to_string(ARRAY [1,2,3,4], '|'); array_to_string ----------------- 1|2|3|4 SELECT array_to_string(ARRAY [1,2,3,4,NULL], '|', 'EMPTY'); array_to_string ----------------- 1|2|3|4|EMPTY","title":"Arrays"},{"location":"sql/data-types/arrays/#arrays","text":"","title":"Arrays"},{"location":"sql/data-types/arrays/#arrays_1","text":"Original Documentation : here","title":"Arrays"},{"location":"sql/data-types/arrays/#syntax","text":"column_name DATATYPE[] {CONSTRAINT} CREATE TABLE table_array ( id SERIAL, name varchar(100), grades text[] ); INSERT INTO table_array (name, grades) VALUES ('person 1', array ['100','45']); INSERT INTO table_array (name, grades) VALUES ('person 2', array ['100','90']); INSERT INTO table_array (name, grades) VALUES ('person 3', array ['100','97']); INSERT INTO table_array (name, grades) VALUES ('person 4', array ['100','94']); SELECT name, grades[1] FROM table_array; name | grades ----------+-------- person 1 | 100 person 2 | 100 person 3 | 100 person 4 | 100","title":"Syntax"},{"location":"sql/data-types/arrays/#array-in-tables","text":"","title":"Array in Tables"},{"location":"sql/data-types/arrays/#insert","text":"for non text data , use {value1,value2} or array ['value1','value2'] for text data , use {\"value1\",\"value2\"} or array [value1,value2] CREATE TABLE teachers ( id serial primary key, class text[] ); CREATE TABLE IF NOT EXISTS teachers ( id serial primary key, class text array ); INSERT INTO teachers (class) VALUES (array ['english','maths']); id | class ----+----------------- 1 | {english,maths}","title":"Insert"},{"location":"sql/data-types/arrays/#query","text":"SELECT class[1] FROM teachers; class --------- english SELECT * FROM teachers WHERE class[1] = 'english'; id | class ----+----------------- 1 | {english,maths} SELECT * FROM teachers WHERE 'english' = any (class); id | class ----+----------------- 1 | {english,maths}","title":"Query"},{"location":"sql/data-types/arrays/#update","text":"update teachers set class[1] = 'dutch' WHERE id = 1; id | class ----+--------------- 1 | {dutch,maths} Update teachers set class[3] = 'science' WHERE id = 1; id | class ----+----------------------- 1 | {dutch,maths,science}","title":"Update"},{"location":"sql/data-types/arrays/#dimensionless","text":"CREATE TABLE teacher2 ( id serial primary key, class text array[1] ); INSERT INTO teacher2 (class) VALUES (array ['english']); id | class ----+----------- 1 | {english} -- dimensions doesnt matter INSERT INTO teacher2 (class) VALUES (array ['english','hindi']); id | class ----+----------------- 1 | {english} 2 | {english,hindi}","title":"Dimensionless"},{"location":"sql/data-types/arrays/#unnest","text":"SELECT id, class, unnest(class) FROM teacher2; id | class | unnest ----+-----------------+--------- 1 | {english} | english 2 | {english,hindi} | english 2 | {english,hindi} | hindi","title":"Unnest"},{"location":"sql/data-types/arrays/#multi-dimensional-array","text":"CREATE TABLE students ( id serial primary key, name varchar(50) not null, grade integer[][] ); INSERT INTO students (name, grade) VALUES ('s1', '{90,2020}'), ('s1', '{70,2020}'), ('s1', '{60,2020}'); SELECT * FROM students; id | name | grade ----+------+----------- 1 | s1 | {90,2020} 2 | s1 | {70,2020} 3 | s1 | {60,2020} SELECT * FROM students WHERE grade @> '{90}'; id | name | grade ----+------+----------- 1 | s1 | {90,2020} SELECT * FROM students WHERE '2020' = any (grade); id | name | grade ----+------+----------- 1 | s1 | {90,2020} 2 | s1 | {70,2020} 3 | s1 | {60,2020} SELECT * FROM students WHERE grade[1] < 80; id | name | grade ----+------+----------- 2 | s1 | {70,2020} 3 | s1 | {60,2020}","title":"Multi Dimensional Array"},{"location":"sql/data-types/arrays/#array-vs-jsonb","text":"","title":"Array vs JSONB"},{"location":"sql/data-types/arrays/#advantages-to-array","text":"It's pretty easy to setup Requires less storage than jsonb It has multi dimensional support Indexing through GIN, greatly speeds up query The PostgreSQL planner is likely to make better decisions with PostgreSQL array, as it collects statistics on its content, but not with JSONB.","title":"Advantages to Array"},{"location":"sql/data-types/arrays/#disadvantages-to-array","text":"Its main advantages is that you are limited to one data type Have to follow strict order of the array data input.","title":"Disadvantages to Array"},{"location":"sql/data-types/arrays/#advantages-to-jsonb","text":"Provides additional operators for querying Support for indexing","title":"Advantages to JSONB"},{"location":"sql/data-types/arrays/#disadvantages-to-jsonb","text":"Has to parse the json data to binary format slow in writing, but faster in reading Doesn't maintain order","title":"Disadvantages to JSONB"},{"location":"sql/data-types/arrays/#ranges","text":"SELECT INT4RANGE(1, 6) AS \"DEFAULT [(\", NUMRANGE(1.432, 6.238, '[]') AS \"[]\", DATERANGE('20200101', '20201222', '()') AS \"DATES ()\", TSRANGE(LOCALTIMESTAMP, LOCALTIMESTAMP + INTERVAL '8 DAYS', '(]') AS \"OPENED CLOSED\"; DEFAULT [( | [] | DATES () | OPENED CLOSED ------------+---------------+-------------------------+----------------------------------------------------------- [1,6) | [1.432,6.238] | [2020-01-02,2020-12-22) | (\"2021-08-24 05:22:13.03625\",\"2021-09-01 05:22:13.03625\"] SELECT ARRAY [1,2,3] AS \"INT ARRAYS\", ARRAY [2.123::FLOAT] AS \"FLOATING NUMBERS\", ARRAY [CURRENT_DATE, CURRENT_DATE + 5]; INT ARRAYS | FLOATING NUMBERS | array ------------+------------------+------------------------- {1,2,3} | {2.123} | {2021-08-24,2021-08-29} SELECT ARRAY [1,2,3,4] = ARRAY [1,2,3,4], ARRAY [1,2,3,4] = ARRAY [1,1,3,4], ARRAY [1,2,3,4] <> ARRAY [1,2,3,4], ARRAY [1,2,3,4] < ARRAY [1,5,3,4], ARRAY [1,2,3,4] <= ARRAY [1,3,3,4], ARRAY [1,2,3,4] > ARRAY [1,2,3,4]; ?column? | ?column? | ?column? | ?column? | ?column? | ?column? ----------+----------+----------+----------+----------+---------- t | f | f | t | t | f","title":"Ranges"},{"location":"sql/data-types/arrays/#inclusion-operators","text":"SELECT ARRAY [1,2,3,4] @> ARRAY [2,3,4] AS \"CONTAINS\", ARRAY ['A','B'] <@ ARRAY ['A','B','C'] AS \"CONTAINED BY\", ARRAY [1,2,3,4] && ARRAY [2,3,4] AS \"IS OVERLAP\"; CONTAINS | CONTAINED BY | IS OVERLAP ----------+--------------+------------ t | t | t","title":"Inclusion Operators"},{"location":"sql/data-types/arrays/#length-and-dimensions","text":"SELECT ARRAY [1,2,3] || ARRAY [4,5,6] AS \"COMBINED ARRAY\"; COMBINED ARRAY ---------------- {1,2,3,4,5,6} SELECT ARRAY_CAT(ARRAY [1,2,3], ARRAY [4,5,6]) AS \"COMBINED ARRAY VIA CAT\"; COMBINED ARRAY VIA CAT ------------------------ {1,2,3,4,5,6} SELECT 4 || ARRAY [1,2,3] AS \"ADDING TO ARRAY\"; ADDING TO ARRAY ----------------- {4,1,2,3} SELECT ARRAY [1,2,3] || 4 AS \"ADDING TO ARRAY\"; ADDING TO ARRAY ----------------- {1,2,3,4} SELECT ARRAY_APPEND(ARRAY [1,2,3], 4) AS \"USING APPEND\"; USING APPEND -------------- {1,2,3,4} SELECT ARRAY_PREPEND(4, ARRAY [1,2,3]) AS \"USING APPEND\"; USING APPEND -------------- {4,1,2,3} SELECT ARRAY_NDIMS(ARRAY [[1,2,3,4],[1,2,3,4],[1,2,3,4]]) AS \"DIMENSIONS\", ARRAY_DIMS(ARRAY [1,2,3,4,2,3,4]) AS \"DIMENSIONS\"; DIMENSIONS | DIMENSIONS ------------+------------ 2 | [1:7] SELECT ARRAY_LENGTH(ARRAY [-111,2,3,4], 1); array_length -------------- 4 SELECT ARRAY_UPPER(ARRAY [1,2,3,4000], 1), ARRAY_LOWER(ARRAY [-100,2,3,4], 1); array_upper | array_lower -------------+------------- 4 | 1","title":"Length and Dimensions"},{"location":"sql/data-types/arrays/#positions","text":"SELECT array_position(array ['jan','feb','mar'], 'feb'); array_position ---------------- 2 SELECT array_position(array [1,2,2,3,4], 2, 3); array_position ---------------- 3 SELECT array_positions(array [1,2,2,3,4], 2); array_positions ----------------- {2,3}","title":"Positions"},{"location":"sql/data-types/arrays/#search-replace-remove","text":"SELECT array_cat(array [1,2], array [3,4]); array_cat ----------- {1,2,3,4} SELECT array_append(array [1,2,3], 4); array_append -------------- {1,2,3,4} SELECT array_remove(array [1,2,3,4,4,4], 4); array_remove -------------- {1,2,3} SELECT array_replace(array [1,2,3,4,4,4], 4, 5); array_replace --------------- {1,2,3,5,5,5}","title":"Search, Replace, Remove"},{"location":"sql/data-types/arrays/#in-not-in-any","text":"SELECT 20 in (1, 2, 3, 20) as \"result\"; -- t SELECT 25 in (1, 2, 3, 20) as \"result\"; -- f SELECT 25 not in (1, 2, 3, 20) as \"result\"; -- t SELECT 20 = all (Array [20,22]), 20 = all (array [20,20]); -- f SELECT 20 = any (Array [1,2,25]) as \"result\"; -- f","title":"IN, NOT IN, ANY"},{"location":"sql/data-types/arrays/#string-to-array","text":"SELECT string_to_array('1,2,3,4,5', ','); string_to_array ----------------- {1,2,3,4,5} SELECT string_to_array('1,2,3,4,5,ABC', ',', 'ABC'); string_to_array ------------------ {1,2,3,4,5,NULL} SELECT string_to_array('1,2,3,4,,6', ',', ''); string_to_array ------------------ {1,2,3,4,NULL,6} SELECT array_to_string(ARRAY [1,2,3,4], '|'); array_to_string ----------------- 1|2|3|4 SELECT array_to_string(ARRAY [1,2,3,4,NULL], '|', 'EMPTY'); array_to_string ----------------- 1|2|3|4|EMPTY","title":"STRING TO Array"},{"location":"sql/data-types/date-time-stamps/","text":"Date/Time/Stamps Date/Time/Stamps Set Date Time Style -- show system date style SHOW datestyle; -- set new datestyle SET datestyle = 'ISO, DMY'; SET datestyle = 'ISO, MDY'; Make SELECT MAKE_DATE (2020,01,01); make_date ------------ 2020-01-01 SELECT MAKE_DATE (2020,01,01); make_date ------------ 2020-01-01 SELECT MAKE_TIME(2,3,14.65); make_time ------------- 02:03:14.65 SELECT MAKE_TIMESTAMP (2020,02,02,10,20,45.44); make_timestamp ------------------------ 2020-02-02 10:20:45.44 Make_interval SELECT MAKE_INTERVAL (2020,01,02,10,20,33); make_interval ----------------------------------- 2020 years 1 mon 24 days 20:33:00 SELECT MAKE_INTERVAL (days => 10); make_interval --------------- 10 days SELECT MAKE_INTERVAL (months => 7, days => 10, mins=>35); make_interval ------------------------- 7 mons 10 days 00:35:00 SELECT MAKE_INTERVAL (weeks => 10); make_interval --------------- 70 days Make_timestamptz SELECT make_timestamptz(2020,02,02,10,30,45.55,'Asia/Calcutta'); make_timestamptz --------------------------- 2020-02-02 05:00:45.55+00 SELECT pg_typeof(make_timestamptz(2020,02,02,10,30,45.55)); pg_typeof -------------------------- timestamp with time zone Date Value Extractor https://www.postgresql.org/docs/8.1/functions-datetime.html https://www.postgresqltutorial.com/postgresql-extract/ Extract select extract ('day' FROM current_timestamp), extract ('month' FROM current_timestamp), extract ('year' FROM current_timestamp); date_part | date_part | date_part -----------+-----------+----------- 14 | 8 | 2021 select extract('epoch' FROM current_timestamp); date_part ------------------- 1628923887.158532 select extract('century' FROM current_timestamp); date_part ----------- 21 Maths Operations on Date Time select '2020-02-02'::date + 04; ?column? ------------ 2020-02-06 select '23:59:59' + INTERVAL '1 SECOND'; ?column? ---------- 24:00:00 select '23:59:59' + INTERVAL '2 SECOND'; ?column? ---------- 24:00:01 SELECT CURRENT_TIMESTAMP + '01:01:01'; ?column? ------------------------------- 2021-08-14 07:53:05.444791+00 SELECT DATE '20200101' + TIME '10:25:10'; ?column? --------------------- 2020-01-01 10:25:10 SELECT '10:10:10' + TIME '10:25:10'; ?column? ---------- 20:35:20 SELECT DATE '20200101' - INTERVAL '1 HOUR'; ?column? --------------------- 2019-12-31 23:00:00 SELECT INTERVAL '30 MINUTES' + '2 HOUR'; ?column? ---------- 02:30:00 Overlap select ( DATE '2020-01-01' , DATE '2020-12-31' ) OVERLAPS ( DATE '2020-12-30', DATE '2020-12-01' ); overlaps ---------- t Current select current_date, current_time, current_time(2), current_timestamp; current_date | current_time | current_time | current_timestamp 2021-08-14 | 06:53:52.187847+00 | 06:53:52.19+00 | 2021-08-14 06:53:52.187847+00 select localtime, localtimestamp, localtimestamp(2); localtime | localtimestamp | localtimestamp -----------------+----------------------------+------------------------ 06:54:07.540777 | 2021-08-14 06:54:07.540777 | 2021-08-14 06:54:07.54 select now(), transaction_timestamp(), clock_timestamp(); now | transaction_timestamp | clock_timestamp 2021-08-14 06:54:31.371838+00 | 2021-08-14 06:54:31.371838+00 | 2021-08-14 06:54:31.371924+00 select statement_timestamp(), timeofday(); statement_timestamp | timeofday -------------------------------+------------------------------------- 2021-08-14 06:55:07.202782+00 | Sat Aug 14 06:55:07.202849 2021 UTC Age select age('2020-01-01', '2019-10-01'); age -------- 3 mons select age(timestamp '2020-01-01'); age ----------------------- 1 year 7 mons 13 days select age(current_date, '2020-01-01'); age ----------------------- 1 year 7 mons 13 days Epochs select age ( timestamp '2020-12-20', timestamp '2020-10-20' ); age -------- 2 mons SELECT EXTRACT (EPOCH FROM TIMESTAMPTZ '2020-10-20') - EXTRACT (EPOCH FROM TIMESTAMPTZ '2020-08-20') AS \"DIFFERENCE IN SECONDS\"; DIFFERENCE IN SECONDS ----------------------- 5270400 Timezone SELECT * FROM pg_timezone_names; SELECT * FROM pg_timezone_abbrevs; SHOW TIME ZONE; SET TIME ZONE 'Asia/Calcutta'; date_part and date_trunc SELECT date_part ('day', date '2021-11-07'); date_part ----------- 7 SELECT date_trunc('hour', timestamptz '2021-07-16 23:38:40.775719 +05:30'); date_trunc ------------------------ 2021-07-16 18:00:00+00","title":"Date/Time/Stamps"},{"location":"sql/data-types/date-time-stamps/#datetimestamps","text":"","title":"Date/Time/Stamps"},{"location":"sql/data-types/date-time-stamps/#datetimestamps_1","text":"","title":"Date/Time/Stamps"},{"location":"sql/data-types/date-time-stamps/#set-date-time-style","text":"-- show system date style SHOW datestyle; -- set new datestyle SET datestyle = 'ISO, DMY'; SET datestyle = 'ISO, MDY';","title":"Set Date Time Style"},{"location":"sql/data-types/date-time-stamps/#make","text":"SELECT MAKE_DATE (2020,01,01); make_date ------------ 2020-01-01 SELECT MAKE_DATE (2020,01,01); make_date ------------ 2020-01-01 SELECT MAKE_TIME(2,3,14.65); make_time ------------- 02:03:14.65 SELECT MAKE_TIMESTAMP (2020,02,02,10,20,45.44); make_timestamp ------------------------ 2020-02-02 10:20:45.44","title":"Make"},{"location":"sql/data-types/date-time-stamps/#make_interval","text":"SELECT MAKE_INTERVAL (2020,01,02,10,20,33); make_interval ----------------------------------- 2020 years 1 mon 24 days 20:33:00 SELECT MAKE_INTERVAL (days => 10); make_interval --------------- 10 days SELECT MAKE_INTERVAL (months => 7, days => 10, mins=>35); make_interval ------------------------- 7 mons 10 days 00:35:00 SELECT MAKE_INTERVAL (weeks => 10); make_interval --------------- 70 days","title":"Make_interval"},{"location":"sql/data-types/date-time-stamps/#make_timestamptz","text":"SELECT make_timestamptz(2020,02,02,10,30,45.55,'Asia/Calcutta'); make_timestamptz --------------------------- 2020-02-02 05:00:45.55+00 SELECT pg_typeof(make_timestamptz(2020,02,02,10,30,45.55)); pg_typeof -------------------------- timestamp with time zone","title":"Make_timestamptz"},{"location":"sql/data-types/date-time-stamps/#date-value-extractor","text":"https://www.postgresql.org/docs/8.1/functions-datetime.html https://www.postgresqltutorial.com/postgresql-extract/","title":"Date Value Extractor"},{"location":"sql/data-types/date-time-stamps/#extract","text":"select extract ('day' FROM current_timestamp), extract ('month' FROM current_timestamp), extract ('year' FROM current_timestamp); date_part | date_part | date_part -----------+-----------+----------- 14 | 8 | 2021 select extract('epoch' FROM current_timestamp); date_part ------------------- 1628923887.158532 select extract('century' FROM current_timestamp); date_part ----------- 21","title":"Extract"},{"location":"sql/data-types/date-time-stamps/#maths-operations-on-date-time","text":"select '2020-02-02'::date + 04; ?column? ------------ 2020-02-06 select '23:59:59' + INTERVAL '1 SECOND'; ?column? ---------- 24:00:00 select '23:59:59' + INTERVAL '2 SECOND'; ?column? ---------- 24:00:01 SELECT CURRENT_TIMESTAMP + '01:01:01'; ?column? ------------------------------- 2021-08-14 07:53:05.444791+00 SELECT DATE '20200101' + TIME '10:25:10'; ?column? --------------------- 2020-01-01 10:25:10 SELECT '10:10:10' + TIME '10:25:10'; ?column? ---------- 20:35:20 SELECT DATE '20200101' - INTERVAL '1 HOUR'; ?column? --------------------- 2019-12-31 23:00:00 SELECT INTERVAL '30 MINUTES' + '2 HOUR'; ?column? ---------- 02:30:00","title":"Maths Operations on Date Time"},{"location":"sql/data-types/date-time-stamps/#overlap","text":"select ( DATE '2020-01-01' , DATE '2020-12-31' ) OVERLAPS ( DATE '2020-12-30', DATE '2020-12-01' ); overlaps ---------- t","title":"Overlap"},{"location":"sql/data-types/date-time-stamps/#current","text":"select current_date, current_time, current_time(2), current_timestamp; current_date | current_time | current_time | current_timestamp 2021-08-14 | 06:53:52.187847+00 | 06:53:52.19+00 | 2021-08-14 06:53:52.187847+00 select localtime, localtimestamp, localtimestamp(2); localtime | localtimestamp | localtimestamp -----------------+----------------------------+------------------------ 06:54:07.540777 | 2021-08-14 06:54:07.540777 | 2021-08-14 06:54:07.54 select now(), transaction_timestamp(), clock_timestamp(); now | transaction_timestamp | clock_timestamp 2021-08-14 06:54:31.371838+00 | 2021-08-14 06:54:31.371838+00 | 2021-08-14 06:54:31.371924+00 select statement_timestamp(), timeofday(); statement_timestamp | timeofday -------------------------------+------------------------------------- 2021-08-14 06:55:07.202782+00 | Sat Aug 14 06:55:07.202849 2021 UTC","title":"Current"},{"location":"sql/data-types/date-time-stamps/#age","text":"select age('2020-01-01', '2019-10-01'); age -------- 3 mons select age(timestamp '2020-01-01'); age ----------------------- 1 year 7 mons 13 days select age(current_date, '2020-01-01'); age ----------------------- 1 year 7 mons 13 days","title":"Age"},{"location":"sql/data-types/date-time-stamps/#epochs","text":"select age ( timestamp '2020-12-20', timestamp '2020-10-20' ); age -------- 2 mons SELECT EXTRACT (EPOCH FROM TIMESTAMPTZ '2020-10-20') - EXTRACT (EPOCH FROM TIMESTAMPTZ '2020-08-20') AS \"DIFFERENCE IN SECONDS\"; DIFFERENCE IN SECONDS ----------------------- 5270400","title":"Epochs"},{"location":"sql/data-types/date-time-stamps/#timezone","text":"SELECT * FROM pg_timezone_names; SELECT * FROM pg_timezone_abbrevs; SHOW TIME ZONE; SET TIME ZONE 'Asia/Calcutta';","title":"Timezone"},{"location":"sql/data-types/date-time-stamps/#date_part-and-date_trunc","text":"SELECT date_part ('day', date '2021-11-07'); date_part ----------- 7 SELECT date_trunc('hour', timestamptz '2021-07-16 23:38:40.775719 +05:30'); date_trunc ------------------------ 2021-07-16 18:00:00+00","title":"date_part and date_trunc"},{"location":"sql/data-types/json/","text":"JSON JSON JSON vs JSONB JSON JSONB stores data in text format stores data in binary format stores data AS-is trims of white spaces slower in operations fASter in operations doesn't support full text indexing supports full text indexing SELECT '{ \"title\":\"book 1\"} '::json; json ----------------------- { + \"title\":\"book 1\"}+ (1 row) SELECT ' {\"title\":\"book 1\"} '::jsonb jsonb --------------------- {\"title\": \"book 1\"} (1 row) Operations CREATE TABLE books_jsonb ( id serial primary key, book_info JSONB ); INSERT INTO books_jsonb (book_info) VALUES ('{ \"title\": \"Book 1\" }'), ('{ \"title\": \"Book 2\" }'), ('{ \"title\": \"Book 3\" }'); id | title ----+-------- 1 | Book 1 2 | Book 2 3 | Book 3 SELECT id, book_info ->> 'title' AS \"title\" FROM books_jsonb WHERE book_info ->> 'title' = 'Book 1'; id | title ----+-------- 1 | Book 1 INSERT INTO books_jsonb (book_info) VALUES ('{ \"title\": \"Book 10\" }'); id | book_info ----+---------------------- 1 | {\"title\": \"Book 1\"} 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 10\"} UPDATE books_jsonb SET book_info = book_info || '{\"title\": \"Book 4\" }' WHERE book_info ->> 'title' = 'Book 10'; id | book_info ----+--------------------- 1 | {\"title\": \"Book 1\"} 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} UPDATE books_jsonb SET book_info = book_info || '{\"author\": \"author 1\" }' WHERE book_info ->> 'title' = 'Book 1'; id | book_info ----+------------------------------------------- 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} 1 | {\"title\": \"Book 1\", \"author\": \"author 1\"} UPDATE books_jsonb SET book_info = book_info - 'author' WHERE book_info ->> 'title' = 'Book 1'; id | book_info ----+--------------------- 1 | {\"title\": \"Book 1\"} 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} UPDATE books_jsonb SET book_info = book_info || '{\"available\":[\"new delhi\",\"Tokyo\",\"sydney\"]}' WHERE book_info ->> 'title' = 'Book 1'; id | book_info 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} 1 | {\"title\": \"Book 1\", \"author\": \"author 1\", \"available\": [\"new delhi\", \"Tokyo\", \"sydney\"]} UPDATE books_jsonb SET book_info = book_info #- '{available,1}' WHERE book_info ->> 'title' = 'Book 1'; id | Book_info 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} 1 | {\"title\": \"Book 1\", \"author\": \"author 1\", \"available\": [\"new delhi\", \"sydney\"]} ROW_TO_JSON() SELECT row_to_json(orders) FROM orders; {\"order_id\":10248,\"customer_id\":\"VINET\",\"employee_id\":5,\"order_date\":\"1996-07-04\",\"required_date\":\"1996-08-01\",\"shipped_date\":\"1996-07-16\",\"ship_via\":3,\"freight\":32.38,\"ship_name\":\"Vins et alcools Chevalier\",\"ship_address\":\"59 rue de l'Abbaye\",\"ship_city\":\"Reims\",\"ship_region\":null,\"ship_postal_code\":\"51100\",\"ship_country\":\"France\"} SELECT row_to_json(t) FROM ( SELECT * FROM orders ) AS t; {\"order_id\":10248,\"customer_id\":\"VINET\",\"employee_id\":5,\"order_date\":\"1996-07-04\",\"required_date\":\"1996-08-01\",\"shipped_date\":\"1996-07-16\",\"ship_via\":3,\"freight\":32.38,\"ship_name\":\"Vins et alcools Chevalier\",\"ship_address\":\"59 rue de l'Abbaye\",\"ship_city\":\"Reims\",\"ship_region\":null,\"ship_postal_code\":\"51100\",\"ship_country\":\"France\"} JSON_AGG() SELECT * FROM orders; SELECT director_id, first_name, lASt_name, ( SELECT json_agg(x) FROM ( SELECT movie_name FROM movies mv WHERE mv.director_id = directors.director_id ) AS x ) :: jsonb FROM directors; JSON_BUILD SELECT json_build_array(1, 2, 3, 4, 5, 6); json_build_array -------------------- [1, 2, 3, 4, 5, 6] SELECT json_build_array(1, 2, 3, 4, 5, 6, 'Hi'); json_build_array -------------------------- [1, 2, 3, 4, 5, 6, \"Hi\"] -- error : argument list must have even number of elements SELECT json_build_object(1, 2, 3, 4, 5); SELECT json_build_object(1, 2, 3, 4, 5, 6, 7, 'Hi'); json_build_object ----------------------------------------- {\"1\" : 2, \"3\" : 4, \"5\" : 6, \"7\" : \"Hi\"} SELECT json_object('{name,email}', '{\"adnan\",\"a@b.com\"}'); json_object ----------------------------------------- {\"name\" : \"adnan\", \"email\" : \"a@b.com\"} Json Functions CREATE TABLE directors_docs ( id serial primary key, body jsonb ); SELECT director_id, first_name, last_name, ( SELECT json_agg(x) AS all_movies FROM ( SELECT movie_name FROM movies mv WHERE mv.director_id = directors.director_id ) x ) :: jsonb FROM directors; INSERT INTO directors_docs (body) SELECT row_to_json(a) FROM ( SELECT director_id, first_name, last_name, ( SELECT json_agg(x) AS all_movies FROM ( SELECT movie_name FROM movies mv WHERE mv.director_id = directors.director_id ) x ) :: jsonb FROM directors ) AS a; SELECT * FROM directors_docs LIMIT 3; 1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} 2 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"There Will Be Blood\"}], \"first_name\": \"Paul\", \"director_id\": 2} 3 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"Grand Budapest Hotel\"}, {\"movie_name\": \"Rushmore\"}, {\"movie_name\": \"The Darjeeling Limited\"}], \"first_name\": \"Wes\", \"director_id\": 3} SELECT *, jsonb_array_length(body -> 'all_movies') AS total_movies FROM directors_docs order by jsonb_array_length(body->'all_movies') DESC; 13 | {\"last_name\": \"Kubrick\", \"all_movies\": [{\"movie_name\": \"A Clockwork Orange\"}, {\"movie_name\": \"Eyes Wide Shut\"}, {\"movie_name\": \"The Shining\"}], \"first_name\": \"Stanley\", \"director_id\": 13} | 3 3 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"Grand Budapest Hotel\"}, {\"movie_name\": \"Rushmore\"}, {\"movie_name\": \"The Darjeeling Limited\"}], \"first_name\": \"Wes\", \"director_id\": 3} | 3 17 | {\"last_name\": \"Lucas\", \"all_movies\": [{\"movie_name\": \"Star Wars: A New Hope\"}, {\"movie_name\": \"Star Wars: Empire Strikes Back\"}, {\"movie_name\": \"Star Wars: Return of the Jedi\"}], \"first_name\": \"George\", \"director_id\": 17} | 3 SELECT *,jsonb_object_keys(body) FROM directors_docs; 1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | last_name 1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | all_movies 1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | first_name SELECT j.key, j.value FROM directors_docs, jsonb_each(body) j; key | value ------------+------------------------------------------ last_name | \"Alfredson\" all_movies | [{\"movie_name\": \"Let the Right One In\"}] first_name | \"Tomas\" Existence Operators SELECT * FROM directors_docs WHERE body -> 'first_name' ? 'John'; 14 | {\"last_name\": \"Lasseter\", \"all_movies\": [{\"movie_name\": \"Toy Story\"}], \"first_name\": \"John\", \"director_id\": 14} Searching JSON SELECT * FROM directors_docs WHERE body @> '{\"first_name\":\"John\"}'; SELECT * FROM directors_docs WHERE body @> '{\"director_id\":1}'; -- error : No operator matches the given name and argument types. You might need to add explicit type casts. SELECT * FROM directors_docs WHERE body -> 'first_name' LIKE 'J%'; SELECT * FROM directors_docs WHERE body ->> 'first_name' LIKE 'J%'; SELECT * FROM directors_docs WHERE (body ->> 'director_id')::integer in (1,2,3,4,5,10);","title":"JSON"},{"location":"sql/data-types/json/#json","text":"","title":"JSON"},{"location":"sql/data-types/json/#json_1","text":"","title":"JSON"},{"location":"sql/data-types/json/#json-vs-jsonb","text":"JSON JSONB stores data in text format stores data in binary format stores data AS-is trims of white spaces slower in operations fASter in operations doesn't support full text indexing supports full text indexing SELECT '{ \"title\":\"book 1\"} '::json; json ----------------------- { + \"title\":\"book 1\"}+ (1 row) SELECT ' {\"title\":\"book 1\"} '::jsonb jsonb --------------------- {\"title\": \"book 1\"} (1 row)","title":"JSON vs JSONB"},{"location":"sql/data-types/json/#operations","text":"CREATE TABLE books_jsonb ( id serial primary key, book_info JSONB ); INSERT INTO books_jsonb (book_info) VALUES ('{ \"title\": \"Book 1\" }'), ('{ \"title\": \"Book 2\" }'), ('{ \"title\": \"Book 3\" }'); id | title ----+-------- 1 | Book 1 2 | Book 2 3 | Book 3 SELECT id, book_info ->> 'title' AS \"title\" FROM books_jsonb WHERE book_info ->> 'title' = 'Book 1'; id | title ----+-------- 1 | Book 1 INSERT INTO books_jsonb (book_info) VALUES ('{ \"title\": \"Book 10\" }'); id | book_info ----+---------------------- 1 | {\"title\": \"Book 1\"} 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 10\"} UPDATE books_jsonb SET book_info = book_info || '{\"title\": \"Book 4\" }' WHERE book_info ->> 'title' = 'Book 10'; id | book_info ----+--------------------- 1 | {\"title\": \"Book 1\"} 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} UPDATE books_jsonb SET book_info = book_info || '{\"author\": \"author 1\" }' WHERE book_info ->> 'title' = 'Book 1'; id | book_info ----+------------------------------------------- 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} 1 | {\"title\": \"Book 1\", \"author\": \"author 1\"} UPDATE books_jsonb SET book_info = book_info - 'author' WHERE book_info ->> 'title' = 'Book 1'; id | book_info ----+--------------------- 1 | {\"title\": \"Book 1\"} 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} UPDATE books_jsonb SET book_info = book_info || '{\"available\":[\"new delhi\",\"Tokyo\",\"sydney\"]}' WHERE book_info ->> 'title' = 'Book 1'; id | book_info 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} 1 | {\"title\": \"Book 1\", \"author\": \"author 1\", \"available\": [\"new delhi\", \"Tokyo\", \"sydney\"]} UPDATE books_jsonb SET book_info = book_info #- '{available,1}' WHERE book_info ->> 'title' = 'Book 1'; id | Book_info 2 | {\"title\": \"Book 2\"} 3 | {\"title\": \"Book 3\"} 4 | {\"title\": \"Book 4\"} 1 | {\"title\": \"Book 1\", \"author\": \"author 1\", \"available\": [\"new delhi\", \"sydney\"]}","title":"Operations"},{"location":"sql/data-types/json/#row_to_json","text":"SELECT row_to_json(orders) FROM orders; {\"order_id\":10248,\"customer_id\":\"VINET\",\"employee_id\":5,\"order_date\":\"1996-07-04\",\"required_date\":\"1996-08-01\",\"shipped_date\":\"1996-07-16\",\"ship_via\":3,\"freight\":32.38,\"ship_name\":\"Vins et alcools Chevalier\",\"ship_address\":\"59 rue de l'Abbaye\",\"ship_city\":\"Reims\",\"ship_region\":null,\"ship_postal_code\":\"51100\",\"ship_country\":\"France\"} SELECT row_to_json(t) FROM ( SELECT * FROM orders ) AS t; {\"order_id\":10248,\"customer_id\":\"VINET\",\"employee_id\":5,\"order_date\":\"1996-07-04\",\"required_date\":\"1996-08-01\",\"shipped_date\":\"1996-07-16\",\"ship_via\":3,\"freight\":32.38,\"ship_name\":\"Vins et alcools Chevalier\",\"ship_address\":\"59 rue de l'Abbaye\",\"ship_city\":\"Reims\",\"ship_region\":null,\"ship_postal_code\":\"51100\",\"ship_country\":\"France\"}","title":"ROW_TO_JSON()"},{"location":"sql/data-types/json/#json_agg","text":"SELECT * FROM orders; SELECT director_id, first_name, lASt_name, ( SELECT json_agg(x) FROM ( SELECT movie_name FROM movies mv WHERE mv.director_id = directors.director_id ) AS x ) :: jsonb FROM directors;","title":"JSON_AGG()"},{"location":"sql/data-types/json/#json_build","text":"SELECT json_build_array(1, 2, 3, 4, 5, 6); json_build_array -------------------- [1, 2, 3, 4, 5, 6] SELECT json_build_array(1, 2, 3, 4, 5, 6, 'Hi'); json_build_array -------------------------- [1, 2, 3, 4, 5, 6, \"Hi\"] -- error : argument list must have even number of elements SELECT json_build_object(1, 2, 3, 4, 5); SELECT json_build_object(1, 2, 3, 4, 5, 6, 7, 'Hi'); json_build_object ----------------------------------------- {\"1\" : 2, \"3\" : 4, \"5\" : 6, \"7\" : \"Hi\"} SELECT json_object('{name,email}', '{\"adnan\",\"a@b.com\"}'); json_object ----------------------------------------- {\"name\" : \"adnan\", \"email\" : \"a@b.com\"}","title":"JSON_BUILD"},{"location":"sql/data-types/json/#json-functions","text":"CREATE TABLE directors_docs ( id serial primary key, body jsonb ); SELECT director_id, first_name, last_name, ( SELECT json_agg(x) AS all_movies FROM ( SELECT movie_name FROM movies mv WHERE mv.director_id = directors.director_id ) x ) :: jsonb FROM directors; INSERT INTO directors_docs (body) SELECT row_to_json(a) FROM ( SELECT director_id, first_name, last_name, ( SELECT json_agg(x) AS all_movies FROM ( SELECT movie_name FROM movies mv WHERE mv.director_id = directors.director_id ) x ) :: jsonb FROM directors ) AS a; SELECT * FROM directors_docs LIMIT 3; 1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} 2 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"There Will Be Blood\"}], \"first_name\": \"Paul\", \"director_id\": 2} 3 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"Grand Budapest Hotel\"}, {\"movie_name\": \"Rushmore\"}, {\"movie_name\": \"The Darjeeling Limited\"}], \"first_name\": \"Wes\", \"director_id\": 3} SELECT *, jsonb_array_length(body -> 'all_movies') AS total_movies FROM directors_docs order by jsonb_array_length(body->'all_movies') DESC; 13 | {\"last_name\": \"Kubrick\", \"all_movies\": [{\"movie_name\": \"A Clockwork Orange\"}, {\"movie_name\": \"Eyes Wide Shut\"}, {\"movie_name\": \"The Shining\"}], \"first_name\": \"Stanley\", \"director_id\": 13} | 3 3 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"Grand Budapest Hotel\"}, {\"movie_name\": \"Rushmore\"}, {\"movie_name\": \"The Darjeeling Limited\"}], \"first_name\": \"Wes\", \"director_id\": 3} | 3 17 | {\"last_name\": \"Lucas\", \"all_movies\": [{\"movie_name\": \"Star Wars: A New Hope\"}, {\"movie_name\": \"Star Wars: Empire Strikes Back\"}, {\"movie_name\": \"Star Wars: Return of the Jedi\"}], \"first_name\": \"George\", \"director_id\": 17} | 3 SELECT *,jsonb_object_keys(body) FROM directors_docs; 1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | last_name 1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | all_movies 1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | first_name SELECT j.key, j.value FROM directors_docs, jsonb_each(body) j; key | value ------------+------------------------------------------ last_name | \"Alfredson\" all_movies | [{\"movie_name\": \"Let the Right One In\"}] first_name | \"Tomas\"","title":"Json Functions"},{"location":"sql/data-types/json/#existence-operators","text":"SELECT * FROM directors_docs WHERE body -> 'first_name' ? 'John'; 14 | {\"last_name\": \"Lasseter\", \"all_movies\": [{\"movie_name\": \"Toy Story\"}], \"first_name\": \"John\", \"director_id\": 14}","title":"Existence Operators"},{"location":"sql/data-types/json/#searching-json","text":"SELECT * FROM directors_docs WHERE body @> '{\"first_name\":\"John\"}'; SELECT * FROM directors_docs WHERE body @> '{\"director_id\":1}'; -- error : No operator matches the given name and argument types. You might need to add explicit type casts. SELECT * FROM directors_docs WHERE body -> 'first_name' LIKE 'J%'; SELECT * FROM directors_docs WHERE body ->> 'first_name' LIKE 'J%'; SELECT * FROM directors_docs WHERE (body ->> 'director_id')::integer in (1,2,3,4,5,10);","title":"Searching JSON"},{"location":"sql/data-types/miscellaneous-1/","text":"Internal Functions Order of execution of SQL statements FROM WHERE SELECT ORDER BY Concatenation Operator select concat(first_name,last_name) as full_name from directors limit 10; full_name ---------------- TomasAlfredson PaulAnderson WesAnderson select concat_ws(' ',first_name,last_name) as full_name from directors limit 3; full_name ----------------- Tomas Alfredson Paul Anderson Wes Anderson if you can have a null value in column, always use concat_ws because it will place nothing in that and and also not place the spacer like | or a space Type Conversion Type of Conversion Notes Implicit data conversion is done AUTOMATICALLY Explicit data conversion is done via 'conversion functions' eg. CAST or :: SELECT * FROM movies; -- exact datatype match : no conversion SELECT * FROM movies WHERE movie_id = 1; -- Implicit conversion : conversion SELECT * FROM movies WHERE movie_id = '1'; -- Explicit conversion : conversion SELECT * FROM movies WHERE movie_id = integer '1'; -- Output of all queries above movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id ----------+--------------------+--------------+------------+--------------+-----------------+------------- 1 | A Clockwork Orange | 112 | English | 1972-02-02 | 18 | 13 Casting -- CAST function -- Syntax : CAST ( expression as target_data_type ); SELECT CAST ( '10' AS INTEGER ); int4 ------ 10 SELECT CAST ('2020-02-02' AS DATE), CAST('01-FEB-2001' AS DATE); date | date ------------+------------ 2020-02-02 | 2001-02-01 SELECT CAST ( 'true' AS BOOLEAN ), CAST ( '1' AS BOOLEAN ), CAST ( '0' AS BOOLEAN ); bool | bool | bool ------+------+------ t | t | f SELECT CAST ( '14.87789' AS DOUBLE PRECISION ); float8 ---------- 14.87789 SELECT '2020-02-02'::DATE , '01-FEB-2001'::DATE; date | date ------------+------------ 2020-02-02 | 2001-02-01 SELECT '2020-02-02 10:20:10.23'::TIMESTAMP; timestamp ------------------------ 2020-02-02 10:20:10.23 SELECT '2020-02-02 10:20:10.23 +05:30'::TIMESTAMPTZ; timestamptz --------------------------- 2020-02-02 04:50:10.23+00 SELECT '10 minute'::interval, '10 hour'::interval, '10 day'::interval, '10 week'::interval, '10 month'::interval; interval | interval | interval | interval | interval ----------+----------+----------+----------+---------- 00:10:00 | 10:00:00 | 10 days | 70 days | 10 mons SELECT 20! AS \"result 1\" , CAST( 20 AS bigint ) ! AS \"result 2\"; result 1 | result 2 ---------------------+--------------------- 2432902008176640000 | 2432902008176640000 SELECT ROUND(10,4) AS \"result 1\", ROUND ( CAST (10 AS NUMERIC) ) AS \"result 2\", ROUND ( CAST (10 AS NUMERIC) , 4 ) AS \"result 3\"; result 1 | result 2 | result 3 ----------+----------+---------- 10.0000 | 10 | 10.0000 SELECT SUBSTR('12345',2) AS \"RESULT 1\", SUBSTR( CAST('12345' AS TEXT) ,2) AS \"RESULT 2\"; RESULT 1 | RESULT 2 ----------+---------- 2345 | 2345 CREATE TABLE ratings ( rating_id SERIAL PRIMARY KEY, rating VARCHAR(2) NOT NULL ); INSERT INTO ratings ( rating ) VALUES ('A'), ('B'), ('C'), ('D'), (1), (2), (3), (4); SELECT rating_id, CASE WHEN rating~E'^\\\\d+$' THEN CAST ( rating as INTEGER ) ELSE 0 END AS rating FROM ratings; rating_id | rating -----------+-------- 1 | 0 2 | 0 3 | 0 4 | 0 5 | 1 6 | 2 7 | 3 8 | 4 Formatting Functions https://www.postgresql.org/docs/12/functions-formatting.html to_char() Refer to the documentation https://www.postgresqltutorial.com/postgresql-to_char/ SELECT TO_CHAR ( 100870, '9,999999' ); to_char ----------- 100870 SELECT release_date, TO_CHAR(release_date,'DD-MM-YYYY'), TO_CHAR(release_date,'Dy, MM, YYYY') FROM movies LIMIT 3; release_date | to_char | to_char --------------+------------+--------------- 1972-02-02 | 02-02-1972 | Wed, 02, 1972 1979-08-15 | 15-08-1979 | Wed, 08, 1979 2001-01-04 | 04-01-2001 | Thu, 01, 2001 SELECT TO_CHAR ( TIMESTAMP '2020-01-01 13:32:30', 'HH24:MI:SS' ); to_char ---------- 13:32:30 to_number() https://www.postgresqltutorial.com/postgresql-to_number/ SELECT TO_NUMBER( '1420.89', '9999.' ); to_number ----------- 1420 SELECT TO_NUMBER( '10,625.78-', '99G999D99S' ); to_number ----------- -10625.78 SELECT TO_NUMBER( '$1,625.78+', '99G999D99S' ); to_number ----------- 1625.78 SELECT to_number( '$1,420.65' , 'L9G999D99' ); to_number ----------- 1420.65 SELECT to_number( '21,420.65' , '99G999D99' ); to_number ----------- 21420.65 to_date() https://www.postgresqltutorial.com/postgresql-to_date/ SELECT TO_DATE( '2020/10/22' , 'YYYY/MM/DD' ); to_date ------------ 2020-10-22 SELECT to_date( '022199' , 'MMDDYY' ); to_date ------------ 1999-02-21 SELECT to_date( 'March 07, 2019' , 'Month DD, YYYY' ); to_date ------------ 2019-03-07 to_timestamp() https://www.postgresqltutorial.com/postgresql-to_timestamp/ SELECT TO_TIMESTAMP( '2017-03-31 9:30:20', 'YYYY-MM-DD HH:MI:SS' ); to_timestamp ------------------------ 2017-03-31 09:30:20+00 SELECT TO_TIMESTAMP('2017 Aug','YYYY MON'); to_timestamp ------------------------ 2017-08-01 00:00:00+00 String Functions Upper(string) Lower(string) INITCAP(string) REVERSE(string) LPAD(string) RPAD(string) LENGTH(string) CHAR_LENGTH(string) : Same as Length POSITION( string in string ) STRPOS ( , < substring > ) SUBSTRING (string , length) REPLACE (string, from_string, to_string) SELECT INITCAP(first_name) as FirstName, INITCAP(last_name) as LastName FROM directors LIMIT 3; firstname | lastname -----------+----------- Tomas | Alfredson Paul | Anderson Wes | Anderson SELECT LEFT('Uday', 3), RIGHT('Uday', 3); left | right ------+------- Uda | day SELECT LEFT('Uday', -3), RIGHT('Uday', -3); left | right ------+------- U | y SELECT REVERSE('UDAY YADAV'); reverse ------------ VADAY YADU SELECT SPLIT_PART('1,2,3,4', ',', 1), SPLIT_PART('1|2|3|4', '|', 2); split_part | split_part ------------+------------ 1 | 2 SELECT TRIM(LEADING FROM ' Amazing PostgreSQL'), TRIM(TRAILING FROM 'Amazing PostgreSQL '), TRIM(' Amazing PostgreSQL '); ltrim | rtrim | btrim --------------------+--------------------+-------------------- Amazing PostgreSQL | Amazing PostgreSQL | Amazing PostgreSQL SELECT TRIM(LEADING '0' FROM CAST(0001245 AS TEXT)); ltrim ------- 1245 SELECT LTRIM('yummy', 'y'), RTRIM('yummy', 'y'), BTRIM('yummy', 'y'); ltrim | rtrim | btrim -------+-------+------- ummy | yumm | umm SELECT upper('uday yadav'), initcap('uday yadav'); upper | initcap ------------+------------ UDAY YADAV | Uday Yadav SELECT INITCAP(first_name) as FirstName, INITCAP(last_name) as LastName FROM directors LIMIT 3; firstname | lastname -----------+----------- Tomas | Alfredson Paul | Anderson Wes | Anderson SELECT LEFT('Uday', 3), RIGHT('Uday', 3); left | right ------+------- Uda | day SELECT LEFT('Uday', -3), RIGHT('Uday', -3); left | right ------+------- U | y SELECT LPAD('Database', 15, '*'), RPAD('Database', 15, '*'); lpad | rpad -----------------+----------------- *******Database | Database******* SELECT LENGTH('Uday Yadav'); length -------- 10 SELECT LENGTH(CAST(10013 AS TEXT)); length -------- 5 SELECT char_length(''), char_length(' '), char_length(NULL); char_length | char_length | char_length -------------+-------------+------------- 0 | 2 | SELECT first_name || ' ' || last_name as FullName, LENGTH(first_name || ' ' || last_name) as FullNameLength FROM Directors ORDER BY 2 DESC LIMIT 2; fullname | fullnamelength -----------------------------------+---------------- Florian Henckel von Donnersmarck | 33 Francis Ford Coppola | 20 SELECT POSITION('Amazing' IN 'Amazing PostgreSQL'), POSITION('is' IN 'This is a computer'); position | position ----------+---------- 1 | 3 SELECT STRPOS('World Bank', 'Bank'); strpos -------- 7 SELECT first_name, last_name FROM directors WHERE strpos(last_name, 'on') > 0 LIMIT 3; first_name | last_name ------------+----------- Tomas | Alfredson Paul | Anderson Wes | Anderson SELECT substring('What a wonderful world' from 1 for 10); substring ------------ What a won SELECT repeat('A', 4), repeat(' ', 9), repeat('.', 8); repeat | repeat | repeat --------+-----------+---------- AAAA | | ........ SELECT REPLACE('ABC XYZ', 'XY', 'Z'); replace --------- ABC ZZ","title":"Internal Functions"},{"location":"sql/data-types/miscellaneous-1/#internal-functions","text":"Order of execution of SQL statements FROM WHERE SELECT ORDER BY","title":"Internal Functions"},{"location":"sql/data-types/miscellaneous-1/#concatenation-operator","text":"select concat(first_name,last_name) as full_name from directors limit 10; full_name ---------------- TomasAlfredson PaulAnderson WesAnderson select concat_ws(' ',first_name,last_name) as full_name from directors limit 3; full_name ----------------- Tomas Alfredson Paul Anderson Wes Anderson if you can have a null value in column, always use concat_ws because it will place nothing in that and and also not place the spacer like | or a space","title":"Concatenation Operator"},{"location":"sql/data-types/miscellaneous-1/#type-conversion","text":"Type of Conversion Notes Implicit data conversion is done AUTOMATICALLY Explicit data conversion is done via 'conversion functions' eg. CAST or :: SELECT * FROM movies; -- exact datatype match : no conversion SELECT * FROM movies WHERE movie_id = 1; -- Implicit conversion : conversion SELECT * FROM movies WHERE movie_id = '1'; -- Explicit conversion : conversion SELECT * FROM movies WHERE movie_id = integer '1'; -- Output of all queries above movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id ----------+--------------------+--------------+------------+--------------+-----------------+------------- 1 | A Clockwork Orange | 112 | English | 1972-02-02 | 18 | 13","title":"Type Conversion"},{"location":"sql/data-types/miscellaneous-1/#casting","text":"-- CAST function -- Syntax : CAST ( expression as target_data_type ); SELECT CAST ( '10' AS INTEGER ); int4 ------ 10 SELECT CAST ('2020-02-02' AS DATE), CAST('01-FEB-2001' AS DATE); date | date ------------+------------ 2020-02-02 | 2001-02-01 SELECT CAST ( 'true' AS BOOLEAN ), CAST ( '1' AS BOOLEAN ), CAST ( '0' AS BOOLEAN ); bool | bool | bool ------+------+------ t | t | f SELECT CAST ( '14.87789' AS DOUBLE PRECISION ); float8 ---------- 14.87789 SELECT '2020-02-02'::DATE , '01-FEB-2001'::DATE; date | date ------------+------------ 2020-02-02 | 2001-02-01 SELECT '2020-02-02 10:20:10.23'::TIMESTAMP; timestamp ------------------------ 2020-02-02 10:20:10.23 SELECT '2020-02-02 10:20:10.23 +05:30'::TIMESTAMPTZ; timestamptz --------------------------- 2020-02-02 04:50:10.23+00 SELECT '10 minute'::interval, '10 hour'::interval, '10 day'::interval, '10 week'::interval, '10 month'::interval; interval | interval | interval | interval | interval ----------+----------+----------+----------+---------- 00:10:00 | 10:00:00 | 10 days | 70 days | 10 mons SELECT 20! AS \"result 1\" , CAST( 20 AS bigint ) ! AS \"result 2\"; result 1 | result 2 ---------------------+--------------------- 2432902008176640000 | 2432902008176640000 SELECT ROUND(10,4) AS \"result 1\", ROUND ( CAST (10 AS NUMERIC) ) AS \"result 2\", ROUND ( CAST (10 AS NUMERIC) , 4 ) AS \"result 3\"; result 1 | result 2 | result 3 ----------+----------+---------- 10.0000 | 10 | 10.0000 SELECT SUBSTR('12345',2) AS \"RESULT 1\", SUBSTR( CAST('12345' AS TEXT) ,2) AS \"RESULT 2\"; RESULT 1 | RESULT 2 ----------+---------- 2345 | 2345 CREATE TABLE ratings ( rating_id SERIAL PRIMARY KEY, rating VARCHAR(2) NOT NULL ); INSERT INTO ratings ( rating ) VALUES ('A'), ('B'), ('C'), ('D'), (1), (2), (3), (4); SELECT rating_id, CASE WHEN rating~E'^\\\\d+$' THEN CAST ( rating as INTEGER ) ELSE 0 END AS rating FROM ratings; rating_id | rating -----------+-------- 1 | 0 2 | 0 3 | 0 4 | 0 5 | 1 6 | 2 7 | 3 8 | 4","title":"Casting"},{"location":"sql/data-types/miscellaneous-1/#formatting-functions","text":"https://www.postgresql.org/docs/12/functions-formatting.html","title":"Formatting Functions"},{"location":"sql/data-types/miscellaneous-1/#to_char","text":"Refer to the documentation https://www.postgresqltutorial.com/postgresql-to_char/ SELECT TO_CHAR ( 100870, '9,999999' ); to_char ----------- 100870 SELECT release_date, TO_CHAR(release_date,'DD-MM-YYYY'), TO_CHAR(release_date,'Dy, MM, YYYY') FROM movies LIMIT 3; release_date | to_char | to_char --------------+------------+--------------- 1972-02-02 | 02-02-1972 | Wed, 02, 1972 1979-08-15 | 15-08-1979 | Wed, 08, 1979 2001-01-04 | 04-01-2001 | Thu, 01, 2001 SELECT TO_CHAR ( TIMESTAMP '2020-01-01 13:32:30', 'HH24:MI:SS' ); to_char ---------- 13:32:30","title":"to_char()"},{"location":"sql/data-types/miscellaneous-1/#to_number","text":"https://www.postgresqltutorial.com/postgresql-to_number/ SELECT TO_NUMBER( '1420.89', '9999.' ); to_number ----------- 1420 SELECT TO_NUMBER( '10,625.78-', '99G999D99S' ); to_number ----------- -10625.78 SELECT TO_NUMBER( '$1,625.78+', '99G999D99S' ); to_number ----------- 1625.78 SELECT to_number( '$1,420.65' , 'L9G999D99' ); to_number ----------- 1420.65 SELECT to_number( '21,420.65' , '99G999D99' ); to_number ----------- 21420.65","title":"to_number()"},{"location":"sql/data-types/miscellaneous-1/#to_date","text":"https://www.postgresqltutorial.com/postgresql-to_date/ SELECT TO_DATE( '2020/10/22' , 'YYYY/MM/DD' ); to_date ------------ 2020-10-22 SELECT to_date( '022199' , 'MMDDYY' ); to_date ------------ 1999-02-21 SELECT to_date( 'March 07, 2019' , 'Month DD, YYYY' ); to_date ------------ 2019-03-07","title":"to_date()"},{"location":"sql/data-types/miscellaneous-1/#to_timestamp","text":"https://www.postgresqltutorial.com/postgresql-to_timestamp/ SELECT TO_TIMESTAMP( '2017-03-31 9:30:20', 'YYYY-MM-DD HH:MI:SS' ); to_timestamp ------------------------ 2017-03-31 09:30:20+00 SELECT TO_TIMESTAMP('2017 Aug','YYYY MON'); to_timestamp ------------------------ 2017-08-01 00:00:00+00","title":"to_timestamp()"},{"location":"sql/data-types/miscellaneous-1/#string-functions","text":"Upper(string) Lower(string) INITCAP(string) REVERSE(string) LPAD(string) RPAD(string) LENGTH(string) CHAR_LENGTH(string) : Same as Length POSITION( string in string ) STRPOS ( , < substring > ) SUBSTRING (string , length) REPLACE (string, from_string, to_string) SELECT INITCAP(first_name) as FirstName, INITCAP(last_name) as LastName FROM directors LIMIT 3; firstname | lastname -----------+----------- Tomas | Alfredson Paul | Anderson Wes | Anderson SELECT LEFT('Uday', 3), RIGHT('Uday', 3); left | right ------+------- Uda | day SELECT LEFT('Uday', -3), RIGHT('Uday', -3); left | right ------+------- U | y SELECT REVERSE('UDAY YADAV'); reverse ------------ VADAY YADU SELECT SPLIT_PART('1,2,3,4', ',', 1), SPLIT_PART('1|2|3|4', '|', 2); split_part | split_part ------------+------------ 1 | 2 SELECT TRIM(LEADING FROM ' Amazing PostgreSQL'), TRIM(TRAILING FROM 'Amazing PostgreSQL '), TRIM(' Amazing PostgreSQL '); ltrim | rtrim | btrim --------------------+--------------------+-------------------- Amazing PostgreSQL | Amazing PostgreSQL | Amazing PostgreSQL SELECT TRIM(LEADING '0' FROM CAST(0001245 AS TEXT)); ltrim ------- 1245 SELECT LTRIM('yummy', 'y'), RTRIM('yummy', 'y'), BTRIM('yummy', 'y'); ltrim | rtrim | btrim -------+-------+------- ummy | yumm | umm SELECT upper('uday yadav'), initcap('uday yadav'); upper | initcap ------------+------------ UDAY YADAV | Uday Yadav SELECT INITCAP(first_name) as FirstName, INITCAP(last_name) as LastName FROM directors LIMIT 3; firstname | lastname -----------+----------- Tomas | Alfredson Paul | Anderson Wes | Anderson SELECT LEFT('Uday', 3), RIGHT('Uday', 3); left | right ------+------- Uda | day SELECT LEFT('Uday', -3), RIGHT('Uday', -3); left | right ------+------- U | y SELECT LPAD('Database', 15, '*'), RPAD('Database', 15, '*'); lpad | rpad -----------------+----------------- *******Database | Database******* SELECT LENGTH('Uday Yadav'); length -------- 10 SELECT LENGTH(CAST(10013 AS TEXT)); length -------- 5 SELECT char_length(''), char_length(' '), char_length(NULL); char_length | char_length | char_length -------------+-------------+------------- 0 | 2 | SELECT first_name || ' ' || last_name as FullName, LENGTH(first_name || ' ' || last_name) as FullNameLength FROM Directors ORDER BY 2 DESC LIMIT 2; fullname | fullnamelength -----------------------------------+---------------- Florian Henckel von Donnersmarck | 33 Francis Ford Coppola | 20 SELECT POSITION('Amazing' IN 'Amazing PostgreSQL'), POSITION('is' IN 'This is a computer'); position | position ----------+---------- 1 | 3 SELECT STRPOS('World Bank', 'Bank'); strpos -------- 7 SELECT first_name, last_name FROM directors WHERE strpos(last_name, 'on') > 0 LIMIT 3; first_name | last_name ------------+----------- Tomas | Alfredson Paul | Anderson Wes | Anderson SELECT substring('What a wonderful world' from 1 for 10); substring ------------ What a won SELECT repeat('A', 4), repeat(' ', 9), repeat('.', 8); repeat | repeat | repeat --------+-----------+---------- AAAA | | ........ SELECT REPLACE('ABC XYZ', 'XY', 'Z'); replace --------- ABC ZZ","title":"String Functions"},{"location":"sql/data-types/sequences/","text":"Sequences Sequences Specify datatype ( SMALLINT | INT | BIGINT ) Default is BIGINT List all sequence SELECT relname AS seq_name FROM pg_class WHERE relkind = 'S'; CREATE SEQUENCE IF NOT EXISTS test_sequence AS bigint; SELECT NEXTVAL('test_sequence'); nextval --------- 1 SELECT CURRVAL('test_sequence'); currval --------- 1 SELECT SETVAL('test_sequence',3); setval -------- 3 -- set this value after the nextval is called, -- check using the currval cmd SELECT SETVAL('test_sequence',300,false); -- CHECKING CURRENT VALUE SELECT CURRVAL('test_sequence'); currval --------- 3 ALTER SEQUENCE test_sequence RESTART WITH 100; SELECT NEXTVAL('test_sequence'); nextval --------- 100 CREATE SEQUENCE IF NOT EXISTS test_seq3 INCREMENT 50 MINVALUE 100 MAXVALUE 1000 START WITH 150; SELECT nextval('test_seq3'); nextval --------- 150 CREATE SEQUENCE IF NOT EXISTS seq_des INCREMENT -1 MINVALUE 1 MAXVALUE 999 START 99 NO CYCLE | CYCLE ; SELECT nextval('seq_des'); nextval --------- 99 -- DROP SEQUENCE DROP SEQUENCE IF EXISTS seq_des; CREATE TABLE IF NOT EXISTS table_seq ( id INT primary key , name VARCHAR(10) ); CREATE sequence IF NOT EXISTS table_seq_id_seq start with 1 owned BY table_seq.id; ALTER TABLE table_seq ALTER COLUMN id SET DEFAULT nextval('table_seq_id_seq') Alpha-Numeric Sequence CREATE sequence table_text_seq; CREATE TABLE contacts ( id text NOT null default ('ID' || nextval('table_text_seq')), name VARCHAR(150) NOT null ); INSERT INTO contacts (name) VALUES ('uday 1'),('uday 2'),('uday 3'); SELECT * FROM contacts; id | name -----+-------- ID1 | uday 1 ID2 | uday 2 ID3 | uday 3","title":"Sequences"},{"location":"sql/data-types/sequences/#sequences","text":"","title":"Sequences"},{"location":"sql/data-types/sequences/#sequences_1","text":"Specify datatype ( SMALLINT | INT | BIGINT ) Default is BIGINT","title":"Sequences"},{"location":"sql/data-types/sequences/#list-all-sequence","text":"SELECT relname AS seq_name FROM pg_class WHERE relkind = 'S'; CREATE SEQUENCE IF NOT EXISTS test_sequence AS bigint; SELECT NEXTVAL('test_sequence'); nextval --------- 1 SELECT CURRVAL('test_sequence'); currval --------- 1 SELECT SETVAL('test_sequence',3); setval -------- 3 -- set this value after the nextval is called, -- check using the currval cmd SELECT SETVAL('test_sequence',300,false); -- CHECKING CURRENT VALUE SELECT CURRVAL('test_sequence'); currval --------- 3 ALTER SEQUENCE test_sequence RESTART WITH 100; SELECT NEXTVAL('test_sequence'); nextval --------- 100 CREATE SEQUENCE IF NOT EXISTS test_seq3 INCREMENT 50 MINVALUE 100 MAXVALUE 1000 START WITH 150; SELECT nextval('test_seq3'); nextval --------- 150 CREATE SEQUENCE IF NOT EXISTS seq_des INCREMENT -1 MINVALUE 1 MAXVALUE 999 START 99 NO CYCLE | CYCLE ; SELECT nextval('seq_des'); nextval --------- 99 -- DROP SEQUENCE DROP SEQUENCE IF EXISTS seq_des; CREATE TABLE IF NOT EXISTS table_seq ( id INT primary key , name VARCHAR(10) ); CREATE sequence IF NOT EXISTS table_seq_id_seq start with 1 owned BY table_seq.id; ALTER TABLE table_seq ALTER COLUMN id SET DEFAULT nextval('table_seq_id_seq')","title":"List all sequence"},{"location":"sql/data-types/sequences/#alpha-numeric-sequence","text":"CREATE sequence table_text_seq; CREATE TABLE contacts ( id text NOT null default ('ID' || nextval('table_text_seq')), name VARCHAR(150) NOT null ); INSERT INTO contacts (name) VALUES ('uday 1'),('uday 2'),('uday 3'); SELECT * FROM contacts; id | name -----+-------- ID1 | uday 1 ID2 | uday 2 ID3 | uday 3","title":"Alpha-Numeric Sequence"},{"location":"sql/data-types/user-defined-data-types/","text":"User Defined Data Types CREATE DOMAIN Create user defined data type with a range, optional, DEFAULT, NOT NULL and CHECK Constraint. They are unique within schema scope. Helps standardise your database types in one place. Composite Type : Only single value return CREATE DOMAIN name datatype constraint -- ex 1 -- 'addr' with domain VARCHAR(100) CREATE DOMAIN addr VARCHAR(100) NOT NULL; CREATE TABLE locations ( address addr ); Table \"public.locations\" Column | Type | Collation | Nullable | Default ---------+------+-----------+----------+--------- address | addr | | | -- Dropping Constraints -- if domain isnt used anywhere drop domain addr; -- this will drop the column in the table it is present in -- use this with caution drop domain addr CASCADE; -- List all domains inside a schema select typname from pg_catalog.pg_type join pg_catalog.pg_namespace on pg_namespace.oid = pg_type.typnamespace where typtype = 'd' and nspname = 'public'; typname ------------------ positive_numeric valid_color addr Number Based Components -- Example 2 -- 'positive_numeric' : value > 0 CREATE DOMAIN positive_numeric INT NOT NULL CHECK (VALUE > 0); CREATE TABLE sample ( number positive_numeric ); INSERT INTO sample (NUMBER) VALUES (10); -- error INSERT INTO sample (NUMBER) VALUES (-10); -- ERROR: value for domain positive_numeric -- violates check constraint \"positive_numeric_check\" SELECT * FROM sample; number -------- 10 Text Based Domain -- Example 3 -- check email domain CREATE DOMAIN proper_email VARCHAR(150) CHECK ( VALUE ~* '^[A-Za-z0-9._%-]+@[A-Za-z0-9.-]+[.][A-Za-z]+$' ); CREATE TABLE email_check ( client_email proper_email ); insert into email_check (client_email) values ('a@b.com') ; -- error insert into email_check (client_email) values ('a@#.com') ; Enum Based Domain -- enum based domain CREATE DOMAIN valid_color VARCHAR(10) CHECK (VALUE IN ('red','green','blue')); CREATE TABLE color ( color valid_color ); INSERT INTO color (color) VALUES ('red'),('blue'),('green'); -- error INSERT INTO color (color) VALUES ('yellow'); Composite Data Types Syntax : (composite_column).city Example 1 -- address type CREATE TYPE address AS ( city VARCHAR(50), country VARCHAR(100) ); CREATE TABLE person ( id SERIAL PRIMARY KEY, address address ); INSERT INTO person ( address ) VALUES (ROW('London','UK')), (ROW('New York','USA')); select * from person; id | address ----+------------------ 1 | (London,UK) 2 | (\"New York\",USA) select (address).country from person; country --------- UK USA Example 2 CREATE TYPE currency AS ENUM( 'USD','EUR','GBP','CHF' ); SELECT 'USD'::currency currency ---------- USD SELECT 'INR'::currency -- ERROR: invalid input value for enum currency: \"INR\" -- LINE 1: SELECT 'INR'::currency ALTER TYPE currency ADD VALUE 'CHF' AFTER 'EUR'; CREATE TABLE stocks ( id SERIAL PRIMARY KEY, symbol currency ); insert into stocks ( symbol ) VALUES ('CHF'); select * from stocks id | symbol ----+-------- 1 | CHF -- DROP TYPE currency; Alter Alter TYPE ALTER TYPE addr RENAME TO user_address ALTER TYPE user_address OWNER TO uday ALTER TYPE user_address SET SCHEMA test_scm ALTER TYPE test_scm.user_address ADD ATTRIBUTE street_address VARCHAR(150) CREATE TYPE mycolors AS ENUM ('green','red','blue') ALTER TYPE mycolors RENAME VALUE 'red' TO 'orange' SELECT enum_range(NULL::mycolors); ALTER TYPE mycolors ADD VALUE 'red' BEFORE 'green' ALTER ENUM CREATE TYPE status_enum AS enum ('queued','waiting','running','done'); CREATE TABLE jobs ( id SERIAL PRIMARY KEY, job_status status_enum ); INSERT INTO jobs ( job_status ) VALUES ('queued'),('waiting'),('running'),('done'); SELECT * FROM jobs; id | job_status ----+------------ 1 | queued 2 | waiting 3 | running 4 | done -- UPDATING waiting to running UPDATE jobs SET job_status = 'running' WHERE job_status = 'waiting'; id | job_status ----+------------ 1 | queued 3 | running 4 | done 2 | running Updating/Replacing ENUM domain ALTER TYPE status_enum RENAME TO status_enum_old; CREATE TYPE status_enum as enum ('queued','running','done'); ALTER TABLE jobs ALTER COLUMN job_status TYPE status_enum USING job_status::text::status_enum; DROP TYPE status_enum_old; Default value ENUM CREATE TYPE status AS ENUM ('PENDING','APPROVED','DECLINE') CREATE TABLE cron_jobs ( id SERIAL, status status DEFAULT 'PENDING' ); INSERT INTO cron_jobs ( status ) VALUES ('APPROVED'); CREATE DOMAIN IF NOT EXISTS DO $$ BEGIN IF NOT EXISTS ( SELECT * FROM pg_type tp INNER JOIN pg_namespace nsp ON nsp.oid = typ.typnamespace WHERE nsp.nspname = current_schema() AND typ.typname = 'a' ) THEN CREATE TYPE ai AS ( a TEXT, i INT ); END IF; END; $$ LANGUAGE plpgsql;","title":"User Defined Data Types"},{"location":"sql/data-types/user-defined-data-types/#user-defined-data-types","text":"","title":"User Defined Data Types"},{"location":"sql/data-types/user-defined-data-types/#create-domain","text":"Create user defined data type with a range, optional, DEFAULT, NOT NULL and CHECK Constraint. They are unique within schema scope. Helps standardise your database types in one place. Composite Type : Only single value return CREATE DOMAIN name datatype constraint -- ex 1 -- 'addr' with domain VARCHAR(100) CREATE DOMAIN addr VARCHAR(100) NOT NULL; CREATE TABLE locations ( address addr ); Table \"public.locations\" Column | Type | Collation | Nullable | Default ---------+------+-----------+----------+--------- address | addr | | | -- Dropping Constraints -- if domain isnt used anywhere drop domain addr; -- this will drop the column in the table it is present in -- use this with caution drop domain addr CASCADE; -- List all domains inside a schema select typname from pg_catalog.pg_type join pg_catalog.pg_namespace on pg_namespace.oid = pg_type.typnamespace where typtype = 'd' and nspname = 'public'; typname ------------------ positive_numeric valid_color addr","title":"CREATE DOMAIN"},{"location":"sql/data-types/user-defined-data-types/#number-based-components","text":"-- Example 2 -- 'positive_numeric' : value > 0 CREATE DOMAIN positive_numeric INT NOT NULL CHECK (VALUE > 0); CREATE TABLE sample ( number positive_numeric ); INSERT INTO sample (NUMBER) VALUES (10); -- error INSERT INTO sample (NUMBER) VALUES (-10); -- ERROR: value for domain positive_numeric -- violates check constraint \"positive_numeric_check\" SELECT * FROM sample; number -------- 10","title":"Number Based Components"},{"location":"sql/data-types/user-defined-data-types/#text-based-domain","text":"-- Example 3 -- check email domain CREATE DOMAIN proper_email VARCHAR(150) CHECK ( VALUE ~* '^[A-Za-z0-9._%-]+@[A-Za-z0-9.-]+[.][A-Za-z]+$' ); CREATE TABLE email_check ( client_email proper_email ); insert into email_check (client_email) values ('a@b.com') ; -- error insert into email_check (client_email) values ('a@#.com') ;","title":"Text Based Domain"},{"location":"sql/data-types/user-defined-data-types/#enum-based-domain","text":"-- enum based domain CREATE DOMAIN valid_color VARCHAR(10) CHECK (VALUE IN ('red','green','blue')); CREATE TABLE color ( color valid_color ); INSERT INTO color (color) VALUES ('red'),('blue'),('green'); -- error INSERT INTO color (color) VALUES ('yellow');","title":"Enum Based Domain"},{"location":"sql/data-types/user-defined-data-types/#composite-data-types","text":"Syntax : (composite_column).city","title":"Composite Data Types"},{"location":"sql/data-types/user-defined-data-types/#example-1","text":"-- address type CREATE TYPE address AS ( city VARCHAR(50), country VARCHAR(100) ); CREATE TABLE person ( id SERIAL PRIMARY KEY, address address ); INSERT INTO person ( address ) VALUES (ROW('London','UK')), (ROW('New York','USA')); select * from person; id | address ----+------------------ 1 | (London,UK) 2 | (\"New York\",USA) select (address).country from person; country --------- UK USA","title":"Example 1"},{"location":"sql/data-types/user-defined-data-types/#example-2","text":"CREATE TYPE currency AS ENUM( 'USD','EUR','GBP','CHF' ); SELECT 'USD'::currency currency ---------- USD SELECT 'INR'::currency -- ERROR: invalid input value for enum currency: \"INR\" -- LINE 1: SELECT 'INR'::currency ALTER TYPE currency ADD VALUE 'CHF' AFTER 'EUR'; CREATE TABLE stocks ( id SERIAL PRIMARY KEY, symbol currency ); insert into stocks ( symbol ) VALUES ('CHF'); select * from stocks id | symbol ----+-------- 1 | CHF -- DROP TYPE currency;","title":"Example 2"},{"location":"sql/data-types/user-defined-data-types/#alter","text":"","title":"Alter"},{"location":"sql/data-types/user-defined-data-types/#alter-type","text":"ALTER TYPE addr RENAME TO user_address ALTER TYPE user_address OWNER TO uday ALTER TYPE user_address SET SCHEMA test_scm ALTER TYPE test_scm.user_address ADD ATTRIBUTE street_address VARCHAR(150) CREATE TYPE mycolors AS ENUM ('green','red','blue') ALTER TYPE mycolors RENAME VALUE 'red' TO 'orange' SELECT enum_range(NULL::mycolors); ALTER TYPE mycolors ADD VALUE 'red' BEFORE 'green'","title":"Alter TYPE"},{"location":"sql/data-types/user-defined-data-types/#alter-enum","text":"CREATE TYPE status_enum AS enum ('queued','waiting','running','done'); CREATE TABLE jobs ( id SERIAL PRIMARY KEY, job_status status_enum ); INSERT INTO jobs ( job_status ) VALUES ('queued'),('waiting'),('running'),('done'); SELECT * FROM jobs; id | job_status ----+------------ 1 | queued 2 | waiting 3 | running 4 | done -- UPDATING waiting to running UPDATE jobs SET job_status = 'running' WHERE job_status = 'waiting'; id | job_status ----+------------ 1 | queued 3 | running 4 | done 2 | running","title":"ALTER ENUM"},{"location":"sql/data-types/user-defined-data-types/#updatingreplacing-enum-domain","text":"ALTER TYPE status_enum RENAME TO status_enum_old; CREATE TYPE status_enum as enum ('queued','running','done'); ALTER TABLE jobs ALTER COLUMN job_status TYPE status_enum USING job_status::text::status_enum; DROP TYPE status_enum_old;","title":"Updating/Replacing ENUM domain"},{"location":"sql/data-types/user-defined-data-types/#default-value-enum","text":"CREATE TYPE status AS ENUM ('PENDING','APPROVED','DECLINE') CREATE TABLE cron_jobs ( id SERIAL, status status DEFAULT 'PENDING' ); INSERT INTO cron_jobs ( status ) VALUES ('APPROVED');","title":"Default value ENUM"},{"location":"sql/data-types/user-defined-data-types/#create-domain-if-not-exists","text":"DO $$ BEGIN IF NOT EXISTS ( SELECT * FROM pg_type tp INNER JOIN pg_namespace nsp ON nsp.oid = typ.typnamespace WHERE nsp.nspname = current_schema() AND typ.typname = 'a' ) THEN CREATE TYPE ai AS ( a TEXT, i INT ); END IF; END; $$ LANGUAGE plpgsql;","title":"CREATE DOMAIN IF NOT EXISTS"},{"location":"sql/functions/","text":"Functions Basic Functions Syntax CREATE OR REPLACE FUNCTION function_name() RETURNS return_type as ' -- SQL COMMAND ' LANGUAGE SQL; Some Examples -- Function to Add CREATE OR REPLACE FUNCTION fn_my_sum( int, int ) RETURNS int as ' SELECT $1 + $2; ' LANGUAGE SQL; -- function call SELECT fn_my_sum(1,2); -- output fn_my_sum ----------- 3 (1 row) -------------------------------- -- Function to printer CREATE OR REPLACE FUNCTION fn_printer( text ) RETURNS text as $$ SELECT 'Hello ' || $1 ; $$ LANGUAGE SQL; --function call SELECT fn_printer( 'Uday' ); -- output fn_printer ------------ Hello Uday (1 row) -- Another syntax CREATE OR REPLACE FUNCTION fn_printer( text ) RETURNS text as $body$ SELECT 'Hello ' || $1 ; $body$ LANGUAGE SQL; -- function call SELECT fn_printer( 'Uday' ); -- output fn_printer ------------ Hello Uday (1 row) Functions with DML -- function CREATE OR REPLACE FUNCTION fn_employee_update_country () returns void AS $$ update employees set country = 'n/a' where country is NULL $$ LANGUAGE SQL SELECT fn_employee_update_country(); Function with DQL CREATE OR REPLACE FUNCTION fn_api_order_latest() RETURNS orders AS $$ select * from orders order by order_date DESC limit 1 $$ LANGUAGE SQL; select (fn_api_order_latest()).*; select (fn_api_order_latest()).order_date; select order_date(fn_api_order_latest()) ----------------------------- CREATE OR REPLACE FUNCTION fn_employee_hire_bydate( p_year integer ) returns setof employees as $$ select * from employees where extract('YEAR' from hire_date) = p_year $$ LANGUAGE SQL SELECT (fn_employee_hire_bydate('1992')).*; Returning table from function CREATE OR REPLACE FUNCTION fn_orders() returns table ( order_id SMALLINT, employee_id SMALLINT ) as $$ select order_id, employee_id from orders; $$ LANGUAGE SQL; SELECT (fn_orders()).*; Function with Defualt parameters CREATE OR REPLACE FUNCTION function_name ( x int default 0, y int DEFAULT 10 ) returns int as $$ select x+y; $$ LANGUAGE SQL; SELECT function_name(); Dropping Function DROP FUNCTION [ IF EXISTS ] function_name ( argument_list ) ( cascade | restrict );","title":"Functions"},{"location":"sql/functions/#functions","text":"","title":"Functions"},{"location":"sql/functions/#basic-functions","text":"","title":"Basic Functions"},{"location":"sql/functions/#syntax","text":"CREATE OR REPLACE FUNCTION function_name() RETURNS return_type as ' -- SQL COMMAND ' LANGUAGE SQL;","title":"Syntax"},{"location":"sql/functions/#some-examples","text":"-- Function to Add CREATE OR REPLACE FUNCTION fn_my_sum( int, int ) RETURNS int as ' SELECT $1 + $2; ' LANGUAGE SQL; -- function call SELECT fn_my_sum(1,2); -- output fn_my_sum ----------- 3 (1 row) -------------------------------- -- Function to printer CREATE OR REPLACE FUNCTION fn_printer( text ) RETURNS text as $$ SELECT 'Hello ' || $1 ; $$ LANGUAGE SQL; --function call SELECT fn_printer( 'Uday' ); -- output fn_printer ------------ Hello Uday (1 row) -- Another syntax CREATE OR REPLACE FUNCTION fn_printer( text ) RETURNS text as $body$ SELECT 'Hello ' || $1 ; $body$ LANGUAGE SQL; -- function call SELECT fn_printer( 'Uday' ); -- output fn_printer ------------ Hello Uday (1 row)","title":"Some Examples"},{"location":"sql/functions/#functions-with-dml","text":"-- function CREATE OR REPLACE FUNCTION fn_employee_update_country () returns void AS $$ update employees set country = 'n/a' where country is NULL $$ LANGUAGE SQL SELECT fn_employee_update_country();","title":"Functions with DML"},{"location":"sql/functions/#function-with-dql","text":"CREATE OR REPLACE FUNCTION fn_api_order_latest() RETURNS orders AS $$ select * from orders order by order_date DESC limit 1 $$ LANGUAGE SQL; select (fn_api_order_latest()).*; select (fn_api_order_latest()).order_date; select order_date(fn_api_order_latest()) ----------------------------- CREATE OR REPLACE FUNCTION fn_employee_hire_bydate( p_year integer ) returns setof employees as $$ select * from employees where extract('YEAR' from hire_date) = p_year $$ LANGUAGE SQL SELECT (fn_employee_hire_bydate('1992')).*;","title":"Function with DQL"},{"location":"sql/functions/#returning-table-from-function","text":"CREATE OR REPLACE FUNCTION fn_orders() returns table ( order_id SMALLINT, employee_id SMALLINT ) as $$ select order_id, employee_id from orders; $$ LANGUAGE SQL; SELECT (fn_orders()).*;","title":"Returning table from function"},{"location":"sql/functions/#function-with-defualt-parameters","text":"CREATE OR REPLACE FUNCTION function_name ( x int default 0, y int DEFAULT 10 ) returns int as $$ select x+y; $$ LANGUAGE SQL; SELECT function_name();","title":"Function with Defualt parameters"},{"location":"sql/functions/#dropping-function","text":"DROP FUNCTION [ IF EXISTS ] function_name ( argument_list ) ( cascade | restrict );","title":"Dropping Function"},{"location":"sql/functions/cursors/","text":"Cursors Rows returned by the SQL query are those which match the condition. It can either be zero or more at a time. Sometimes you need to traverse through the rows one by one, forward or backwards Life Cycle of Cursor DECLARE OPEN FETCH CLOSE Cursor enable SQL to retrieve ( or update, or delete ) a single row at a time. Cursor needs to be created in DECLARE cur_al_movie refcursor; -- or cursor-name [cursor-scrollability] cursor [(name datatype ...)] FOR query-expression cursor-scrollability : SCROLL OR NO SCROLL , NO SCROLL mean the cursor cannot scrol backward. query-expression : You can use any legal SELECT statement as a query expression. The result set rows are considered as scope of the cursor. Example DECLARE cur_all_movies CURSOR FOR SELECT movie_name, movie_length FROM movies; Cursor with Parameters DECLARE cur_all_movies_by_year CURSOR ( custom_year integer ) FOR SELECT * FROM movies WHERE EXTRACT ('YEAR' FROM release_date ) = custom_year Opening a cursor Opening an unbound cursor OPEN unbound_cursor_variable [[NO] SCROLL] FOR query; Opening un bound cursor OPEN cur_directors_us FOR SELECT first_name, last_name, date_of_birth FROM directors WHERE nationality = 'American' Opening an un bound cursor with dynamic query OPEN unbound_cursor_variable [[NO] SCROLL] FOR EXECUTE query-expression [using expression [,...]]; select * from movies order by movie_name; DO $$ DECLARE output_text text default ''; rec_movie record; cur_all_movies CURSOR FOR SELECT * FROM movies; BEGIN OPEN cur_all_movies; LOOP FETCH cur_all_movies into rec_movie; EXIT WHEN NOT FOUND; output_text := output_text || ',' || rec_movie.movie_name; END LOOP; RAISE NOTICE 'ALL MOVIES NAME %' , output_text; END; $$","title":"Cursors"},{"location":"sql/functions/cursors/#cursors","text":"Rows returned by the SQL query are those which match the condition. It can either be zero or more at a time. Sometimes you need to traverse through the rows one by one, forward or backwards Life Cycle of Cursor DECLARE OPEN FETCH CLOSE Cursor enable SQL to retrieve ( or update, or delete ) a single row at a time. Cursor needs to be created in DECLARE cur_al_movie refcursor; -- or cursor-name [cursor-scrollability] cursor [(name datatype ...)] FOR query-expression cursor-scrollability : SCROLL OR NO SCROLL , NO SCROLL mean the cursor cannot scrol backward. query-expression : You can use any legal SELECT statement as a query expression. The result set rows are considered as scope of the cursor.","title":"Cursors"},{"location":"sql/functions/cursors/#example","text":"DECLARE cur_all_movies CURSOR FOR SELECT movie_name, movie_length FROM movies;","title":"Example"},{"location":"sql/functions/cursors/#cursor-with-parameters","text":"DECLARE cur_all_movies_by_year CURSOR ( custom_year integer ) FOR SELECT * FROM movies WHERE EXTRACT ('YEAR' FROM release_date ) = custom_year","title":"Cursor with Parameters"},{"location":"sql/functions/cursors/#opening-a-cursor","text":"Opening an unbound cursor OPEN unbound_cursor_variable [[NO] SCROLL] FOR query;","title":"Opening a cursor"},{"location":"sql/functions/cursors/#opening-un-bound-cursor","text":"OPEN cur_directors_us FOR SELECT first_name, last_name, date_of_birth FROM directors WHERE nationality = 'American'","title":"Opening un bound cursor"},{"location":"sql/functions/cursors/#opening-an-un-bound-cursor-with-dynamic-query","text":"OPEN unbound_cursor_variable [[NO] SCROLL] FOR EXECUTE query-expression [using expression [,...]]; select * from movies order by movie_name; DO $$ DECLARE output_text text default ''; rec_movie record; cur_all_movies CURSOR FOR SELECT * FROM movies; BEGIN OPEN cur_all_movies; LOOP FETCH cur_all_movies into rec_movie; EXIT WHEN NOT FOUND; output_text := output_text || ',' || rec_movie.movie_name; END LOOP; RAISE NOTICE 'ALL MOVIES NAME %' , output_text; END; $$","title":"Opening an un bound cursor with dynamic query"},{"location":"sql/functions/pl-pgsql/","text":"PL/PGSQL Declaring Variables DO $$ DECLARE mynum integer := 89; first_name varchar(20) := 'Uday'; hire_date date := '2020-01-01'; start_time timestamp := NOW(); emptyvar integer; BEGIN RAISE NOTICE 'My variable % % % % %', mynum, first_name, hire_date, start_time, emptyvar ; END; $$ Parameters to Function CREATE OR REPLACE FUNCTION function_name (INT, INT) RETURNS INT as $$ DECLARE x alias for $1; y alias for $2; BEGIN -- END; $$ Assigning value from result into variable DO $$ DECLARE product_title products.product_name%TYPE; BEGIN SELECT product_name FROM products INTO product_title where product_id = 1 limit 1; RAISE NOTICE 'Your product name is %', product_title; END; $$ Function Parameter w/t IN and OUT CREATE OR REPLACE FUNCTION fn_sum_using_inout ( IN x integer, IN y integer, OUT Z integer ) as $$ BEGIN z := x+y; END; $$ LANGUAGE PLPGSQL; select fn_sum_using_inout(2,3); -- another example CREATE OR REPLACE FUNCTION fn_sum_using_inouts ( IN x integer, IN y integer, OUT Z integer, OUT w integer ) as $$ BEGIN z := x+y; w := x*y; END; $$ LANGUAGE PLPGSQL; select * from fn_sum_using_inouts(2,3); Nested functions DO $$ << Parent >> DECLARE counter integer := 0; BEGIN counter := counter+1; RAISE NOTICE 'the current value of counter (IN PARENT) is %', counter; DECLARE counter integer := 0; BEGIN counter := counter + 5; RAISE NOTICE 'The current value of counter at subblocks is %', counter; RAISE NOTICE 'The parent value of counter at subblocks is %', PARENT.counter; END; counter := counter + 5; RAISE NOTICE 'the current value of counter (IN PARENT) is %', counter; END; $$ LANGUAGE PLPGSQL; Returning ResultSet from function CREATE OR REPLACE FUNCTION fn_order_by_date_pro() RETURNS SETOF orders AS $$ BEGIN RETURN QUERY SELECT * FROM orders limit 10; END; $$ LANGUAGE PLPGSQL; SELECT * FROM fn_order_by_date_pro(); Conditional Statement inside functions Default Parameters CREATE OR REPLACE FUNCTION fn_which_is_greater ( x integer default 0, y integer default 0 ) RETURNS text AS $$ BEGIN IF x > y then return ' x > y '; else return ' x < y '; end if ; END; $$ LANGUAGE PLPGSQL; SELECT fn_which_is_greater(4,3); Switch Case Example CREATE OR REPLACE FUNCTION fn_checker ( x integer default 0 ) RETURNS text AS $$ BEGIN CASE x when 10 then return 'value = 10'; when 20 then return 'value = 20'; else RETURN 'MORE'; END CASE; END; $$ LANGUAGE PLPGSQL; SELECT fn_checker(30); Loops in PLPGSQL DO $$ DECLARE i_counter integer = 0; BEGIN LOOP RAISE NOTICE '%', i_counter; i_counter := i_counter+1; EXIT WHEN i_counter = 5; END LOOP; END; $$ LANGUAGE PLPGSQL; Loops in range exaple DO $$ BEGIN FOR counter IN 1..5 BY 1 LOOP RAISE NOTICE 'COUNTER : %', counter; END LOOP; END; $$ LANGUAGE PLPGSQL; Reverse Loops DO $$ BEGIN FOR counter IN REVERSE 5..1 BY 1 LOOP RAISE NOTICE 'COUNTER : %', counter; END LOOP; END; $$ LANGUAGE PLPGSQL; Iterating over result set DO $$ DECLARE rec record ; BEGIN FOR rec in select order_id, customer_id from orders LIMIT 10 LOOP RAISE NOTICE '% %', rec.order_id, rec.customer_id; END LOOP; END; $$ LANGUAGE PLPGSQL Loop with Exit condition DO $$ DECLARE i_counter int = 0; BEGIN LOOP i_counter = i_counter + 1; EXIT WHEN i_counter > 20; CONTINUE WHEN MOD (i_counter,2) = 0; RAISE NOTICE 'COUNTER : %', i_counter; END LOOP; END; $$ LANGUAGE PLPGSQL; Declaring arrays in PLPGSQL DO $$ DECLARE arr1 int[] := array[1,2,3]; arr2 int[] := array[4,5,6,7,8]; var int; BEGIN FOREACH var IN ARRAY arr1||ARR2 LOOP RAISE NOTICE '%', var; END LOOP; END; $$ LANGUAGE PLPGSQL; While Loop in PLPGSQL CREATE OR REPLACE FUNCTION fn_while_loop_sum_all(x integer) returns numeric as $$ DECLARE counter integer := 1; sum_all integer := 0; BEGIN WHILE counter <= x LOOP sum_all := sum_all + counter; counter := counter + 1; END LOOP; return sum_all; END; $$ language plpgsql select fn_while_loop_sum_all(4); Returning specific Query from column CREATE OR REPLACE FUNCTION fn_api_products_by_names(p_pattern varchar) RETURNS TABLE ( productname varchar, unitprice real ) AS $$ BEGIN RETURN QUERY SELECT product_name,unit_price from products where product_name like p_pattern; END; $$ LANGUAGE PLPGSQL; SELECT * FROM fn_api_products_by_names('A%'); CREATE OR REPLACE FUNCTION fn_all_orders_greater() RETURNS SETOF order_details as $$ DECLARE r record; BEGIN for r in select * from order_details where unit_price > 100 loop return next r; end loop; return; end; $$ language plpgsql; select * from fn_all_orders_greater(); If data not found condition DO $$ DECLARE rec record; orderid smallint = 1; BEGIN SELECT customer_id, order_date FROM orders INTO STRICT rec WHERE order_id = orderid; EXCEPTION WHEN NO_DATA_FOUND THEN RAISE EXCEPTION 'No order id was found'; END; $$ LANGUAGE PLPGSQL; Throwing execption on condition DO $$ DECLARE rec record; orderid smallint = 1; BEGIN SELECT customer_id, order_date FROM orders INTO STRICT rec WHERE order_id > 1000; EXCEPTION WHEN TOO_MANY_ROWS THEN RAISE EXCEPTION 'Too many rows were found'; END; $$ LANGUAGE PLPGSQL; Throwing execption example CREATE OR REPLACE FUNCTION fn_div_exception (x real, y real) RETURNS real as $$ DECLARE ret real; BEGIN ret := x / y; return ret; EXCEPTION WHEN division_by_zero then RAISE INFO 'division by zero error'; RAISE INFO 'ERROR % %', SQLSTATE, SQLERRM; END; $$ LANGUAGE PLPGSQL; SELECT fn_div_exception(5,0);","title":"PL/PGSQL"},{"location":"sql/functions/pl-pgsql/#plpgsql","text":"","title":"PL/PGSQL"},{"location":"sql/functions/pl-pgsql/#declaring-variables","text":"DO $$ DECLARE mynum integer := 89; first_name varchar(20) := 'Uday'; hire_date date := '2020-01-01'; start_time timestamp := NOW(); emptyvar integer; BEGIN RAISE NOTICE 'My variable % % % % %', mynum, first_name, hire_date, start_time, emptyvar ; END; $$","title":"Declaring Variables"},{"location":"sql/functions/pl-pgsql/#parameters-to-function","text":"CREATE OR REPLACE FUNCTION function_name (INT, INT) RETURNS INT as $$ DECLARE x alias for $1; y alias for $2; BEGIN -- END; $$","title":"Parameters to Function"},{"location":"sql/functions/pl-pgsql/#assigning-value-from-result-into-variable","text":"DO $$ DECLARE product_title products.product_name%TYPE; BEGIN SELECT product_name FROM products INTO product_title where product_id = 1 limit 1; RAISE NOTICE 'Your product name is %', product_title; END; $$","title":"Assigning value from result into variable"},{"location":"sql/functions/pl-pgsql/#function-parameter-wt-in-and-out","text":"CREATE OR REPLACE FUNCTION fn_sum_using_inout ( IN x integer, IN y integer, OUT Z integer ) as $$ BEGIN z := x+y; END; $$ LANGUAGE PLPGSQL; select fn_sum_using_inout(2,3); -- another example CREATE OR REPLACE FUNCTION fn_sum_using_inouts ( IN x integer, IN y integer, OUT Z integer, OUT w integer ) as $$ BEGIN z := x+y; w := x*y; END; $$ LANGUAGE PLPGSQL; select * from fn_sum_using_inouts(2,3);","title":"Function Parameter w/t IN and OUT"},{"location":"sql/functions/pl-pgsql/#nested-functions","text":"DO $$ << Parent >> DECLARE counter integer := 0; BEGIN counter := counter+1; RAISE NOTICE 'the current value of counter (IN PARENT) is %', counter; DECLARE counter integer := 0; BEGIN counter := counter + 5; RAISE NOTICE 'The current value of counter at subblocks is %', counter; RAISE NOTICE 'The parent value of counter at subblocks is %', PARENT.counter; END; counter := counter + 5; RAISE NOTICE 'the current value of counter (IN PARENT) is %', counter; END; $$ LANGUAGE PLPGSQL;","title":"Nested functions"},{"location":"sql/functions/pl-pgsql/#returning-resultset-from-function","text":"CREATE OR REPLACE FUNCTION fn_order_by_date_pro() RETURNS SETOF orders AS $$ BEGIN RETURN QUERY SELECT * FROM orders limit 10; END; $$ LANGUAGE PLPGSQL; SELECT * FROM fn_order_by_date_pro();","title":"Returning ResultSet from function"},{"location":"sql/functions/pl-pgsql/#conditional-statement-inside-functions","text":"","title":"Conditional Statement inside functions"},{"location":"sql/functions/pl-pgsql/#default-parameters","text":"CREATE OR REPLACE FUNCTION fn_which_is_greater ( x integer default 0, y integer default 0 ) RETURNS text AS $$ BEGIN IF x > y then return ' x > y '; else return ' x < y '; end if ; END; $$ LANGUAGE PLPGSQL; SELECT fn_which_is_greater(4,3);","title":"Default Parameters"},{"location":"sql/functions/pl-pgsql/#switch-case-example","text":"CREATE OR REPLACE FUNCTION fn_checker ( x integer default 0 ) RETURNS text AS $$ BEGIN CASE x when 10 then return 'value = 10'; when 20 then return 'value = 20'; else RETURN 'MORE'; END CASE; END; $$ LANGUAGE PLPGSQL; SELECT fn_checker(30);","title":"Switch Case Example"},{"location":"sql/functions/pl-pgsql/#loops-in-plpgsql","text":"DO $$ DECLARE i_counter integer = 0; BEGIN LOOP RAISE NOTICE '%', i_counter; i_counter := i_counter+1; EXIT WHEN i_counter = 5; END LOOP; END; $$ LANGUAGE PLPGSQL;","title":"Loops in PLPGSQL"},{"location":"sql/functions/pl-pgsql/#loops-in-range-exaple","text":"DO $$ BEGIN FOR counter IN 1..5 BY 1 LOOP RAISE NOTICE 'COUNTER : %', counter; END LOOP; END; $$ LANGUAGE PLPGSQL;","title":"Loops in range exaple"},{"location":"sql/functions/pl-pgsql/#reverse-loops","text":"DO $$ BEGIN FOR counter IN REVERSE 5..1 BY 1 LOOP RAISE NOTICE 'COUNTER : %', counter; END LOOP; END; $$ LANGUAGE PLPGSQL;","title":"Reverse Loops"},{"location":"sql/functions/pl-pgsql/#iterating-over-result-set","text":"DO $$ DECLARE rec record ; BEGIN FOR rec in select order_id, customer_id from orders LIMIT 10 LOOP RAISE NOTICE '% %', rec.order_id, rec.customer_id; END LOOP; END; $$ LANGUAGE PLPGSQL","title":"Iterating over result set"},{"location":"sql/functions/pl-pgsql/#loop-with-exit-condition","text":"DO $$ DECLARE i_counter int = 0; BEGIN LOOP i_counter = i_counter + 1; EXIT WHEN i_counter > 20; CONTINUE WHEN MOD (i_counter,2) = 0; RAISE NOTICE 'COUNTER : %', i_counter; END LOOP; END; $$ LANGUAGE PLPGSQL;","title":"Loop with Exit condition"},{"location":"sql/functions/pl-pgsql/#declaring-arrays-in-plpgsql","text":"DO $$ DECLARE arr1 int[] := array[1,2,3]; arr2 int[] := array[4,5,6,7,8]; var int; BEGIN FOREACH var IN ARRAY arr1||ARR2 LOOP RAISE NOTICE '%', var; END LOOP; END; $$ LANGUAGE PLPGSQL;","title":"Declaring arrays in PLPGSQL"},{"location":"sql/functions/pl-pgsql/#while-loop-in-plpgsql","text":"CREATE OR REPLACE FUNCTION fn_while_loop_sum_all(x integer) returns numeric as $$ DECLARE counter integer := 1; sum_all integer := 0; BEGIN WHILE counter <= x LOOP sum_all := sum_all + counter; counter := counter + 1; END LOOP; return sum_all; END; $$ language plpgsql select fn_while_loop_sum_all(4);","title":"While Loop in PLPGSQL"},{"location":"sql/functions/pl-pgsql/#returning-specific-query-from-column","text":"CREATE OR REPLACE FUNCTION fn_api_products_by_names(p_pattern varchar) RETURNS TABLE ( productname varchar, unitprice real ) AS $$ BEGIN RETURN QUERY SELECT product_name,unit_price from products where product_name like p_pattern; END; $$ LANGUAGE PLPGSQL; SELECT * FROM fn_api_products_by_names('A%'); CREATE OR REPLACE FUNCTION fn_all_orders_greater() RETURNS SETOF order_details as $$ DECLARE r record; BEGIN for r in select * from order_details where unit_price > 100 loop return next r; end loop; return; end; $$ language plpgsql; select * from fn_all_orders_greater();","title":"Returning specific Query from column"},{"location":"sql/functions/pl-pgsql/#if-data-not-found-condition","text":"DO $$ DECLARE rec record; orderid smallint = 1; BEGIN SELECT customer_id, order_date FROM orders INTO STRICT rec WHERE order_id = orderid; EXCEPTION WHEN NO_DATA_FOUND THEN RAISE EXCEPTION 'No order id was found'; END; $$ LANGUAGE PLPGSQL;","title":"If data not found condition"},{"location":"sql/functions/pl-pgsql/#throwing-execption-on-condition","text":"DO $$ DECLARE rec record; orderid smallint = 1; BEGIN SELECT customer_id, order_date FROM orders INTO STRICT rec WHERE order_id > 1000; EXCEPTION WHEN TOO_MANY_ROWS THEN RAISE EXCEPTION 'Too many rows were found'; END; $$ LANGUAGE PLPGSQL;","title":"Throwing execption on condition"},{"location":"sql/functions/pl-pgsql/#throwing-execption-example","text":"CREATE OR REPLACE FUNCTION fn_div_exception (x real, y real) RETURNS real as $$ DECLARE ret real; BEGIN ret := x / y; return ret; EXCEPTION WHEN division_by_zero then RAISE INFO 'division by zero error'; RAISE INFO 'ERROR % %', SQLSTATE, SQLERRM; END; $$ LANGUAGE PLPGSQL; SELECT fn_div_exception(5,0);","title":"Throwing execption example"},{"location":"sql/functions/stored-procedures/","text":"Stored Procedures They are compiled objects The procedure allows SELECT as well as DML(INSERT/UPDATE/DELETE) statement in it whereas Function allows only SELECT statement in it. Procedures cannot be utilized in a SELECT statement whereas Function can be embedded in a SELECT statement. Stored Procedures cannot be used in the SQL statements anywhere in the WHERE/HAVING/SELECT section whereas Function can be. Functions that return tables can be treated as another rowset. This can be used in JOINs with other tables. Inline Function can be though of as views that take parameters and can be used in JOINs and other Rowset operations. An exception can be handled by try-catch block in a Procedure whereas try-catch block cannot be used in a Function. We can use Transactions in Procedure whereas we can't use Transactions in Function. Sample Data CREATE TABLE t_accounts ( recid SERIAL PRIMARY KEY, name VARCHAR NOT NULL, balance dec(15,2) NOT NULL ); drop table t_accounts; INSERT INTO t_accounts (name,balance) values ('Adam',100),('Linda',100); select * from t_accounts; Creating Procedure CREATE OR REPLACE PROCEDURE pr_money_transfer (sender int, receiver int, amount dec) AS $$ BEGIN UPDATE t_accounts SET balance = balance - amount WHERE recid = sender; UPDATE t_accounts SET balance = balance + amount WHERE recid = receiver; COMMIT; END; $$ LANGUAGE PLPGSQL; CALL pr_money_transfer(1,2,30); select * from t_accounts; Another Example CREATE OR REPLACE PROCEDURE pr_orders_count(INOUT total_count INTEGER DEFAULT 0 ) AS $$ BEGIN SELECT COUNT(*) INTO total_count FROM orders; END; $$ LANGUAGE PLPGSQL; CALL pr_orders_count();","title":"Stored Procedures"},{"location":"sql/functions/stored-procedures/#stored-procedures","text":"They are compiled objects The procedure allows SELECT as well as DML(INSERT/UPDATE/DELETE) statement in it whereas Function allows only SELECT statement in it. Procedures cannot be utilized in a SELECT statement whereas Function can be embedded in a SELECT statement. Stored Procedures cannot be used in the SQL statements anywhere in the WHERE/HAVING/SELECT section whereas Function can be. Functions that return tables can be treated as another rowset. This can be used in JOINs with other tables. Inline Function can be though of as views that take parameters and can be used in JOINs and other Rowset operations. An exception can be handled by try-catch block in a Procedure whereas try-catch block cannot be used in a Function. We can use Transactions in Procedure whereas we can't use Transactions in Function.","title":"Stored Procedures"},{"location":"sql/functions/stored-procedures/#sample-data","text":"CREATE TABLE t_accounts ( recid SERIAL PRIMARY KEY, name VARCHAR NOT NULL, balance dec(15,2) NOT NULL ); drop table t_accounts; INSERT INTO t_accounts (name,balance) values ('Adam',100),('Linda',100); select * from t_accounts;","title":"Sample Data"},{"location":"sql/functions/stored-procedures/#creating-procedure","text":"CREATE OR REPLACE PROCEDURE pr_money_transfer (sender int, receiver int, amount dec) AS $$ BEGIN UPDATE t_accounts SET balance = balance - amount WHERE recid = sender; UPDATE t_accounts SET balance = balance + amount WHERE recid = receiver; COMMIT; END; $$ LANGUAGE PLPGSQL; CALL pr_money_transfer(1,2,30); select * from t_accounts;","title":"Creating Procedure"},{"location":"sql/functions/stored-procedures/#another-example","text":"CREATE OR REPLACE PROCEDURE pr_orders_count(INOUT total_count INTEGER DEFAULT 0 ) AS $$ BEGIN SELECT COUNT(*) INTO total_count FROM orders; END; $$ LANGUAGE PLPGSQL; CALL pr_orders_count();","title":"Another Example"},{"location":"sql/functions/triggers/","text":"Triggers A postgresql trigger is a functoin invoked automatically whenever 'an event' associated with a table occurs. An event could be any of the following; INSERT UDPATE DELETE TRUNCATE You can associate a trigger with a Table View Foreign Table A trigger is a special 'user-defined function' A trigger is automatically invoked We can create a triger BEFORE Trigger is fired before an event is about to happen AFTER Trigger is fired after the event is completed INSTEAD In case the event fails, trigger is fired Cannot be fired manually Fired in alphbetically order DO Not change in primary key, foriegn key or unique key column DO Not update records in the table that you normally read during the transaction DO Not read data from a table that is updating during the same transaction DO Not aggregate/summarized over the table that you are updating Types of Triggers Row level If row is marked for FOR EACH ROW , then trigger will be called for each row that is getting modfied by the event Statement level The FOR EACH STATEMENT will call the trigger function only ONCE for each statement, regardless of the number of rows getting modified. When Event Row-level Statement-level INSERT/UDPATE/DELETE Tables Tables and view before Truncate ---------- -------------------- --------- --------------- INSERT/UDPATE/DELETE Tables Tables and view AFTER Truncate ---------- -------------------- --------- --------------- INSERT/UDPATE/DELETE Views INSTEAD OF Truncate Create your own Trigger in PostgreSQL CREATE FUNCTION trigger_function( ) RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN -- TRIGGER LOGIC END; $$ Syntax CREATE TRIGGER trigger_name {BEFORE|AFTER} {EVENT} ON table_name [FOR [EACH] {ROW | STATEMENT}] EXECUTE PROCEDURE trigger_function -- FOR EACH [ROW|STATEMENT] Data Auditing with Triggers Setup Example tables CREATE TABLE players ( player_id SERIAL PRIMARY KEY, name VARCHAR(100) ); CREATE TABLE player_audits ( player_audit_id SERIAL PRIMARY KEY, player_id INT NOT NULL, name VARCHAR(100) NOT NULL, new_name VARCHAR(100) NOT NULL, edit_date TIMESTAMP NOT NULL ); Setup Function to TRIGGER -- Function executed by TRIGGER CREATE OR REPLACE FUNCTION fn_players_name_changes_log() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN IF NEW.name <> OLD.name THEN INSERT INTO player_audits ( player_id, name, new_name, edit_date ) values ( OLD.player_id, OLD.name, NEW.name ,NOW() ); END IF; RETURN NEW; END; $$ -- TRIGGER Definition CREATE TRIGGER trg_players_name_changes BEFORE UPDATE ON players FOR EACH ROW EXECUTE PROCEDURE fn_players_name_changes_log(); DML to fire above trigger INSERT INTO players (name) VALUES ('UDAY'),('YADAV'); SELECT * FROM players; select * from player_audits; UPDATE players SET name = 'UDAY 2' WHERE player_id = 2; Another Trigger Example -- create table for example CREATE TABLE t_temperature_log ( id_temperature SERIAL PRIMARY KEY, add_date TIMESTAMP, temperature NUMERIC ); -- create function for trigger to execute CREATE OR REPLACE FUNCTION fn_temperature_value_check_at_insert() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN IF NEW.temperature < -30 then NEW.temperature = 0; END IF; RETURN NEW; END; $$ -- creating trigger CREATE TRIGGER trg_temperature_value_check_at_insert BEFORE INSERT ON t_temperature_log FOR EACH ROW EXECUTE PROCEDURE fn_temperature_value_check_at_insert(); -- Queries INSERT INTO t_temperature_log ( add_date, temperature ) values ( '2020-10-01' , 10 ); select * from t_temperature_log; INSERT INTO t_temperature_log ( add_date, temperature ) values ( '2020-10-01' , -33 ); select * from t_temperature_log;","title":"Triggers"},{"location":"sql/functions/triggers/#triggers","text":"A postgresql trigger is a functoin invoked automatically whenever 'an event' associated with a table occurs. An event could be any of the following; INSERT UDPATE DELETE TRUNCATE You can associate a trigger with a Table View Foreign Table A trigger is a special 'user-defined function' A trigger is automatically invoked We can create a triger BEFORE Trigger is fired before an event is about to happen AFTER Trigger is fired after the event is completed INSTEAD In case the event fails, trigger is fired Cannot be fired manually Fired in alphbetically order DO Not change in primary key, foriegn key or unique key column DO Not update records in the table that you normally read during the transaction DO Not read data from a table that is updating during the same transaction DO Not aggregate/summarized over the table that you are updating","title":"Triggers"},{"location":"sql/functions/triggers/#types-of-triggers","text":"Row level If row is marked for FOR EACH ROW , then trigger will be called for each row that is getting modfied by the event Statement level The FOR EACH STATEMENT will call the trigger function only ONCE for each statement, regardless of the number of rows getting modified. When Event Row-level Statement-level INSERT/UDPATE/DELETE Tables Tables and view before Truncate ---------- -------------------- --------- --------------- INSERT/UDPATE/DELETE Tables Tables and view AFTER Truncate ---------- -------------------- --------- --------------- INSERT/UDPATE/DELETE Views INSTEAD OF Truncate","title":"Types of Triggers"},{"location":"sql/functions/triggers/#create-your-own-trigger-in-postgresql","text":"CREATE FUNCTION trigger_function( ) RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN -- TRIGGER LOGIC END; $$","title":"Create your own Trigger in PostgreSQL"},{"location":"sql/functions/triggers/#syntax","text":"CREATE TRIGGER trigger_name {BEFORE|AFTER} {EVENT} ON table_name [FOR [EACH] {ROW | STATEMENT}] EXECUTE PROCEDURE trigger_function -- FOR EACH [ROW|STATEMENT]","title":"Syntax"},{"location":"sql/functions/triggers/#data-auditing-with-triggers","text":"","title":"Data Auditing with Triggers"},{"location":"sql/functions/triggers/#setup-example-tables","text":"CREATE TABLE players ( player_id SERIAL PRIMARY KEY, name VARCHAR(100) ); CREATE TABLE player_audits ( player_audit_id SERIAL PRIMARY KEY, player_id INT NOT NULL, name VARCHAR(100) NOT NULL, new_name VARCHAR(100) NOT NULL, edit_date TIMESTAMP NOT NULL );","title":"Setup Example tables"},{"location":"sql/functions/triggers/#setup-function-to-trigger","text":"-- Function executed by TRIGGER CREATE OR REPLACE FUNCTION fn_players_name_changes_log() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN IF NEW.name <> OLD.name THEN INSERT INTO player_audits ( player_id, name, new_name, edit_date ) values ( OLD.player_id, OLD.name, NEW.name ,NOW() ); END IF; RETURN NEW; END; $$ -- TRIGGER Definition CREATE TRIGGER trg_players_name_changes BEFORE UPDATE ON players FOR EACH ROW EXECUTE PROCEDURE fn_players_name_changes_log();","title":"Setup Function to TRIGGER"},{"location":"sql/functions/triggers/#dml-to-fire-above-trigger","text":"INSERT INTO players (name) VALUES ('UDAY'),('YADAV'); SELECT * FROM players; select * from player_audits; UPDATE players SET name = 'UDAY 2' WHERE player_id = 2;","title":"DML to fire above trigger"},{"location":"sql/functions/triggers/#another-trigger-example","text":"-- create table for example CREATE TABLE t_temperature_log ( id_temperature SERIAL PRIMARY KEY, add_date TIMESTAMP, temperature NUMERIC ); -- create function for trigger to execute CREATE OR REPLACE FUNCTION fn_temperature_value_check_at_insert() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN IF NEW.temperature < -30 then NEW.temperature = 0; END IF; RETURN NEW; END; $$ -- creating trigger CREATE TRIGGER trg_temperature_value_check_at_insert BEFORE INSERT ON t_temperature_log FOR EACH ROW EXECUTE PROCEDURE fn_temperature_value_check_at_insert(); -- Queries INSERT INTO t_temperature_log ( add_date, temperature ) values ( '2020-10-01' , 10 ); select * from t_temperature_log; INSERT INTO t_temperature_log ( add_date, temperature ) values ( '2020-10-01' , -33 ); select * from t_temperature_log;","title":"Another Trigger Example"},{"location":"sql/functions/triggers/more-on-triggers/","text":"More on Triggers Getting Internal Information CREATE OR REPLACE FUNCTION fn_trigger_variables_display() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN RAISE NOTICE 'TG_NAME: %', TG_NAME; RAISE NOTICE 'TG_RELNAME: %', TG_RELNAME; RAISE NOTICE 'TG_TABLE_SCHEMA: %', TG_TABLE_SCHEMA; RAISE NOTICE 'TG_WHEN: %', TG_WHEN; RAISE NOTICE 'TG_LEVEL: %', TG_LEVEL; RAISE NOTICE 'TG_OP: %', TG_OP; RAISE NOTICE 'TG_NARGS: %', TG_NARGS; RAISE NOTICE 'TG_ARGV: %', TG_NAME; RETURN NEW; END; $$ CREATE TRIGGER trg_trigger_variables_display AFTER INSERT ON t_temperature_log FOR EACH ROW EXECUTE PROCEDURE fn_trigger_variables_display(); INSERT INTO t_temperature_log ( add_date, temperature ) values ('2020-02-02', -40); Disallow DELETE on table -- create sample table create table test_delete ( id int ); insert into test_delete (id ) values (1),(2),(3); select * from test_delete; -- creating function create or replace function fn_generic_cancel_op() returns trigger language plpgsql as $$ begin if TG_WHEN = 'AFTER' THEN raise exception 'you are not allowed to % rows in %.%', tg_op,tg_table_schema,tg_table_name; end if; raise notice '% on rows in %.% wont happen', tg_op, tg_table_schema, tg_table_name; return null; end; $$ -- creating trigger : AFTER CREATE TRIGGER trg_disallow_delete AFTER DELETE ON test_delete FOR EACH ROW EXECUTE PROCEDURE fn_generic_cancel_op(); delete from test_delete where id = 1; -- creating trigger : BEFORE CREATE TRIGGER trg_disallow_delete_before BEFORE DELETE ON test_delete FOR EACH ROW EXECUTE PROCEDURE fn_generic_cancel_op(); delete from test_delete where id = 1; -- checking select * from test_delete; Disallow truncating CREATE TRIGGER trg_disallow_truncate_after AFTER TRUNCATE ON test_delete FOR EACH STATEMENT EXECUTE PROCEDURE fn_generic_cancel_op(); CREATE TRIGGER trg_disallow_truncate_beforE BEFORE TRUNCATE ON test_delete FOR EACH STATEMENT EXECUTE PROCEDURE fn_generic_cancel_op(); Creating Audit Trigger To log data changes to tables in a consistent and transparent manner. CREATE TABLE audit ( id INT ); CREATE TABLE audit_log ( username TEXT, add_time TIMESTAMP, table_name TEXT, operation TEXT, row_before JSON, row_after JSON ); Please not that new OLD are not null for DELETE and INSERT triggers. CREATE OR REPLACE FUNCTION fn_audit_trigger() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN DECLARE old_row json = NULL; new_row json = NULL; BEGIN IF TG_OP IN ('UPDATE','DELETE') THEN old_row = row_to_json(OLD); END IF; IF TG_OP IN ('INSERT','UPDATE') THEN new_row = row_to_json(NEW); END IF; INSERT INTO audit_log ( username, add_time, table_name, operation, row_before, row_after ) values ( session_user, NOW(), TG_TABLE_SCHEMA || '.' || TG_TABLE_NAME, TG_OP, old_row, new_row ); RETURN NEW; END; END; $$ -- bind trigger CREATE TRIGGER trg_audit_trigger AFTER INSERT OR UPDATE OR DELETE ON audit FOR EACH ROW EXECUTE PROCEDURE fn_audit_trigger(); -- Queries insert into audit(id) values (1),(2),(3); update audit set id = '100' where id = 1; delete from audit where id = 2; select * from audit; select * from audit_log; Creating Conditional Triggers Created by using generic WHEN clause. With a WHEN clause, you can write some conditions except a subquery -- sample table CREATE TABLE mytask ( task_id SERIAL PRIMARY KEY, task text ); -- trigger function CREATE OR REPLACE FUNCTION fn_cancel_with_message() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN RAISE EXCEPTION '%', TG_ARGV[0]; RETURN NULL; END; $$ -- function binding to trigger CREATE TRIGGER trg_no_update BEFORE INSERT OR UPDATE OR DELETE OR TRUNCATE ON mytask FOR EACH STATEMENT WHEN ( EXTRACT ('DOW' FROM CURRENT_TIMESTAMP) = 5 -- 5 means friday AND CURRENT_TIME > '12:00' ) EXECUTE PROCEDURE fn_cancel_with_message('NO UPDATE ARE ALLOWED'); -- Queries INSERT INTO mytask (task) values ('task 1'), ('task 2'), ('task 3'); Disallow updating Primary Key of table create table pg_table( id serial primary key, t text ); insert into pg_table(t) values ('t1'),('t2'); create trigger disallow_pk_change after update of id on pg_table for each row execute procedure fn_generic_cancel_op(); update pg_table set id = 10 where id = 1; Event Triggers Event triggers are data-specific and not bind or attached to a table Unlike regular triggers they capture system level DLL events Event triggers can be BEFORE or AFTER triggers Trigger function can be written in any language except SQL Event triggers are disabled in the single user mode and can only be created by a superuser Syntax : CREATE EVENT TRIGGER trg_name Before creating an event trigger, we must have a function that the trigger will execute The function must return a specifi type called EVENT_TRIGGER This function need not (and may not) return a valuel the return type serivces merely as s signal that the function is to be invoked as an event trigger. Can we create conditional event trigger ? Yes, using the when clause Event trigger cannot be executed in an aborted transaction Event trigger events when explaination ddl_command_start This event occurs jsut BEFORE a CREATE, ALTER, or DROP DLL command is executed ddl_command_end This event occurs just AFTER a create, alter, or drop command has finished executing table_rewrite This event occurs just before a table is re written by some action of the commands ALTER TABLE and ALTER TYPE . sql_drop This evetn occurs just before the ddl_command_end eevent for the commands that frop database objects Event trigger variables TG_TAG : this variable contains the 'TAG' or the command for which the trigger is executed. TG_EVENT : This variable contains the event name, which can be ddl_command_start, ddl_comman_end, and sql_drop. Creating an auditing event trigger CREATE TABLE audit_dll ( audit_ddl_id SERIAL PRIMARY KEY, username TEXT, ddl_event TEXT, ddl_command TEXT, ddl_add_time TIMESTAMPTZ ); CREATE OR REPLACE FUNCTION fn_event_audit_ddl() RETURNS EVENT_TRIGGER LANGUAGE PLPGSQL SECURITY DEFINER AS $$ BEGIN INSERT INTO public.audit_dll (username, ddl_event, ddl_command, ddl_add_time) VALUES (session_user, TG_EVENT, TG_TAG, NOW()); RAISE NOTICE 'DDL activity is created'; END; $$ -- without condition create event trigger trg_event_audit_ddl_no_cond on ddl_command_start execute procedure fn_event_audit_ddl(); -- with condition create event trigger trg_event_audit_ddl on ddl_command_start when TAG IN ('CREATE TABLE') execute procedure fn_event_audit_ddl(); Dont allow anyone to create table between time CREATE OR REPLACE FUNCTION fn_event_abort_create_table_func() RETURNS EVENT_TRIGGER LANGUAGE PLPGSQL SECURITY DEFINER AS $$ DECLARE current_hour int = EXTRACT ('HOUR' FROM NOW()); BEGIN IF current_hour between 4 and 16 then RAISE EXCEPTION 'tables are not allowed to be created during 9-4'; END IF; END; $$ CREATE EVENT TRIGGER trg_event_create_table_function ON ddl_command_start WHEN TAG IN ('CREATE TABLE') EXECUTE PROCEDURE fn_event_abort_create_table_func(); Dropping event trigger drop event trigger trg_name;","title":"More on Triggers"},{"location":"sql/functions/triggers/more-on-triggers/#more-on-triggers","text":"","title":"More on Triggers"},{"location":"sql/functions/triggers/more-on-triggers/#getting-internal-information","text":"CREATE OR REPLACE FUNCTION fn_trigger_variables_display() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN RAISE NOTICE 'TG_NAME: %', TG_NAME; RAISE NOTICE 'TG_RELNAME: %', TG_RELNAME; RAISE NOTICE 'TG_TABLE_SCHEMA: %', TG_TABLE_SCHEMA; RAISE NOTICE 'TG_WHEN: %', TG_WHEN; RAISE NOTICE 'TG_LEVEL: %', TG_LEVEL; RAISE NOTICE 'TG_OP: %', TG_OP; RAISE NOTICE 'TG_NARGS: %', TG_NARGS; RAISE NOTICE 'TG_ARGV: %', TG_NAME; RETURN NEW; END; $$ CREATE TRIGGER trg_trigger_variables_display AFTER INSERT ON t_temperature_log FOR EACH ROW EXECUTE PROCEDURE fn_trigger_variables_display(); INSERT INTO t_temperature_log ( add_date, temperature ) values ('2020-02-02', -40);","title":"Getting Internal Information"},{"location":"sql/functions/triggers/more-on-triggers/#disallow-delete-on-table","text":"-- create sample table create table test_delete ( id int ); insert into test_delete (id ) values (1),(2),(3); select * from test_delete; -- creating function create or replace function fn_generic_cancel_op() returns trigger language plpgsql as $$ begin if TG_WHEN = 'AFTER' THEN raise exception 'you are not allowed to % rows in %.%', tg_op,tg_table_schema,tg_table_name; end if; raise notice '% on rows in %.% wont happen', tg_op, tg_table_schema, tg_table_name; return null; end; $$ -- creating trigger : AFTER CREATE TRIGGER trg_disallow_delete AFTER DELETE ON test_delete FOR EACH ROW EXECUTE PROCEDURE fn_generic_cancel_op(); delete from test_delete where id = 1; -- creating trigger : BEFORE CREATE TRIGGER trg_disallow_delete_before BEFORE DELETE ON test_delete FOR EACH ROW EXECUTE PROCEDURE fn_generic_cancel_op(); delete from test_delete where id = 1; -- checking select * from test_delete;","title":"Disallow DELETE on table"},{"location":"sql/functions/triggers/more-on-triggers/#disallow-truncating","text":"CREATE TRIGGER trg_disallow_truncate_after AFTER TRUNCATE ON test_delete FOR EACH STATEMENT EXECUTE PROCEDURE fn_generic_cancel_op(); CREATE TRIGGER trg_disallow_truncate_beforE BEFORE TRUNCATE ON test_delete FOR EACH STATEMENT EXECUTE PROCEDURE fn_generic_cancel_op();","title":"Disallow truncating"},{"location":"sql/functions/triggers/more-on-triggers/#creating-audit-trigger","text":"To log data changes to tables in a consistent and transparent manner. CREATE TABLE audit ( id INT ); CREATE TABLE audit_log ( username TEXT, add_time TIMESTAMP, table_name TEXT, operation TEXT, row_before JSON, row_after JSON ); Please not that new OLD are not null for DELETE and INSERT triggers. CREATE OR REPLACE FUNCTION fn_audit_trigger() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN DECLARE old_row json = NULL; new_row json = NULL; BEGIN IF TG_OP IN ('UPDATE','DELETE') THEN old_row = row_to_json(OLD); END IF; IF TG_OP IN ('INSERT','UPDATE') THEN new_row = row_to_json(NEW); END IF; INSERT INTO audit_log ( username, add_time, table_name, operation, row_before, row_after ) values ( session_user, NOW(), TG_TABLE_SCHEMA || '.' || TG_TABLE_NAME, TG_OP, old_row, new_row ); RETURN NEW; END; END; $$ -- bind trigger CREATE TRIGGER trg_audit_trigger AFTER INSERT OR UPDATE OR DELETE ON audit FOR EACH ROW EXECUTE PROCEDURE fn_audit_trigger(); -- Queries insert into audit(id) values (1),(2),(3); update audit set id = '100' where id = 1; delete from audit where id = 2; select * from audit; select * from audit_log;","title":"Creating Audit Trigger"},{"location":"sql/functions/triggers/more-on-triggers/#creating-conditional-triggers","text":"Created by using generic WHEN clause. With a WHEN clause, you can write some conditions except a subquery -- sample table CREATE TABLE mytask ( task_id SERIAL PRIMARY KEY, task text ); -- trigger function CREATE OR REPLACE FUNCTION fn_cancel_with_message() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN RAISE EXCEPTION '%', TG_ARGV[0]; RETURN NULL; END; $$ -- function binding to trigger CREATE TRIGGER trg_no_update BEFORE INSERT OR UPDATE OR DELETE OR TRUNCATE ON mytask FOR EACH STATEMENT WHEN ( EXTRACT ('DOW' FROM CURRENT_TIMESTAMP) = 5 -- 5 means friday AND CURRENT_TIME > '12:00' ) EXECUTE PROCEDURE fn_cancel_with_message('NO UPDATE ARE ALLOWED'); -- Queries INSERT INTO mytask (task) values ('task 1'), ('task 2'), ('task 3');","title":"Creating Conditional Triggers"},{"location":"sql/functions/triggers/more-on-triggers/#disallow-updating-primary-key-of-table","text":"create table pg_table( id serial primary key, t text ); insert into pg_table(t) values ('t1'),('t2'); create trigger disallow_pk_change after update of id on pg_table for each row execute procedure fn_generic_cancel_op(); update pg_table set id = 10 where id = 1;","title":"Disallow updating Primary Key of table"},{"location":"sql/functions/triggers/more-on-triggers/#event-triggers","text":"Event triggers are data-specific and not bind or attached to a table Unlike regular triggers they capture system level DLL events Event triggers can be BEFORE or AFTER triggers Trigger function can be written in any language except SQL Event triggers are disabled in the single user mode and can only be created by a superuser Syntax : CREATE EVENT TRIGGER trg_name Before creating an event trigger, we must have a function that the trigger will execute The function must return a specifi type called EVENT_TRIGGER This function need not (and may not) return a valuel the return type serivces merely as s signal that the function is to be invoked as an event trigger. Can we create conditional event trigger ? Yes, using the when clause Event trigger cannot be executed in an aborted transaction","title":"Event Triggers"},{"location":"sql/functions/triggers/more-on-triggers/#event-trigger-events","text":"when explaination ddl_command_start This event occurs jsut BEFORE a CREATE, ALTER, or DROP DLL command is executed ddl_command_end This event occurs just AFTER a create, alter, or drop command has finished executing table_rewrite This event occurs just before a table is re written by some action of the commands ALTER TABLE and ALTER TYPE . sql_drop This evetn occurs just before the ddl_command_end eevent for the commands that frop database objects","title":"Event trigger events"},{"location":"sql/functions/triggers/more-on-triggers/#event-trigger-variables","text":"TG_TAG : this variable contains the 'TAG' or the command for which the trigger is executed. TG_EVENT : This variable contains the event name, which can be ddl_command_start, ddl_comman_end, and sql_drop.","title":"Event trigger variables"},{"location":"sql/functions/triggers/more-on-triggers/#creating-an-auditing-event-trigger","text":"CREATE TABLE audit_dll ( audit_ddl_id SERIAL PRIMARY KEY, username TEXT, ddl_event TEXT, ddl_command TEXT, ddl_add_time TIMESTAMPTZ ); CREATE OR REPLACE FUNCTION fn_event_audit_ddl() RETURNS EVENT_TRIGGER LANGUAGE PLPGSQL SECURITY DEFINER AS $$ BEGIN INSERT INTO public.audit_dll (username, ddl_event, ddl_command, ddl_add_time) VALUES (session_user, TG_EVENT, TG_TAG, NOW()); RAISE NOTICE 'DDL activity is created'; END; $$ -- without condition create event trigger trg_event_audit_ddl_no_cond on ddl_command_start execute procedure fn_event_audit_ddl(); -- with condition create event trigger trg_event_audit_ddl on ddl_command_start when TAG IN ('CREATE TABLE') execute procedure fn_event_audit_ddl();","title":"Creating an auditing event trigger"},{"location":"sql/functions/triggers/more-on-triggers/#dont-allow-anyone-to-create-table-between-time","text":"CREATE OR REPLACE FUNCTION fn_event_abort_create_table_func() RETURNS EVENT_TRIGGER LANGUAGE PLPGSQL SECURITY DEFINER AS $$ DECLARE current_hour int = EXTRACT ('HOUR' FROM NOW()); BEGIN IF current_hour between 4 and 16 then RAISE EXCEPTION 'tables are not allowed to be created during 9-4'; END IF; END; $$ CREATE EVENT TRIGGER trg_event_create_table_function ON ddl_command_start WHEN TAG IN ('CREATE TABLE') EXECUTE PROCEDURE fn_event_abort_create_table_func();","title":"Dont allow anyone to create table between time"},{"location":"sql/functions/triggers/more-on-triggers/#dropping-event-trigger","text":"drop event trigger trg_name;","title":"Dropping event trigger"},{"location":"sql/getting-started/","text":"Getting Started Starting Database with Docker docker run --name <docker_name> -e POSTGRES_PASSWORD=<password> -d -p 5432:5432 postgres:13.3 docker exec -it <docker_name> bash psql -U postgres Installing PostgreSQL on windows https://www.postgresql.org/download/windows/ Installing PostgreSQL on MacOS https://www.postgresql.org/download/macosx/ Setup and Basics : using apt Installation sudo apt-get install postgresql Usage commands service postgresql Switch to default user sudo su postgres Getting Started Connect to a database Connection options: -h, --host=HOSTNAME # database server host or socket directory # (default: \"local socket\") -p, --port=PORT # database server port # (default: \"5432\") -U, --username=USERNAME # database user name (default: \"root\") -w, --no-password # never prompt for password -W, --password # force password prompt (should happen automatically) Here port : 5432 is default and can be get from psql --help postgres is the super user . Create another user and connect using that. to connect to database with user : \\c db_name user_name Commands -- to list all tables \\dt -- to list all databases \\l -- to list all table spaces \\db -- to list all schemas \\dn -- to list all indices \\di -- to list all sequences \\ds -- to list all roles \\dg -- to list data types \\dT -- to list all domain datatypes \\dD -- to list all views \\dv -- to list previous commands \\g -- to list Command History \\s -- to run command from file \\i filename -- it should be inside the server -- to list help \\h \\h create table -- to display null \\pset null (null) -- to make terminal better \\pset linestyle unicode \\pset border 2 -- to watch a command in time \\watch 2 -- to turn on timing \\timing Load Data Download the sample data file from here https://drive.google.com/file/d/1vsFVuybjNDacNaV5LmaSI__a6dCpEdmT/view?usp=sharing","title":"Getting Started"},{"location":"sql/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"sql/getting-started/#starting-database-with-docker","text":"docker run --name <docker_name> -e POSTGRES_PASSWORD=<password> -d -p 5432:5432 postgres:13.3 docker exec -it <docker_name> bash psql -U postgres","title":"Starting Database with Docker"},{"location":"sql/getting-started/#installing-postgresql-on-windows","text":"https://www.postgresql.org/download/windows/","title":"Installing PostgreSQL on windows"},{"location":"sql/getting-started/#installing-postgresql-on-macos","text":"https://www.postgresql.org/download/macosx/","title":"Installing PostgreSQL on MacOS"},{"location":"sql/getting-started/#setup-and-basics-using-apt","text":"Installation sudo apt-get install postgresql Usage commands service postgresql Switch to default user sudo su postgres","title":"Setup and Basics : using apt"},{"location":"sql/getting-started/#getting-started_1","text":"","title":"Getting Started"},{"location":"sql/getting-started/#connect-to-a-database","text":"Connection options: -h, --host=HOSTNAME # database server host or socket directory # (default: \"local socket\") -p, --port=PORT # database server port # (default: \"5432\") -U, --username=USERNAME # database user name (default: \"root\") -w, --no-password # never prompt for password -W, --password # force password prompt (should happen automatically) Here port : 5432 is default and can be get from psql --help postgres is the super user . Create another user and connect using that. to connect to database with user : \\c db_name user_name","title":"Connect to a database"},{"location":"sql/getting-started/#commands","text":"-- to list all tables \\dt -- to list all databases \\l -- to list all table spaces \\db -- to list all schemas \\dn -- to list all indices \\di -- to list all sequences \\ds -- to list all roles \\dg -- to list data types \\dT -- to list all domain datatypes \\dD -- to list all views \\dv -- to list previous commands \\g -- to list Command History \\s -- to run command from file \\i filename -- it should be inside the server -- to list help \\h \\h create table -- to display null \\pset null (null) -- to make terminal better \\pset linestyle unicode \\pset border 2 -- to watch a command in time \\watch 2 -- to turn on timing \\timing","title":"Commands"},{"location":"sql/getting-started/#load-data","text":"Download the sample data file from here https://drive.google.com/file/d/1vsFVuybjNDacNaV5LmaSI__a6dCpEdmT/view?usp=sharing","title":"Load Data"},{"location":"sql/getting-started/administration-task/","text":"PgAdmin Tool Creating New User Options General Name : username Comment : description Definition Password : user password Account Expires Connection Limit : number of connections that user can have to DB, default -1 Refer to documentation for more details https://www.pgadmin.org/docs/pgadmin4/development/role_dialog.html Query Editor Load Data using pgAdmin4","title":"PgAdmin Tool"},{"location":"sql/getting-started/administration-task/#pgadmin-tool","text":"","title":"PgAdmin Tool"},{"location":"sql/getting-started/administration-task/#creating-new-user","text":"","title":"Creating New User"},{"location":"sql/getting-started/administration-task/#options","text":"General Name : username Comment : description Definition Password : user password Account Expires Connection Limit : number of connections that user can have to DB, default -1 Refer to documentation for more details https://www.pgadmin.org/docs/pgadmin4/development/role_dialog.html","title":"Options"},{"location":"sql/getting-started/administration-task/#query-editor","text":"","title":"Query Editor"},{"location":"sql/getting-started/administration-task/#load-data-using-pgadmin4","text":"","title":"Load Data using pgAdmin4"},{"location":"sql/indexing/","text":"Indexing What is a Index in Database An index help improve the access of data in our database Indexed tuple point to the table page where the tuple is stored on disk. An Index is a data structure that allows faster access to the underlying table so that specific tuples can be found quickly . Here \"quickly\" means much faster than scanning the entire table and analysing every single tuple. They add a cost to running a query, consume more memory to maintain the data structure. INDEX : idx_table_name_column_name UNIQUE INDEX : idx_u_table_name_column_name Syntax create index index_name on table_name (col1,col2,.....) -- create index for only unique values in column create unique index index_name on table_name (col1,col2,.....) -- more descriptive method create index index_name on table_name [USING method] ( column_name [ASC|DESC] [NULLS {FIRST | LAST}], ... ); create index idx_orders_order_date on orders (order_date); create index idx_orders_ship_city on orders (ship_city); create index idx_orders_customer_id_order_id on orders (customer_id,order_id); CREATE INDEX idx_shippers_company_name ON public.shippers USING btree (company_name ASC NULLS LAST); Table \"public.orders\" Column | Type | Collation | Nullable | Default ------------------+-----------------------+-----------+----------+--------- order_id | smallint | | not null | customer_id | bpchar | | | employee_id | smallint | | | order_date | date | | | required_date | date | | | shipped_date | date | | | ship_via | smallint | | | freight | real | | | ship_name | character varying(40) | | | ship_address | character varying(60) | | | ship_city | character varying(15) | | | ship_region | character varying(15) | | | ship_postal_code | character varying(10) | | | ship_country | character varying(15) | | | -- indexes present on table Indexes: \"pk_orders\" PRIMARY KEY, btree (order_id) \"idx_orders_customer_id_order_id\" btree (customer_id, order_id) \"idx_orders_order_date\" btree (order_date) \"idx_orders_ship_city\" btree (ship_city) Foreign-key constraints: \"fk_orders_customers\" FOREIGN KEY (customer_id) REFERENCES customers(customer_id) \"fk_orders_employees\" FOREIGN KEY (employee_id) REFERENCES employees(employee_id) \"fk_orders_shippers\" FOREIGN KEY (ship_via) REFERENCES shippers(shipper_id) Referenced by: TABLE \"order_details\" CONSTRAINT \"fk_order_details_orders\" FOREIGN KEY (order_id) REFERENCES orders(order_id) Get All Indexes select * from pg_indexes limit 3; schemaname | tablename | indexname | tablespace | indexdef ------------+--------------+----------------------------------+------------+------------------------------------------------------------------------------------------------------------------------------- pg_catalog | pg_statistic | pg_statistic_relid_att_inh_index | | CREATE UNIQUE INDEX pg_statistic_relid_att_inh_index ON pg_catalog.pg_statistic USING btree (starelid, staattnum, stainherit) pg_catalog | pg_type | pg_type_oid_index | | CREATE UNIQUE INDEX pg_type_oid_index ON pg_catalog.pg_type USING btree (oid) pg_catalog | pg_type | pg_type_typname_nsp_index | | CREATE UNIQUE INDEX pg_type_typname_nsp_index ON pg_catalog.pg_type USING btree (typname, typnamespace) select * from pg_indexes where schemaname = 'public' limti 3; schemaname | tablename | indexname | tablespace | indexdef ------------+-----------+--------------+------------+-------------------------------------------------------------------------------- public | us_states | pk_usstates | | CREATE UNIQUE INDEX pk_usstates ON public.us_states USING btree (state_id) public | customers | pk_customers | | CREATE UNIQUE INDEX pk_customers ON public.customers USING btree (customer_id) public | orders | pk_orders | | CREATE UNIQUE INDEX pk_orders ON public.orders USING btree (order_id) Size of Indexes select pg_size_pretty(pg_indexes_size('tablename')); pg_size_pretty ---------------- 128 kB Stats about indexes select * from postgres.pg_catalog.pg_stat_all_indexes; relid | indexrelid | schemaname | relname | indexrelname | idx_scan | idx_tup_read | idx_tup_fetch -------+------------+------------+--------------+----------------------------------+----------+--------------+--------------- 2619 | 2696 | pg_catalog | pg_statistic | pg_statistic_relid_att_inh_index | 1592 | 1155 | 1155 1247 | 2703 | pg_catalog | pg_type | pg_type_oid_index | 924 | 924 | 911 1247 | 2704 | pg_catalog | pg_type | pg_type_typname_nsp_index | 162 | 120 | 120 (3 rows) Drop Index DROP INDEX [ concurrently ] [ IF EXISTS ] INDEX_NAME [ CASCADE | RESTRICT ]; CASCADE : If object has dependent objects, you will also drop the dependent ones after dropping it. RESTRICT : It denies the user to drop the index if a dependency exists CONCURRENTLY : PostgreSQL will require exclusive lock over the whole table and block access until index is removed Vacuum analyze \\ When a vacuum process runs, the space occupied by these dead tuples is marked reusable by other tuples. An \u201canalyze\u201d operation does what its name says \u2013 it analyzes the contents of a database's tables and collects statistics about the distribution of values in each column of every table. vacuum analyze table_name; Rebuilding Indexes REINDEX ( VERBOSE ) INDEX concurrently idx_orders_ship_city;","title":"Indexing"},{"location":"sql/indexing/#indexing","text":"","title":"Indexing"},{"location":"sql/indexing/#what-is-a-index-in-database","text":"An index help improve the access of data in our database Indexed tuple point to the table page where the tuple is stored on disk. An Index is a data structure that allows faster access to the underlying table so that specific tuples can be found quickly . Here \"quickly\" means much faster than scanning the entire table and analysing every single tuple. They add a cost to running a query, consume more memory to maintain the data structure. INDEX : idx_table_name_column_name UNIQUE INDEX : idx_u_table_name_column_name","title":"What is a Index in Database"},{"location":"sql/indexing/#syntax","text":"create index index_name on table_name (col1,col2,.....) -- create index for only unique values in column create unique index index_name on table_name (col1,col2,.....) -- more descriptive method create index index_name on table_name [USING method] ( column_name [ASC|DESC] [NULLS {FIRST | LAST}], ... ); create index idx_orders_order_date on orders (order_date); create index idx_orders_ship_city on orders (ship_city); create index idx_orders_customer_id_order_id on orders (customer_id,order_id); CREATE INDEX idx_shippers_company_name ON public.shippers USING btree (company_name ASC NULLS LAST); Table \"public.orders\" Column | Type | Collation | Nullable | Default ------------------+-----------------------+-----------+----------+--------- order_id | smallint | | not null | customer_id | bpchar | | | employee_id | smallint | | | order_date | date | | | required_date | date | | | shipped_date | date | | | ship_via | smallint | | | freight | real | | | ship_name | character varying(40) | | | ship_address | character varying(60) | | | ship_city | character varying(15) | | | ship_region | character varying(15) | | | ship_postal_code | character varying(10) | | | ship_country | character varying(15) | | | -- indexes present on table Indexes: \"pk_orders\" PRIMARY KEY, btree (order_id) \"idx_orders_customer_id_order_id\" btree (customer_id, order_id) \"idx_orders_order_date\" btree (order_date) \"idx_orders_ship_city\" btree (ship_city) Foreign-key constraints: \"fk_orders_customers\" FOREIGN KEY (customer_id) REFERENCES customers(customer_id) \"fk_orders_employees\" FOREIGN KEY (employee_id) REFERENCES employees(employee_id) \"fk_orders_shippers\" FOREIGN KEY (ship_via) REFERENCES shippers(shipper_id) Referenced by: TABLE \"order_details\" CONSTRAINT \"fk_order_details_orders\" FOREIGN KEY (order_id) REFERENCES orders(order_id)","title":"Syntax"},{"location":"sql/indexing/#get-all-indexes","text":"select * from pg_indexes limit 3; schemaname | tablename | indexname | tablespace | indexdef ------------+--------------+----------------------------------+------------+------------------------------------------------------------------------------------------------------------------------------- pg_catalog | pg_statistic | pg_statistic_relid_att_inh_index | | CREATE UNIQUE INDEX pg_statistic_relid_att_inh_index ON pg_catalog.pg_statistic USING btree (starelid, staattnum, stainherit) pg_catalog | pg_type | pg_type_oid_index | | CREATE UNIQUE INDEX pg_type_oid_index ON pg_catalog.pg_type USING btree (oid) pg_catalog | pg_type | pg_type_typname_nsp_index | | CREATE UNIQUE INDEX pg_type_typname_nsp_index ON pg_catalog.pg_type USING btree (typname, typnamespace) select * from pg_indexes where schemaname = 'public' limti 3; schemaname | tablename | indexname | tablespace | indexdef ------------+-----------+--------------+------------+-------------------------------------------------------------------------------- public | us_states | pk_usstates | | CREATE UNIQUE INDEX pk_usstates ON public.us_states USING btree (state_id) public | customers | pk_customers | | CREATE UNIQUE INDEX pk_customers ON public.customers USING btree (customer_id) public | orders | pk_orders | | CREATE UNIQUE INDEX pk_orders ON public.orders USING btree (order_id)","title":"Get All Indexes"},{"location":"sql/indexing/#size-of-indexes","text":"select pg_size_pretty(pg_indexes_size('tablename')); pg_size_pretty ---------------- 128 kB","title":"Size of Indexes"},{"location":"sql/indexing/#stats-about-indexes","text":"select * from postgres.pg_catalog.pg_stat_all_indexes; relid | indexrelid | schemaname | relname | indexrelname | idx_scan | idx_tup_read | idx_tup_fetch -------+------------+------------+--------------+----------------------------------+----------+--------------+--------------- 2619 | 2696 | pg_catalog | pg_statistic | pg_statistic_relid_att_inh_index | 1592 | 1155 | 1155 1247 | 2703 | pg_catalog | pg_type | pg_type_oid_index | 924 | 924 | 911 1247 | 2704 | pg_catalog | pg_type | pg_type_typname_nsp_index | 162 | 120 | 120 (3 rows)","title":"Stats about indexes"},{"location":"sql/indexing/#drop-index","text":"DROP INDEX [ concurrently ] [ IF EXISTS ] INDEX_NAME [ CASCADE | RESTRICT ]; CASCADE : If object has dependent objects, you will also drop the dependent ones after dropping it. RESTRICT : It denies the user to drop the index if a dependency exists CONCURRENTLY : PostgreSQL will require exclusive lock over the whole table and block access until index is removed","title":"Drop Index"},{"location":"sql/indexing/#vacuum-analyze","text":"\\ When a vacuum process runs, the space occupied by these dead tuples is marked reusable by other tuples. An \u201canalyze\u201d operation does what its name says \u2013 it analyzes the contents of a database's tables and collects statistics about the distribution of values in each column of every table. vacuum analyze table_name;","title":"Vacuum analyze"},{"location":"sql/indexing/#rebuilding-indexes","text":"REINDEX ( VERBOSE ) INDEX concurrently idx_orders_ship_city;","title":"Rebuilding Indexes"},{"location":"sql/indexing/custom-indexes/","text":"Custom Indexes Example : CREATE a index for social security number , which is the the format of 1111-222-nnnn, you have to index the last 4 characters CREATE TABLE if not exists ssn ( ssn text ); INSERT INTO ssn (ssn) values ('111-11-0100'), ('222-22-0120'), ('333-33-0140'), ('444-44-0160'); select * from ssn; explain select * from ssn; CREATE OR REPLACE FUNCTION FN_FIX_SSN(TEXT) RETURNS text AS $$ BEGIN return substring($1, 8) || replace(substring($1, 1, 7), '-', ''); end ; $$ LANGUAGE plpgsql IMMUTABLE; SELECT ssn, FN_FIX_SSN(ssn) from ssn; drop function fn_ssn_compare cascade; CREATE OR REPLACE FUNCTION fn_ssn_compare(text,text) returns int as $$ BEGIN if FN_FIX_SSN($1) < FN_FIX_SSN($2) then return -1; elseif FN_FIX_SSN($1) > FN_FIX_SSN($2) then return 1; else return 0; end if; end; $$ language plpgsql immutable; CREATE OPERATOR CLASS op_class_ssn_ops1 FOR TYPE text USING btree as OPERATOR 1 <, OPERATOR 2 <=, OPERATOR 3 =, OPERATOR 4 >=, operator 5 >, function 1 fn_ssn_compare(text,text); CREATE INDEX idx_ssn on ssn(ssn op_class_ssn_ops1); explain select * from ssn where ssn.ssn = '222-22-0120'; set enable_seqscan = 'off'; show enable_seqscan;","title":"Custom Indexes"},{"location":"sql/indexing/custom-indexes/#custom-indexes","text":"Example : CREATE a index for social security number , which is the the format of 1111-222-nnnn, you have to index the last 4 characters CREATE TABLE if not exists ssn ( ssn text ); INSERT INTO ssn (ssn) values ('111-11-0100'), ('222-22-0120'), ('333-33-0140'), ('444-44-0160'); select * from ssn; explain select * from ssn; CREATE OR REPLACE FUNCTION FN_FIX_SSN(TEXT) RETURNS text AS $$ BEGIN return substring($1, 8) || replace(substring($1, 1, 7), '-', ''); end ; $$ LANGUAGE plpgsql IMMUTABLE; SELECT ssn, FN_FIX_SSN(ssn) from ssn; drop function fn_ssn_compare cascade; CREATE OR REPLACE FUNCTION fn_ssn_compare(text,text) returns int as $$ BEGIN if FN_FIX_SSN($1) < FN_FIX_SSN($2) then return -1; elseif FN_FIX_SSN($1) > FN_FIX_SSN($2) then return 1; else return 0; end if; end; $$ language plpgsql immutable; CREATE OPERATOR CLASS op_class_ssn_ops1 FOR TYPE text USING btree as OPERATOR 1 <, OPERATOR 2 <=, OPERATOR 3 =, OPERATOR 4 >=, operator 5 >, function 1 fn_ssn_compare(text,text); CREATE INDEX idx_ssn on ssn(ssn op_class_ssn_ops1); explain select * from ssn where ssn.ssn = '222-22-0120'; set enable_seqscan = 'off'; show enable_seqscan;","title":"Custom Indexes"},{"location":"sql/indexing/gin-index/","text":"GIN Index GIN index stands for Generalised Invert Index. Speeds up full text searches A GIN index stores a set of (key, posting list) pairs, where a posting list is a set of row IDs in which the key occurs. The same row ID can appear in multiple posting lists, since an item can contain more than one key. Each key value is stored only once, so a GIN index is very compact for cases where the same key appears many times. Query select * from contacts_docs where body @> '{\"first_name\":\"John\"}'; explain select * from contacts_docs where body @> '{\"first_name\":\"John\"}'; Creating a GIN Index create index idx_gin_contacts_docs_body on contacts_docs USING GIN(body); Get Size of Index select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body'::regclass))) as index_name; Using JSONB_PATH_OPS ( better ) create index idx_gin_contacts_docs_body_cool on contacts_docs USING GIN(body jsonb_path_ops); Size with jsonb_ path_ops select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body_cool'::regclass))) as index_name; On Specific column for smaller size ( not working ) select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body_fname'::regclass))) as index_name;","title":"GIN Index"},{"location":"sql/indexing/gin-index/#gin-index","text":"GIN index stands for Generalised Invert Index. Speeds up full text searches A GIN index stores a set of (key, posting list) pairs, where a posting list is a set of row IDs in which the key occurs. The same row ID can appear in multiple posting lists, since an item can contain more than one key. Each key value is stored only once, so a GIN index is very compact for cases where the same key appears many times.","title":"GIN Index"},{"location":"sql/indexing/gin-index/#query","text":"select * from contacts_docs where body @> '{\"first_name\":\"John\"}'; explain select * from contacts_docs where body @> '{\"first_name\":\"John\"}';","title":"Query"},{"location":"sql/indexing/gin-index/#creating-a-gin-index","text":"create index idx_gin_contacts_docs_body on contacts_docs USING GIN(body);","title":"Creating a GIN Index"},{"location":"sql/indexing/gin-index/#get-size-of-index","text":"select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body'::regclass))) as index_name;","title":"Get Size of Index"},{"location":"sql/indexing/gin-index/#using-jsonb_path_ops-better","text":"create index idx_gin_contacts_docs_body_cool on contacts_docs USING GIN(body jsonb_path_ops);","title":"Using JSONB_PATH_OPS ( better )"},{"location":"sql/indexing/gin-index/#size-with-jsonb_path_ops","text":"select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body_cool'::regclass))) as index_name;","title":"Size with jsonb_path_ops"},{"location":"sql/indexing/gin-index/#on-specific-column-for-smaller-size-not-working","text":"select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body_fname'::regclass))) as index_name;","title":"On Specific column  for smaller size ( not working )"},{"location":"sql/indexing/indexes/","text":"Indexes Data structure Data structure used : B-Tree Self Balancing Index SELECT, INSERT, DELETE and sequential access in logarithmic time Can be used for most of operations and column type supports unique condition Used in Primary Key Used with operators Used when pattern matching Hash Index https://codingsight.com/hash-index-understanding-hash-indexes/ for equality operators not for range Larger than btree in size Brin Index block range index block data -> min to max value smaller index less costly to maintain than btree index Can be used on very large table create table t_big ( id serial, name text ); drop table t_big; insert into t_big (name) select 'adam' from generate_series(1, 2000000); CREATE INDEX CONCURRENTLY brin_index ON public.t_big USING brin (id); create index btree_index on t_big(id); select pg_size_pretty(pg_total_relation_size('t_big')); select pg_size_pretty(pg_indexes_size('t_big')); drop index brin_index; drop index btree_index; explain analyse select * from t_big where id = 9999; explain analyse select id from t_big order by id desc limit 100; Partial Index To performance of the query while reducing the index size. create index if not exists partial_inx on t_big(id) where id > 1000000; explain analyse select * from t_big where id = 10000033; explain analyse select * from t_big where id = 99999; Expression Index PostgreSQL will use this index when WHERE clause or ORDER BY clause in statement Very Expensive to use CREATE TABLE IF NOT EXISTS t_dates AS SELECT d, repeat(md5(d::text),10) as padding FROM generate_series (timestamp '1800-01-01', timestamp '2100-01-01', interval '1 day') s(d); select * from t_dates limit 10; vacuum analyse t_dates; EXPLAIN ANALYSE SELECT * FROM t_dates WHERE d BETWEEN '2001-01-01' AND '2001-01-31'; CREATE INDEX IF NOT EXISTS idx_t_dates_d on t_dates (d); ANALYSE t_dates; EXPLAIN ANALYSE SELECT * FROM t_dates WHERE d BETWEEN '2001-01-01' AND '2001-01-31'; EXPLAIN ANALYSE SELECT * FROM t_dates WHERE EXTRACT(DAY FROM d) = 1; CREATE INDEX idx_expr_t_dates on t_dates (extract(day from d)); ANALYSE t_dates; EXPLAIN ANALYSE SELECT * FROM t_dates WHERE EXTRACT(DAY FROM d) = 1; Heap Index","title":"Indexes"},{"location":"sql/indexing/indexes/#indexes","text":"","title":"Indexes"},{"location":"sql/indexing/indexes/#data-structure","text":"Data structure used : B-Tree Self Balancing Index SELECT, INSERT, DELETE and sequential access in logarithmic time Can be used for most of operations and column type supports unique condition Used in Primary Key Used with operators Used when pattern matching","title":"Data structure"},{"location":"sql/indexing/indexes/#hash-index","text":"https://codingsight.com/hash-index-understanding-hash-indexes/ for equality operators not for range Larger than btree in size","title":"Hash Index"},{"location":"sql/indexing/indexes/#brin-index","text":"block range index block data -> min to max value smaller index less costly to maintain than btree index Can be used on very large table create table t_big ( id serial, name text ); drop table t_big; insert into t_big (name) select 'adam' from generate_series(1, 2000000); CREATE INDEX CONCURRENTLY brin_index ON public.t_big USING brin (id); create index btree_index on t_big(id); select pg_size_pretty(pg_total_relation_size('t_big')); select pg_size_pretty(pg_indexes_size('t_big')); drop index brin_index; drop index btree_index; explain analyse select * from t_big where id = 9999; explain analyse select id from t_big order by id desc limit 100;","title":"Brin Index"},{"location":"sql/indexing/indexes/#partial-index","text":"To performance of the query while reducing the index size. create index if not exists partial_inx on t_big(id) where id > 1000000; explain analyse select * from t_big where id = 10000033; explain analyse select * from t_big where id = 99999;","title":"Partial Index"},{"location":"sql/indexing/indexes/#expression-index","text":"PostgreSQL will use this index when WHERE clause or ORDER BY clause in statement Very Expensive to use CREATE TABLE IF NOT EXISTS t_dates AS SELECT d, repeat(md5(d::text),10) as padding FROM generate_series (timestamp '1800-01-01', timestamp '2100-01-01', interval '1 day') s(d); select * from t_dates limit 10; vacuum analyse t_dates; EXPLAIN ANALYSE SELECT * FROM t_dates WHERE d BETWEEN '2001-01-01' AND '2001-01-31'; CREATE INDEX IF NOT EXISTS idx_t_dates_d on t_dates (d); ANALYSE t_dates; EXPLAIN ANALYSE SELECT * FROM t_dates WHERE d BETWEEN '2001-01-01' AND '2001-01-31'; EXPLAIN ANALYSE SELECT * FROM t_dates WHERE EXTRACT(DAY FROM d) = 1; CREATE INDEX idx_expr_t_dates on t_dates (extract(day from d)); ANALYSE t_dates; EXPLAIN ANALYSE SELECT * FROM t_dates WHERE EXTRACT(DAY FROM d) = 1;","title":"Expression Index"},{"location":"sql/indexing/indexes/#heap-index","text":"","title":"Heap Index"},{"location":"sql/indexing/sql/","text":"SQL Lifetime of Query Parser : handles the textual form of the statement and verifies whether it is correct or not Re-writer : applying the syntactic rules to rewrite the original SQL statement. Optimiser : finding the fastest path to the data Executor : responsible for effectively going to the storage and retrieving the data from the physical storage. Optimiser Finds all the paths and gets the path with cheapest COST LOWEST COST WINS !! Scan Nodes Nodes are available for : every operation every access methods Nodes are stack-able Parent Node ( cost = 0.00 ... ) Child Node Child Node Types of Nodes Sequential Scan Index Scan, Index Only Scan, Bitmap Index Scan Nested Loop, Hash Join and Merge Join Gather and Merge parallel nodes Get All Node Types : SELECT * FROM pg_am; Sequential Scan Performs a sequential scan on the whole table. Index Scan Index is used to access Data Types Index scan Index only scan Bitmap Index Index only scan Hash Join Used when joining tables Joins preformed on 2 table at a time, if more tables are joined together, the output at one join in treated as input to a subsequent join When joining large number of tables, the genetic query optimiser settings may effect what combinations of joins are considered. Types Inner Table : Build a hash table from the inner table, keyed by the join key. Outer Table : then scan the outer table checking if a corresponding value is present Memory Size ( used by sort operation and hash table ) : 4 MB ****","title":"SQL"},{"location":"sql/indexing/sql/#sql","text":"","title":"SQL"},{"location":"sql/indexing/sql/#lifetime-of-query","text":"Parser : handles the textual form of the statement and verifies whether it is correct or not Re-writer : applying the syntactic rules to rewrite the original SQL statement. Optimiser : finding the fastest path to the data Executor : responsible for effectively going to the storage and retrieving the data from the physical storage.","title":"Lifetime of Query"},{"location":"sql/indexing/sql/#optimiser","text":"Finds all the paths and gets the path with cheapest COST LOWEST COST WINS !!","title":"Optimiser"},{"location":"sql/indexing/sql/#scan-nodes","text":"Nodes are available for : every operation every access methods Nodes are stack-able Parent Node ( cost = 0.00 ... ) Child Node Child Node Types of Nodes Sequential Scan Index Scan, Index Only Scan, Bitmap Index Scan Nested Loop, Hash Join and Merge Join Gather and Merge parallel nodes Get All Node Types : SELECT * FROM pg_am;","title":"Scan Nodes"},{"location":"sql/indexing/sql/#sequential-scan","text":"Performs a sequential scan on the whole table.","title":"Sequential Scan"},{"location":"sql/indexing/sql/#index-scan","text":"Index is used to access Data Types Index scan Index only scan Bitmap Index","title":"Index Scan"},{"location":"sql/indexing/sql/#index-only-scan","text":"","title":"Index only scan"},{"location":"sql/indexing/sql/#hash-join","text":"Used when joining tables Joins preformed on 2 table at a time, if more tables are joined together, the output at one join in treated as input to a subsequent join When joining large number of tables, the genetic query optimiser settings may effect what combinations of joins are considered. Types Inner Table : Build a hash table from the inner table, keyed by the join key. Outer Table : then scan the outer table checking if a corresponding value is present Memory Size ( used by sort operation and hash table ) : 4 MB ****","title":"Hash Join"},{"location":"sql/indexing/unique-index/","text":"Unique Index CREATE UNIQUE INDEX idx_products_product_id ON public.products USING btree (product_id ASC NULLS LAST) TABLESPACE pg_default; CREATE UNIQUE INDEX idx_u_emp_emp_id ON public.employees USING btree (employee_id ASC NULLS LAST) TABLESPACE pg_default; CREATE UNIQUE INDEX idx_orders_customer_id_order_id ON public.orders USING btree (order_id ASC NULLS LAST, customer_id ASC NULLS LAST) TABLESPACE pg_default; CREATE UNIQUE INDEX idx_u_emp_emp_id_hire_date ON public.employees USING btree (employee_id ASC NULLS LAST, hire_date ASC NULLS LAST)","title":"Unique Index"},{"location":"sql/indexing/unique-index/#unique-index","text":"CREATE UNIQUE INDEX idx_products_product_id ON public.products USING btree (product_id ASC NULLS LAST) TABLESPACE pg_default; CREATE UNIQUE INDEX idx_u_emp_emp_id ON public.employees USING btree (employee_id ASC NULLS LAST) TABLESPACE pg_default; CREATE UNIQUE INDEX idx_orders_customer_id_order_id ON public.orders USING btree (order_id ASC NULLS LAST, customer_id ASC NULLS LAST) TABLESPACE pg_default; CREATE UNIQUE INDEX idx_u_emp_emp_id_hire_date ON public.employees USING btree (employee_id ASC NULLS LAST, hire_date ASC NULLS LAST)","title":"Unique Index"},{"location":"sql/joins/cross-join/","text":"Cross & Natural Joins Cross Join -- every row in the left table will match with every row -- in the right table in cross join select * from left_product cross join right_product; product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 1 | a | 2 | B 1 | a | 3 | C 1 | a | 4 | d 1 | a | 7 | E1 2 | B | 1 | a 2 | B | 2 | B 2 | B | 3 | C 2 | B | 4 | d 2 | B | 7 | E1 3 | C | 1 | a 3 | C | 2 | B 3 | C | 3 | C 3 | C | 4 | d 3 | C | 7 | E1 5 | E | 1 | a 5 | E | 2 | B 5 | E | 3 | C 5 | E | 4 | d 5 | E | 7 | E1 -- short hand notation for cross join select * from left_product , right_product; -- same output as above -- using inner join to preform query like cross join -- using ON TRUE at the end select * from left_product inner join right_product on true; -- same output as above Natural Join Loading sample data DROP TABLE IF EXISTS c1; CREATE TABLE c1 ( category_id serial PRIMARY KEY, category_name VARCHAR(255) NOT NULL ); DROP TABLE IF EXISTS p1; CREATE TABLE p1 ( product_id serial PRIMARY KEY, product_name VARCHAR(255) NOT NULL, category_id INT NOT NULL, FOREIGN KEY (category_id) REFERENCES c1 (category_id) ); INSERT INTO c1 (category_name) VALUES ('Smart Phone'), ('Laptop'), ('Tablet'); INSERT INTO p1 (product_name, category_id) VALUES ('iPhone', 1), ('Samsung Galaxy', 1), ('HP Elite', 2), ('Lenovo Think pad', 2), ('iPad', 3), ('Kindle Fire', 3); Queries SELECT * FROM p1 NATURAL JOIN c1; category_id | product_id | product_name | category_name -------------+------------+------------------+--------------- 1 | 1 | iPhone | Smart Phone 1 | 2 | Samsung Galaxy | Smart Phone 2 | 3 | HP Elite | Laptop 2 | 4 | Lenovo Think pad | Laptop 3 | 5 | iPad | Tablet 3 | 6 | Kindle Fire | Tablet --same query using inner join SELECT * FROM p1 INNER JOIN c1 USING (category_id); -- same output as above","title":"Cross & Natural Joins"},{"location":"sql/joins/cross-join/#cross-natural-joins","text":"","title":"Cross &amp; Natural Joins"},{"location":"sql/joins/cross-join/#cross-join","text":"-- every row in the left table will match with every row -- in the right table in cross join select * from left_product cross join right_product; product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 1 | a | 2 | B 1 | a | 3 | C 1 | a | 4 | d 1 | a | 7 | E1 2 | B | 1 | a 2 | B | 2 | B 2 | B | 3 | C 2 | B | 4 | d 2 | B | 7 | E1 3 | C | 1 | a 3 | C | 2 | B 3 | C | 3 | C 3 | C | 4 | d 3 | C | 7 | E1 5 | E | 1 | a 5 | E | 2 | B 5 | E | 3 | C 5 | E | 4 | d 5 | E | 7 | E1 -- short hand notation for cross join select * from left_product , right_product; -- same output as above -- using inner join to preform query like cross join -- using ON TRUE at the end select * from left_product inner join right_product on true; -- same output as above","title":"Cross Join"},{"location":"sql/joins/cross-join/#natural-join","text":"","title":"Natural Join"},{"location":"sql/joins/cross-join/#loading-sample-data","text":"DROP TABLE IF EXISTS c1; CREATE TABLE c1 ( category_id serial PRIMARY KEY, category_name VARCHAR(255) NOT NULL ); DROP TABLE IF EXISTS p1; CREATE TABLE p1 ( product_id serial PRIMARY KEY, product_name VARCHAR(255) NOT NULL, category_id INT NOT NULL, FOREIGN KEY (category_id) REFERENCES c1 (category_id) ); INSERT INTO c1 (category_name) VALUES ('Smart Phone'), ('Laptop'), ('Tablet'); INSERT INTO p1 (product_name, category_id) VALUES ('iPhone', 1), ('Samsung Galaxy', 1), ('HP Elite', 2), ('Lenovo Think pad', 2), ('iPad', 3), ('Kindle Fire', 3);","title":"Loading sample data"},{"location":"sql/joins/cross-join/#queries","text":"SELECT * FROM p1 NATURAL JOIN c1; category_id | product_id | product_name | category_name -------------+------------+------------------+--------------- 1 | 1 | iPhone | Smart Phone 1 | 2 | Samsung Galaxy | Smart Phone 2 | 3 | HP Elite | Laptop 2 | 4 | Lenovo Think pad | Laptop 3 | 5 | iPad | Tablet 3 | 6 | Kindle Fire | Tablet --same query using inner join SELECT * FROM p1 INNER JOIN c1 USING (category_id); -- same output as above","title":"Queries"},{"location":"sql/joins/full-multiple-and-self-joins/","text":"Full, Multiple & Self Joins Full Join select * from right_product full join left_product on right_product.product_id = left_poduct.product_id; -- gets result from tables o both side -- those entries that match the each other are in one row -- else they are present in seperate row in table product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 2 | B | 2 | B 3 | C | 3 | C 4 | d | | 7 | E1 | | | | 5 | E select dir.first_name || ' ' || dir.last_name as \"name\", mv.movie_name from directors dir full join movies mv on mv.director_id = dir.director_id name | movie_name -----------------+------------------------ Tomas Alfredson | Let the Right One In Paul Anderson | There Will Be Blood Wes Anderson | The Darjeeling Limited Wes Anderson | Rushmore Wes Anderson | Grand Budapest Hotel Multiple Join select r.movie_id, movie_name, dir.first_name || ' ' || dir.last_name as \"dir name\" from movies mv JOIN movies_revenues r on r.movie_id = mv.movie_id JOIN directors dir on dir.director_id = mv.director_id limit 5; movie_id | movie_name | dir name ----------+--------------------+------------------ 45 | The Wizard of Oz | Victor Fleming 13 | Gone with the Wind | Victor Fleming 23 | Mary Poppins | Robert Stevenson 44 | The Sound of Music | Robert Wise -- same result even after re-arranging select r.movie_id, movie_name, dir.first_name || ' ' || dir.last_name as \"dir name\" from movies mv JOIN directors dir on dir.director_id = mv.director_id JOIN movies_revenues r on r.movie_id = mv.movie_id limit 5; Self Join select * from left_product t1 INNER JOIN left_product t2 ON t1.product_id = t2.product_id; product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 2 | B | 2 | B 3 | C | 3 | C 5 | E | 5 | E","title":"Full, Multiple & Self Joins"},{"location":"sql/joins/full-multiple-and-self-joins/#full-multiple-self-joins","text":"","title":"Full, Multiple &amp; Self Joins"},{"location":"sql/joins/full-multiple-and-self-joins/#full-join","text":"select * from right_product full join left_product on right_product.product_id = left_poduct.product_id; -- gets result from tables o both side -- those entries that match the each other are in one row -- else they are present in seperate row in table product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 2 | B | 2 | B 3 | C | 3 | C 4 | d | | 7 | E1 | | | | 5 | E select dir.first_name || ' ' || dir.last_name as \"name\", mv.movie_name from directors dir full join movies mv on mv.director_id = dir.director_id name | movie_name -----------------+------------------------ Tomas Alfredson | Let the Right One In Paul Anderson | There Will Be Blood Wes Anderson | The Darjeeling Limited Wes Anderson | Rushmore Wes Anderson | Grand Budapest Hotel","title":"Full Join"},{"location":"sql/joins/full-multiple-and-self-joins/#multiple-join","text":"select r.movie_id, movie_name, dir.first_name || ' ' || dir.last_name as \"dir name\" from movies mv JOIN movies_revenues r on r.movie_id = mv.movie_id JOIN directors dir on dir.director_id = mv.director_id limit 5; movie_id | movie_name | dir name ----------+--------------------+------------------ 45 | The Wizard of Oz | Victor Fleming 13 | Gone with the Wind | Victor Fleming 23 | Mary Poppins | Robert Stevenson 44 | The Sound of Music | Robert Wise -- same result even after re-arranging select r.movie_id, movie_name, dir.first_name || ' ' || dir.last_name as \"dir name\" from movies mv JOIN directors dir on dir.director_id = mv.director_id JOIN movies_revenues r on r.movie_id = mv.movie_id limit 5;","title":"Multiple Join"},{"location":"sql/joins/full-multiple-and-self-joins/#self-join","text":"select * from left_product t1 INNER JOIN left_product t2 ON t1.product_id = t2.product_id; product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 2 | B | 2 | B 3 | C | 3 | C 5 | E | 5 | E","title":"Self Join"},{"location":"sql/joins/inner-join/","text":"Inner Join Basic Remember : All common column defined at ON must match values on both tables -- syntax SELECT table_a.column1 table_b.column2 FROM table_a INNER JOIN table_b ON table1.column1 = table2.column2 SELECT mv.*, dir.* FROM movies as mv INNER JOIN directors as dir ON mv.director_id = dir.director_id LIMIT 5; -- director_id column will be repeated movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id | director_id | first_name | last_name | date_of_birth | nationality ----------+------------------------+--------------+------------+--------------+-----------------+-------------+-------------+------------+-----------+---------------+------------- 20 | Let the Right One In | 128 | Swedish | 2008-10-24 | 15 | 1 | 1 | Tomas | Alfredson | 1965-04-01 | Swedish 46 | There Will Be Blood | 168 | English | 2007-12-26 | 15 | 2 | 2 | Paul | Anderson | 1970-06-26 | American 40 | The Darjeeling Limited | 119 | English | 2007-09-29 | PG | 3 | 3 | Wes | Anderson | 1969-05-01 | American 30 | Rushmore | 104 | English | 1998-11-12 | 12 | 3 | 3 | Wes | Anderson | 1969-05-01 | American 15 | Grand Budapest Hotel | 117 | English | 2014-07-03 | PG | 3 | 3 | Wes | Anderson | 1969-05-01 | American Selecting only required columns SELECT movie_name, dir.first_name || ' ' || dir.last_name as \"Director Name\", mv.movie_id, dir.director_id FROM movies as mv INNER JOIN directors as dir ON mv.director_id = dir.director_id LIMIT 5; movie_name | Director Name | movie_id | director_id ------------------------+-----------------+----------+------------- Let the Right One In | Tomas Alfredson | 20 | 1 There Will Be Blood | Paul Anderson | 46 | 2 The Darjeeling Limited | Wes Anderson | 40 | 3 Rushmore | Wes Anderson | 30 | 3 Grand Budapest Hotel | Wes Anderson | 15 | 3 with WHERE clause SELECT movie_name, mv.movie_lang, dir.first_name || ' ' || dir.last_name as \"Director Name\", mv.movie_id, dir.director_id FROM movies as mv INNER JOIN directors as dir ON mv.director_id = dir.director_id WHERE mv.movie_lang = 'English' limit 5; movie_name | movie_lang | Director Name | movie_id | director_id ------------------------+------------+----------------+----------+------------- There Will Be Blood | English | Paul Anderson | 46 | 2 The Darjeeling Limited | English | Wes Anderson | 40 | 3 Rushmore | English | Wes Anderson | 30 | 3 Grand Budapest Hotel | English | Wes Anderson | 15 | 3 Submarine | English | Richard Ayoade | 38 | 4 Inner Join with USING we use USING only when joining tables have the SAME column name, rather then ON ! select table1.column1, table2.column1 from table1 INNER JOIN table2 USING (column1); select * from movies INNER JOIN directors USING (director_id) LIMIT 10; director_id | movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | first_name | last_name | date_of_birth | nationality -------------+----------+--------------------+--------------+------------+--------------+-----------------+------------+--------------+---------------+------------- 13 | 1 | A Clockwork Orange | 112 | English | 1972-02-02 | 18 | Stanley | Kubrick | 1928-07-26 | American 9 | 2 | Apocalypse Now | 168 | English | 1979-08-15 | 15 | Francis | Ford Coppola | 1939-04-07 | American","title":"Inner Join"},{"location":"sql/joins/inner-join/#inner-join","text":"","title":"Inner Join"},{"location":"sql/joins/inner-join/#basic","text":"Remember : All common column defined at ON must match values on both tables -- syntax SELECT table_a.column1 table_b.column2 FROM table_a INNER JOIN table_b ON table1.column1 = table2.column2 SELECT mv.*, dir.* FROM movies as mv INNER JOIN directors as dir ON mv.director_id = dir.director_id LIMIT 5; -- director_id column will be repeated movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id | director_id | first_name | last_name | date_of_birth | nationality ----------+------------------------+--------------+------------+--------------+-----------------+-------------+-------------+------------+-----------+---------------+------------- 20 | Let the Right One In | 128 | Swedish | 2008-10-24 | 15 | 1 | 1 | Tomas | Alfredson | 1965-04-01 | Swedish 46 | There Will Be Blood | 168 | English | 2007-12-26 | 15 | 2 | 2 | Paul | Anderson | 1970-06-26 | American 40 | The Darjeeling Limited | 119 | English | 2007-09-29 | PG | 3 | 3 | Wes | Anderson | 1969-05-01 | American 30 | Rushmore | 104 | English | 1998-11-12 | 12 | 3 | 3 | Wes | Anderson | 1969-05-01 | American 15 | Grand Budapest Hotel | 117 | English | 2014-07-03 | PG | 3 | 3 | Wes | Anderson | 1969-05-01 | American","title":"Basic"},{"location":"sql/joins/inner-join/#selecting-only-required-columns","text":"SELECT movie_name, dir.first_name || ' ' || dir.last_name as \"Director Name\", mv.movie_id, dir.director_id FROM movies as mv INNER JOIN directors as dir ON mv.director_id = dir.director_id LIMIT 5; movie_name | Director Name | movie_id | director_id ------------------------+-----------------+----------+------------- Let the Right One In | Tomas Alfredson | 20 | 1 There Will Be Blood | Paul Anderson | 46 | 2 The Darjeeling Limited | Wes Anderson | 40 | 3 Rushmore | Wes Anderson | 30 | 3 Grand Budapest Hotel | Wes Anderson | 15 | 3","title":"Selecting only required columns"},{"location":"sql/joins/inner-join/#with-where-clause","text":"SELECT movie_name, mv.movie_lang, dir.first_name || ' ' || dir.last_name as \"Director Name\", mv.movie_id, dir.director_id FROM movies as mv INNER JOIN directors as dir ON mv.director_id = dir.director_id WHERE mv.movie_lang = 'English' limit 5; movie_name | movie_lang | Director Name | movie_id | director_id ------------------------+------------+----------------+----------+------------- There Will Be Blood | English | Paul Anderson | 46 | 2 The Darjeeling Limited | English | Wes Anderson | 40 | 3 Rushmore | English | Wes Anderson | 30 | 3 Grand Budapest Hotel | English | Wes Anderson | 15 | 3 Submarine | English | Richard Ayoade | 38 | 4","title":"with WHERE clause"},{"location":"sql/joins/inner-join/#inner-join-with-using","text":"we use USING only when joining tables have the SAME column name, rather then ON ! select table1.column1, table2.column1 from table1 INNER JOIN table2 USING (column1); select * from movies INNER JOIN directors USING (director_id) LIMIT 10; director_id | movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | first_name | last_name | date_of_birth | nationality -------------+----------+--------------------+--------------+------------+--------------+-----------------+------------+--------------+---------------+------------- 13 | 1 | A Clockwork Orange | 112 | English | 1972-02-02 | 18 | Stanley | Kubrick | 1928-07-26 | American 9 | 2 | Apocalypse Now | 168 | English | 1979-08-15 | 15 | Francis | Ford Coppola | 1939-04-07 | American","title":"Inner Join with USING"},{"location":"sql/joins/left-and-right-join/","text":"Left and Right JOIN Left Join Basic Returns every row from LEFT table plus rows that match values in the joined column from the RIGHT table Syntax SELECT table1.column1, table2.column1 FROM table1 LEFT JOIN table2 ON table1.column1 = table2.column2 Loading Sample Data create table left_product ( product_id INT PRIMARY KEY, product_name VARCHAR(100) ); CREATE TABLE right_product ( product_id INT PRIMARY KEY, product_name VARCHAR(100) ); INSERT INTO left_product ( PRODUCT_ID, PRODUCT_NAME ) VALUES (1,'a'),('2','B'),('3','C'),('5','E'); INSERT INTO right_product ( PRODUCT_ID, PRODUCT_NAME ) VALUES (1,'a'),('2','B'),('3','C'),('4','d'),(7,'E1'); -- left join selects all data in the left data and -- picks all data in the right table matching the condition SELECT * from left_product left join right_product on left_product.product_id = right_product.product_id; product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 2 | B | 2 | B 3 | C | 3 | C 5 | E |-- no match | -- no match -- select name from director table -- and select movie from another table -- where director.id matches director id in movies table select dir.first_name, dir.last_name, mv.movie_name from directors dir left join movies mv ON mv.director_id = dir.director_id limit 5; first_name | last_name | movie_name ------------+-----------+------------------------ Tomas | Alfredson | Let the Right One In Paul | Anderson | There Will Be Blood Wes | Anderson | The Darjeeling Limited Wes | Anderson | Rushmore Wes | Anderson | Grand Budapest Hotel -- same above query with where clause select dir.first_name || ' ' || dir.last_name as \"Directors Name\", mv.movie_name, mv.movie_lang from directors dir left join movies mv ON mv.director_id = dir.director_id where mv.movie_lang in ('English', 'Chinese'); Right Join Syntax SELECT table1.column1, table2.column2 FROM table1 RIGHT JOIN table2 ON table1.column1 = table2.column2; Load Sample Data CREATE TABLE films ( film_id SERIAL PRIMARY KEY, title varchar(255) NOT NULL ); INSERT INTO films(title) VALUES ('Joker'), ('Avengers: Endgame'), ('Parasite'); CREATE TABLE film_reviews ( review_id SERIAL PRIMARY KEY, film_id INT, review VARCHAR(255) NOT NULL ); INSERT INTO film_reviews(film_id, review) VALUES (1, 'Excellent'), (1, 'Awesome'), (2, 'Cool'), (NULL, 'Beautiful'); -- contains products from left table -- that are in right table -- and all rows from right table that do not -- match anything from left SELECT * from left_product right join right_product on left_product.product_id = right_product.product_id; product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 2 | B | 2 | B 3 | C | 3 | C | | 4 | d | | 7 | E1 SELECT review, title FROM films RIGHT JOIN film_reviews using (film_id) WHERE title IS NULL; review | title -----------+------- Beautiful |","title":"Left and Right JOIN"},{"location":"sql/joins/left-and-right-join/#left-and-right-join","text":"","title":"Left and Right JOIN"},{"location":"sql/joins/left-and-right-join/#left-join","text":"","title":"Left Join"},{"location":"sql/joins/left-and-right-join/#basic","text":"Returns every row from LEFT table plus rows that match values in the joined column from the RIGHT table","title":"Basic"},{"location":"sql/joins/left-and-right-join/#syntax","text":"SELECT table1.column1, table2.column1 FROM table1 LEFT JOIN table2 ON table1.column1 = table2.column2","title":"Syntax"},{"location":"sql/joins/left-and-right-join/#loading-sample-data","text":"create table left_product ( product_id INT PRIMARY KEY, product_name VARCHAR(100) ); CREATE TABLE right_product ( product_id INT PRIMARY KEY, product_name VARCHAR(100) ); INSERT INTO left_product ( PRODUCT_ID, PRODUCT_NAME ) VALUES (1,'a'),('2','B'),('3','C'),('5','E'); INSERT INTO right_product ( PRODUCT_ID, PRODUCT_NAME ) VALUES (1,'a'),('2','B'),('3','C'),('4','d'),(7,'E1'); -- left join selects all data in the left data and -- picks all data in the right table matching the condition SELECT * from left_product left join right_product on left_product.product_id = right_product.product_id; product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 2 | B | 2 | B 3 | C | 3 | C 5 | E |-- no match | -- no match -- select name from director table -- and select movie from another table -- where director.id matches director id in movies table select dir.first_name, dir.last_name, mv.movie_name from directors dir left join movies mv ON mv.director_id = dir.director_id limit 5; first_name | last_name | movie_name ------------+-----------+------------------------ Tomas | Alfredson | Let the Right One In Paul | Anderson | There Will Be Blood Wes | Anderson | The Darjeeling Limited Wes | Anderson | Rushmore Wes | Anderson | Grand Budapest Hotel -- same above query with where clause select dir.first_name || ' ' || dir.last_name as \"Directors Name\", mv.movie_name, mv.movie_lang from directors dir left join movies mv ON mv.director_id = dir.director_id where mv.movie_lang in ('English', 'Chinese');","title":"Loading Sample Data"},{"location":"sql/joins/left-and-right-join/#right-join","text":"","title":"Right Join"},{"location":"sql/joins/left-and-right-join/#syntax_1","text":"SELECT table1.column1, table2.column2 FROM table1 RIGHT JOIN table2 ON table1.column1 = table2.column2;","title":"Syntax"},{"location":"sql/joins/left-and-right-join/#load-sample-data","text":"CREATE TABLE films ( film_id SERIAL PRIMARY KEY, title varchar(255) NOT NULL ); INSERT INTO films(title) VALUES ('Joker'), ('Avengers: Endgame'), ('Parasite'); CREATE TABLE film_reviews ( review_id SERIAL PRIMARY KEY, film_id INT, review VARCHAR(255) NOT NULL ); INSERT INTO film_reviews(film_id, review) VALUES (1, 'Excellent'), (1, 'Awesome'), (2, 'Cool'), (NULL, 'Beautiful'); -- contains products from left table -- that are in right table -- and all rows from right table that do not -- match anything from left SELECT * from left_product right join right_product on left_product.product_id = right_product.product_id; product_id | product_name | product_id | product_name ------------+--------------+------------+-------------- 1 | a | 1 | a 2 | B | 2 | B 3 | C | 3 | C | | 4 | d | | 7 | E1 SELECT review, title FROM films RIGHT JOIN film_reviews using (film_id) WHERE title IS NULL; review | title -----------+------- Beautiful |","title":"Load Sample Data"},{"location":"sql/summarization/","text":"Summarization When we are aggregating data, we generally remove the detail data that lies below the summarize table. The whole point of aggregation is to replace detail data with summaries. This is where subtotals comes in! A grouping set is a set of columns by which you group. The PostgreSQL ROLLUP is a subclass of the GROUP BY clause that offers shorthand for defining ' multiple grouping sets '","title":"Summarization"},{"location":"sql/summarization/#summarization","text":"When we are aggregating data, we generally remove the detail data that lies below the summarize table. The whole point of aggregation is to replace detail data with summaries. This is where subtotals comes in! A grouping set is a set of columns by which you group. The PostgreSQL ROLLUP is a subclass of the GROUP BY clause that offers shorthand for defining ' multiple grouping sets '","title":"Summarization"},{"location":"sql/summarization/subqueries/","text":"SubQueries Allows you to construct a complex query A sub-query is nested inside another query can be nested inside SELECT , INSERT , UPDATE , DELETE SubQueries with SELECT clause SELECT movie_name, movie_length FROM movies mv WHERE movie_length >= ( SELECT avg(movie_length) FROM movies ) SELECT movie_name, movie_length FROM movies mv WHERE movie_length >= ( SELECT avg(movie_length) FROM movies WHERE movie_lang = 'English' ); SubQueries With WHERE Clause SELECT first_name, last_name, date_of_birth FROM actors WHERE date_of_birth > ( SELECT date_of_birth FROM actors WHERE first_name = 'Douglas' -- 1922-06-10 ); -- using IN operator SELECT movie_name, movie_length FROM movies WHERE movie_id in ( SELECT movie_id FROM movies_revenues WHERE revenues_domestic > 200 ); SELECT movie_id, movie_name FROM movies WHERE movie_id IN ( SELECT movie_id FROM movies_revenues WHERE revenues_domestic > movies_revenues.revenues_international ); SubQueries with JOINS -- with joins SELECT d.director_id, d.first_name || ' ' || d.last_name as \"Director Name\", SUM(r.revenues_international + r.revenues_domestic) as \"total_revenues\" FROM directors d INNER JOIN movies mv ON mv.director_id = d.director_id INNER JOIN movies_revenues r on r.movie_id = mv.movie_id WHERE (r.revenues_domestic + r.revenues_international) > ( SELECT avg(revenues_domestic + revenues_international) as \"avg_total_revenue\" FROM movies_revenues ) GROUP BY d.director_id ORDER BY total_revenues; SELECT d.director_id, SUM(COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0)) AS \"totaL_reveneues\" FROM directors d INNER JOIN movies mv ON mv.director_id = d.director_id INNER JOIN movies_revenues r ON r.movie_id = mv.movie_id WHERE COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0) > ( SELECT AVG(COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0)) as \"avg_total_reveneues\" FROM movies_revenues r INNER JOIN movies mv ON mv.movie_id = r.movie_id WHERE mv.movie_lang = 'English' ) GROUP BY d.director_id ORDER BY 2 DESC, 1 ASC; SubQueries with Alias -- as alias SELECT * FROM ( SELECT * FROM movies ) t1; -- query without FROM SELECT ( SELECT avg(revenues_domestic) as \"Average Revenue\" FROM movies_revenues ), ( SELECT min(revenues_domestic) as \"MIN Revenue\" FROM movies_revenues ), ( SELECT max(revenues_domestic) as \"MAX Revenue\" FROM movies_revenues )","title":"SubQueries"},{"location":"sql/summarization/subqueries/#subqueries","text":"Allows you to construct a complex query A sub-query is nested inside another query can be nested inside SELECT , INSERT , UPDATE , DELETE","title":"SubQueries"},{"location":"sql/summarization/subqueries/#subqueries-with-select-clause","text":"SELECT movie_name, movie_length FROM movies mv WHERE movie_length >= ( SELECT avg(movie_length) FROM movies ) SELECT movie_name, movie_length FROM movies mv WHERE movie_length >= ( SELECT avg(movie_length) FROM movies WHERE movie_lang = 'English' );","title":"SubQueries with SELECT clause"},{"location":"sql/summarization/subqueries/#subqueries-with-where-clause","text":"SELECT first_name, last_name, date_of_birth FROM actors WHERE date_of_birth > ( SELECT date_of_birth FROM actors WHERE first_name = 'Douglas' -- 1922-06-10 ); -- using IN operator SELECT movie_name, movie_length FROM movies WHERE movie_id in ( SELECT movie_id FROM movies_revenues WHERE revenues_domestic > 200 ); SELECT movie_id, movie_name FROM movies WHERE movie_id IN ( SELECT movie_id FROM movies_revenues WHERE revenues_domestic > movies_revenues.revenues_international );","title":"SubQueries With WHERE Clause"},{"location":"sql/summarization/subqueries/#subqueries-with-joins","text":"-- with joins SELECT d.director_id, d.first_name || ' ' || d.last_name as \"Director Name\", SUM(r.revenues_international + r.revenues_domestic) as \"total_revenues\" FROM directors d INNER JOIN movies mv ON mv.director_id = d.director_id INNER JOIN movies_revenues r on r.movie_id = mv.movie_id WHERE (r.revenues_domestic + r.revenues_international) > ( SELECT avg(revenues_domestic + revenues_international) as \"avg_total_revenue\" FROM movies_revenues ) GROUP BY d.director_id ORDER BY total_revenues; SELECT d.director_id, SUM(COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0)) AS \"totaL_reveneues\" FROM directors d INNER JOIN movies mv ON mv.director_id = d.director_id INNER JOIN movies_revenues r ON r.movie_id = mv.movie_id WHERE COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0) > ( SELECT AVG(COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0)) as \"avg_total_reveneues\" FROM movies_revenues r INNER JOIN movies mv ON mv.movie_id = r.movie_id WHERE mv.movie_lang = 'English' ) GROUP BY d.director_id ORDER BY 2 DESC, 1 ASC;","title":"SubQueries with JOINS"},{"location":"sql/summarization/subqueries/#subqueries-with-alias","text":"-- as alias SELECT * FROM ( SELECT * FROM movies ) t1; -- query without FROM SELECT ( SELECT avg(revenues_domestic) as \"Average Revenue\" FROM movies_revenues ), ( SELECT min(revenues_domestic) as \"MIN Revenue\" FROM movies_revenues ), ( SELECT max(revenues_domestic) as \"MAX Revenue\" FROM movies_revenues )","title":"SubQueries with Alias"},{"location":"sql/summarization/window/","text":"Window Rollup The ROLLUP option allows you to include extra rows that represent the subtotals, which are commonly referred to as super-aggregate rows, along with the grand total row. The ROLLUP assumes a hierarchy among the input columns. For example, if the input column is (c1,c2), the hierarchy c1 > c2 . -- SYNTAX SELECT c1, c2, aggregate_function(c3) FROM table GROUP BY ROLLUP (c1, c2); -- EXAMPLE SELECT region, round(avg(imports), 2) FROM trades GROUP BY rollup (region); -- OUTPUT region | round -----------------+----------------- | 73325290066.17 NORTH AMERICA | 70993412846.58 SOUTH AMERICA | 152968951703.49 ASIA | 115476296703.29 CENTRAL AMERICA | 38853979545.30 SELECT region, country, round(avg(imports), 2) FROM trades WHERE country in ('USA', 'Argentina', 'Singapore', 'Brazil') GROUP BY rollup (region, country) order by 1; region | country | round ---------------+-----------+------------------ ASIA | Singapore | 209191973057.39 ASIA | | 209191973057.39 SOUTH AMERICA | Argentina | 40764435340.44 SOUTH AMERICA | Brazil | 103647680768.84 SOUTH AMERICA | USA | 1590283700017.03 SOUTH AMERICA | | 579677530557.70 | | 482346579011.01 SELECT region, country, round(avg(imports / 1000000)) FROM trades WHERE country in ('USA', 'France', 'Germany') GROUP BY cube (region, country); region | country | round ---------------+---------+--------- | | 974454 EUROPE | France | 481421 SOUTH AMERICA | USA | 1590284 EUROPE | Germany | 800653 SOUTH AMERICA | | 1590284 EUROPE | | 649743 | USA | 1590284 | Germany | 800653 | France | 481421 SELECT region, country, round(avg(imports) / 1000000, 2) FROM trades WHERE country in ('USA', 'FRANCE', 'Germany') GROUP BY grouping sets ( (), country, region ); region | country | round ---------------+---------+------------ | | 1195468.31 | USA | 1590283.70 | Germany | 800652.92 SOUTH AMERICA | | 1590283.70 EUROPE | | 800652.92 SELECT region, country, round(avg(imports) / 1000000, 2) FROM trades WHERE country in ('USA', 'FRANCE', 'Germany') GROUP BY grouping sets ( (), country, region ); region | country | round ---------------+---------+------------ | | 1195468.31 | USA | 1590283.70 | Germany | 800652.92 SOUTH AMERICA | | 1590283.70 EUROPE | | 800652.92 SELECT region, avg(exports) as avg_all, avg(exports) filter ( WHERE trades.year < 1995 ) as avg_old, avg(exports) filter ( WHERE trades.year >= 1995) as avg_latest FROM trades GROUP BY rollup (region); region | avg_all | avg_old | avg_latest -----------------+-----------------------+-----------------------+----------------------- | 72443407670.13915858 | 42905138774.45808383 | 74914795899.38702405 NORTH AMERICA | 74417101760.04615385 | 62932794640.69230769 | 75693135884.41880342 SOUTH AMERICA | 109338636484.50140845 | 49195642528.37777778 | 118069071091.03548387 ASIA | 122562815407.87438424 | 59879433497.46250000 | 129413458239.61338798 CENTRAL AMERICA | 34961597492.66203704 | 12904661532.05555556 | 36966773489.08080808 SELECT AVG(imports), avg(exports) FROM trades; avg | avg ----------------------+---------------------- 73325290066.16504854 | 72443407670.13915858 More Queries SELECT country, year, imports, exports, avg(exports) OVER () as avg_exports FROM trades; SELECT country, year, imports, exports, avg(exports) OVER (partition by country) as avg_exports FROM trades; SELECT country, year, imports, exports, avg(exports) OVER (partition by year < 2000 ) as avg_exports FROM trades; SELECT country, year, exports, min(exports) OVER (PARTITION BY country order by year) FROM trades WHERE year > 2001 and country in ('USA', 'France');","title":"Window"},{"location":"sql/summarization/window/#window","text":"","title":"Window"},{"location":"sql/summarization/window/#rollup","text":"The ROLLUP option allows you to include extra rows that represent the subtotals, which are commonly referred to as super-aggregate rows, along with the grand total row. The ROLLUP assumes a hierarchy among the input columns. For example, if the input column is (c1,c2), the hierarchy c1 > c2 . -- SYNTAX SELECT c1, c2, aggregate_function(c3) FROM table GROUP BY ROLLUP (c1, c2); -- EXAMPLE SELECT region, round(avg(imports), 2) FROM trades GROUP BY rollup (region); -- OUTPUT region | round -----------------+----------------- | 73325290066.17 NORTH AMERICA | 70993412846.58 SOUTH AMERICA | 152968951703.49 ASIA | 115476296703.29 CENTRAL AMERICA | 38853979545.30 SELECT region, country, round(avg(imports), 2) FROM trades WHERE country in ('USA', 'Argentina', 'Singapore', 'Brazil') GROUP BY rollup (region, country) order by 1; region | country | round ---------------+-----------+------------------ ASIA | Singapore | 209191973057.39 ASIA | | 209191973057.39 SOUTH AMERICA | Argentina | 40764435340.44 SOUTH AMERICA | Brazil | 103647680768.84 SOUTH AMERICA | USA | 1590283700017.03 SOUTH AMERICA | | 579677530557.70 | | 482346579011.01 SELECT region, country, round(avg(imports / 1000000)) FROM trades WHERE country in ('USA', 'France', 'Germany') GROUP BY cube (region, country); region | country | round ---------------+---------+--------- | | 974454 EUROPE | France | 481421 SOUTH AMERICA | USA | 1590284 EUROPE | Germany | 800653 SOUTH AMERICA | | 1590284 EUROPE | | 649743 | USA | 1590284 | Germany | 800653 | France | 481421 SELECT region, country, round(avg(imports) / 1000000, 2) FROM trades WHERE country in ('USA', 'FRANCE', 'Germany') GROUP BY grouping sets ( (), country, region ); region | country | round ---------------+---------+------------ | | 1195468.31 | USA | 1590283.70 | Germany | 800652.92 SOUTH AMERICA | | 1590283.70 EUROPE | | 800652.92 SELECT region, country, round(avg(imports) / 1000000, 2) FROM trades WHERE country in ('USA', 'FRANCE', 'Germany') GROUP BY grouping sets ( (), country, region ); region | country | round ---------------+---------+------------ | | 1195468.31 | USA | 1590283.70 | Germany | 800652.92 SOUTH AMERICA | | 1590283.70 EUROPE | | 800652.92 SELECT region, avg(exports) as avg_all, avg(exports) filter ( WHERE trades.year < 1995 ) as avg_old, avg(exports) filter ( WHERE trades.year >= 1995) as avg_latest FROM trades GROUP BY rollup (region); region | avg_all | avg_old | avg_latest -----------------+-----------------------+-----------------------+----------------------- | 72443407670.13915858 | 42905138774.45808383 | 74914795899.38702405 NORTH AMERICA | 74417101760.04615385 | 62932794640.69230769 | 75693135884.41880342 SOUTH AMERICA | 109338636484.50140845 | 49195642528.37777778 | 118069071091.03548387 ASIA | 122562815407.87438424 | 59879433497.46250000 | 129413458239.61338798 CENTRAL AMERICA | 34961597492.66203704 | 12904661532.05555556 | 36966773489.08080808 SELECT AVG(imports), avg(exports) FROM trades; avg | avg ----------------------+---------------------- 73325290066.16504854 | 72443407670.13915858","title":"Rollup"},{"location":"sql/summarization/window/#more-queries","text":"SELECT country, year, imports, exports, avg(exports) OVER () as avg_exports FROM trades; SELECT country, year, imports, exports, avg(exports) OVER (partition by country) as avg_exports FROM trades; SELECT country, year, imports, exports, avg(exports) OVER (partition by year < 2000 ) as avg_exports FROM trades; SELECT country, year, exports, min(exports) OVER (PARTITION BY country order by year) FROM trades WHERE year > 2001 and country in ('USA', 'France');","title":"More Queries"},{"location":"sql/tables/","text":"Table Creating Table Altering Table -- adding column to table ALTER TABLE [schema].[table name] ADD COLUMN [column name] [data type]; -- example ALTER TABLE public.rainfalls ADD COLUMN is_enable boolean [CONSTRAINT]; -- renaming column in table ALTER TABLE public.rainfalls RENAME is_enable TO accurate; -- add mulitple columns ALTER TABLE rainfals ADD COLUMN city VARCHAR(20), ADD COLUMN pincode VARCHAR(50); -- create dummy table CREATE TABLE IF NOT EXISTS wrong_table ( name text ); -- rename table using ALTER ALTER TABLE wrong_table RENAME TO name_table; DROP TABLE IF EXISTS name_table; -- drop column ALTER TABLE public.rainfalls DROP COLUMN pincode; -- change data type of column ALTER TABLE public.rainfalls ALTER COLUMN accurate TYPE TEXT; Table \"public.rainfalls\" Column | Type | ----------+-----------------------+- location | text | year | integer | month | integer | raindays | integer | accurate | TEXT | city | character varying(20) | -- altering column datatype ALTER TABLE rainfalls ALTER COLUMN accurate TYPE REAL USING accurate::REAL; Table \"public.rainfalls\" Column | Type | ----------+-----------------------+- location | text | year | integer | month | integer | raindays | integer | accurate | real | city | character varying(20) | -- set default value of column ALTER TABLE users ALTER COLUMN is_enable SET DEFAULT 'Y'; Delete -- delete table DROP TABLE IF EXISTS [table name]; -- delete row DROP TABLE IF EXISTS [table name]; -- delete column ALTER TABLE [table name] DROP column [column name]; -- delete constraints ALTER TABLE [table name] DROP CONSTRAINT [constrain name]; Select The SELECT statement has the following clauses: Select distinct rows using DISTINCT operator. Sort rows using ORDER BY clause. Filter rows using WHERE clause. Select a subset of rows from a table using LIMIT or FETCH clause. Group rows into groups using GROUP BY clause. Filter groups using HAVING clause. Join with other tables using joins such as INNER JOIN , LEFT JOIN , FULL OUTER JOIN , CROSS JOIN clauses. Perform set operations using UNION , INTERSECT , and EXCEPT . SELECT first_name, last_name FROM directors LIMIT 5; Representation Function ASC Ascending DESC Descending SELECT * FROM directors LIMIT 3; director_id | first_name | last_name | date_of_birth | nationality -------------+------------+-----------+---------------+------------- 1 | Tomas | Alfredson | 1965-04-01 | Swedish 2 | Paul | Anderson | 1970-06-26 | American 3 | Wes | Anderson | 1969-05-01 | American SELECT * FROM directors ORBER BY date_of_birth ASC|DESC; SELECT DISTINCT nationality FROM directors; -- to select one column with condtion SELECT * FROM directors WHERE nationality = 'Chinese'; -- Two or more conditions SELECT * FROM directors WHERE nationality = 'Mexican' AND date_of_birth='1964-10-09'; Column Alias -- normal column aliases SELECT first_name || ' ' || last_name as full_name FROM directors LIMIT 5; -- with spaces SELECT first_name || ' ' || last_name \"full name\" FROM customer; Insert CREATE TABLE IF NOT EXISTS temp_table ( col1 text, col2 text, ); INSERT INTO table (col1, col2) VALUES ( 'VALUE 1' , 'VALUE 2'); -- MULTIPLE INSERT INTO table (col1, col2) VALUES ( 'VALUE 1' , 'VALUE 2','VALUE 3','VALUE 4'); -- STRING WITH QUOTES , add another ' before ' INSERT INTO table (col1) VALUES ( 'VALUE''S 1' ); -- RETURNING ROWS INSERT INTO temp_table (col1, col2) VALUES ( 'VALUE 5' , 'VALUE 6') RETURNING *; Updating UPDATE directors set last_name = 'Walker' where director_id = 2; -- returning updated row update actors set last_name = 'Anderson' where director_id = 150 returning *; -- update all records update [table name] set [column name] = [value]; UPDATE [table name] SET email = 'not found' WHERE email IS NULL; Upsert -- syntax for upsert INSERT INTO tablename ( col_list ) VALUES ( value_list ) ON CONFLICT (COL_NAME) DO NOTHING -- OR UPDATE SET col = val where condition; -- syntax for upsert INSERT INTO tablename ( COL_NAME ) VALUES ( value_list ) ON CONFLICT (COL_NAME) DO UPDATE SET COL_NAME = EXCLUDED.COL_NAME;","title":"Table"},{"location":"sql/tables/#table","text":"","title":"Table"},{"location":"sql/tables/#creating-table","text":"","title":"Creating Table"},{"location":"sql/tables/#altering-table","text":"-- adding column to table ALTER TABLE [schema].[table name] ADD COLUMN [column name] [data type]; -- example ALTER TABLE public.rainfalls ADD COLUMN is_enable boolean [CONSTRAINT]; -- renaming column in table ALTER TABLE public.rainfalls RENAME is_enable TO accurate; -- add mulitple columns ALTER TABLE rainfals ADD COLUMN city VARCHAR(20), ADD COLUMN pincode VARCHAR(50); -- create dummy table CREATE TABLE IF NOT EXISTS wrong_table ( name text ); -- rename table using ALTER ALTER TABLE wrong_table RENAME TO name_table; DROP TABLE IF EXISTS name_table; -- drop column ALTER TABLE public.rainfalls DROP COLUMN pincode; -- change data type of column ALTER TABLE public.rainfalls ALTER COLUMN accurate TYPE TEXT; Table \"public.rainfalls\" Column | Type | ----------+-----------------------+- location | text | year | integer | month | integer | raindays | integer | accurate | TEXT | city | character varying(20) | -- altering column datatype ALTER TABLE rainfalls ALTER COLUMN accurate TYPE REAL USING accurate::REAL; Table \"public.rainfalls\" Column | Type | ----------+-----------------------+- location | text | year | integer | month | integer | raindays | integer | accurate | real | city | character varying(20) | -- set default value of column ALTER TABLE users ALTER COLUMN is_enable SET DEFAULT 'Y';","title":"Altering Table"},{"location":"sql/tables/#delete","text":"-- delete table DROP TABLE IF EXISTS [table name]; -- delete row DROP TABLE IF EXISTS [table name]; -- delete column ALTER TABLE [table name] DROP column [column name]; -- delete constraints ALTER TABLE [table name] DROP CONSTRAINT [constrain name];","title":"Delete"},{"location":"sql/tables/#select","text":"The SELECT statement has the following clauses: Select distinct rows using DISTINCT operator. Sort rows using ORDER BY clause. Filter rows using WHERE clause. Select a subset of rows from a table using LIMIT or FETCH clause. Group rows into groups using GROUP BY clause. Filter groups using HAVING clause. Join with other tables using joins such as INNER JOIN , LEFT JOIN , FULL OUTER JOIN , CROSS JOIN clauses. Perform set operations using UNION , INTERSECT , and EXCEPT . SELECT first_name, last_name FROM directors LIMIT 5; Representation Function ASC Ascending DESC Descending SELECT * FROM directors LIMIT 3; director_id | first_name | last_name | date_of_birth | nationality -------------+------------+-----------+---------------+------------- 1 | Tomas | Alfredson | 1965-04-01 | Swedish 2 | Paul | Anderson | 1970-06-26 | American 3 | Wes | Anderson | 1969-05-01 | American SELECT * FROM directors ORBER BY date_of_birth ASC|DESC; SELECT DISTINCT nationality FROM directors; -- to select one column with condtion SELECT * FROM directors WHERE nationality = 'Chinese'; -- Two or more conditions SELECT * FROM directors WHERE nationality = 'Mexican' AND date_of_birth='1964-10-09';","title":"Select"},{"location":"sql/tables/#column-alias","text":"-- normal column aliases SELECT first_name || ' ' || last_name as full_name FROM directors LIMIT 5; -- with spaces SELECT first_name || ' ' || last_name \"full name\" FROM customer;","title":"Column Alias"},{"location":"sql/tables/#insert","text":"CREATE TABLE IF NOT EXISTS temp_table ( col1 text, col2 text, ); INSERT INTO table (col1, col2) VALUES ( 'VALUE 1' , 'VALUE 2'); -- MULTIPLE INSERT INTO table (col1, col2) VALUES ( 'VALUE 1' , 'VALUE 2','VALUE 3','VALUE 4'); -- STRING WITH QUOTES , add another ' before ' INSERT INTO table (col1) VALUES ( 'VALUE''S 1' ); -- RETURNING ROWS INSERT INTO temp_table (col1, col2) VALUES ( 'VALUE 5' , 'VALUE 6') RETURNING *;","title":"Insert"},{"location":"sql/tables/#updating","text":"UPDATE directors set last_name = 'Walker' where director_id = 2; -- returning updated row update actors set last_name = 'Anderson' where director_id = 150 returning *; -- update all records update [table name] set [column name] = [value]; UPDATE [table name] SET email = 'not found' WHERE email IS NULL;","title":"Updating"},{"location":"sql/tables/#upsert","text":"-- syntax for upsert INSERT INTO tablename ( col_list ) VALUES ( value_list ) ON CONFLICT (COL_NAME) DO NOTHING -- OR UPDATE SET col = val where condition; -- syntax for upsert INSERT INTO tablename ( COL_NAME ) VALUES ( value_list ) ON CONFLICT (COL_NAME) DO UPDATE SET COL_NAME = EXCLUDED.COL_NAME;","title":"Upsert"},{"location":"sql/tables/aggregation/","text":"Aggregation COUNT (column) SUM (column) MIN & MAX LEAST & GREATEST AVG count SELECT COUNT(DISTINCT (movie_lang)) from movies; count ------- 8 -- with where clause SELECT COUNT(movie_lang) FROM movies where movie_lang = 'English'; count ------- 38 Sum select sum(revenues_domestic) from movies_revenues; sum -------- 5719.5 select SUM(revenues_domestic::numeric) from movies_revenues where revenues_domestic::numeric > 200; sum -------- 3425.6 SELECT SUM(DISTINCT revenues_domestic) FROM movies_revenues; sum -------- 5708.4 Min and Max SELECT min(movie_length), MAX(movie_length) FROM movies min | max -----+----- 87 | 168 Average, Greatest, Latest SELECT GREATEST(10, 20, 30, 40), LEAST(10, 20, 30, 40); greatest | least ----------+------- 40 | 10 SELECT GREATEST('A', 'B', 'C', 'D'), LEAST('A', 'B', 'C', 'D'); greatest | least ----------+------- D | A SELECT GREATEST('A', 'B', 'C', 1); -- ERROR: invalid input syntax for type integer: \"A\" -- LINE 1: SELECT GREATEST('A', 'B', 'C', 1); SELECT AVG(movie_length) FROM movies; avg ---------------------- 126.1320754716981132","title":"Aggregation"},{"location":"sql/tables/aggregation/#aggregation","text":"COUNT (column) SUM (column) MIN & MAX LEAST & GREATEST AVG","title":"Aggregation"},{"location":"sql/tables/aggregation/#count","text":"SELECT COUNT(DISTINCT (movie_lang)) from movies; count ------- 8 -- with where clause SELECT COUNT(movie_lang) FROM movies where movie_lang = 'English'; count ------- 38","title":"count"},{"location":"sql/tables/aggregation/#sum","text":"select sum(revenues_domestic) from movies_revenues; sum -------- 5719.5 select SUM(revenues_domestic::numeric) from movies_revenues where revenues_domestic::numeric > 200; sum -------- 3425.6 SELECT SUM(DISTINCT revenues_domestic) FROM movies_revenues; sum -------- 5708.4","title":"Sum"},{"location":"sql/tables/aggregation/#min-and-max","text":"SELECT min(movie_length), MAX(movie_length) FROM movies min | max -----+----- 87 | 168","title":"Min and Max"},{"location":"sql/tables/aggregation/#average-greatest-latest","text":"SELECT GREATEST(10, 20, 30, 40), LEAST(10, 20, 30, 40); greatest | least ----------+------- 40 | 10 SELECT GREATEST('A', 'B', 'C', 'D'), LEAST('A', 'B', 'C', 'D'); greatest | least ----------+------- D | A SELECT GREATEST('A', 'B', 'C', 1); -- ERROR: invalid input syntax for type integer: \"A\" -- LINE 1: SELECT GREATEST('A', 'B', 'C', 1); SELECT AVG(movie_length) FROM movies; avg ---------------------- 126.1320754716981132","title":"Average, Greatest, Latest"},{"location":"sql/tables/coalesce/","text":"Usefull Functions Case SELECT movie_id, movie_name, CASE WHEN movies.movie_length < 100 AND movies.movie_length <= 50 THEN 'Short' WHEN movies.movie_length > 100 AND movies.movie_length <= 130 THEN 'Medium' WHEN movies.movie_length > 130 THEN 'Long' END duration FROM movies ORDER BY movie_name; LIMIT 10; movie_id | movie_name | duration ----------+-------------------------------+---------- 1 | A Clockwork Orange | Medium 2 | Apocalypse Now | Long 3 | Battle Royale | Medium 4 | Blade Runner | Medium 5 | Chungking Express | Medium 6 | City of God | Long 7 | City of Men | Long 8 | Cold Fish | Medium 9 | Crouching Tiger Hidden Dragon | Long 10 | Eyes Wide Shut | Medium (10 rows) SELECT SUM(CASE age_certificate WHEN '12' THEN 1 ELSE 0 END) \"Kids\", SUM(CASE age_certificate WHEN '15' THEN 1 ELSE 0 END) \"School\", SUM(CASE age_certificate WHEN '18' THEN 1 ELSE 0 END) \"Teens\", SUM(CASE age_certificate WHEN 'PG' THEN 1 ELSE 0 END) \"Restricted\", SUM(CASE age_certificate WHEN 'U' THEN 1 ELSE 0 END) \"Universal\" FROM movies; Kids | School | Teens | Restricted | Universal ------+--------+-------+------------+----------- 11 | 16 | 8 | 12 | 6 Coalesce COALESCE function that returns the first non-null argument. SELECT COALESCE (NULL, 2 , 1); NULLIF The NULLIF function returns a null value if argument_1 equals to argument_2 , otherwise it returns argument_1 SELECT NULLIF (1, 1); -- return NULL SELECT NULLIF (1, 0); -- return 1 SELECT NULLIF ('A', 'B'); -- return A Cube First, specify the CUBE subclause in the the GROUP BY clause of the SELECT statement. Second, in the select list, specify the columns (dimensions or dimension columns) which you want to analyze and aggregation function expressions. Third, in the GROUP BY clause, specify the dimension columns within the parentheses of the CUBE subclause. select age_certificate, movie_length from movies group by age_certificate, cube (age_certificate, movie_length ) order by age_certificate;","title":"Usefull Functions"},{"location":"sql/tables/coalesce/#usefull-functions","text":"","title":"Usefull Functions"},{"location":"sql/tables/coalesce/#case","text":"SELECT movie_id, movie_name, CASE WHEN movies.movie_length < 100 AND movies.movie_length <= 50 THEN 'Short' WHEN movies.movie_length > 100 AND movies.movie_length <= 130 THEN 'Medium' WHEN movies.movie_length > 130 THEN 'Long' END duration FROM movies ORDER BY movie_name; LIMIT 10; movie_id | movie_name | duration ----------+-------------------------------+---------- 1 | A Clockwork Orange | Medium 2 | Apocalypse Now | Long 3 | Battle Royale | Medium 4 | Blade Runner | Medium 5 | Chungking Express | Medium 6 | City of God | Long 7 | City of Men | Long 8 | Cold Fish | Medium 9 | Crouching Tiger Hidden Dragon | Long 10 | Eyes Wide Shut | Medium (10 rows) SELECT SUM(CASE age_certificate WHEN '12' THEN 1 ELSE 0 END) \"Kids\", SUM(CASE age_certificate WHEN '15' THEN 1 ELSE 0 END) \"School\", SUM(CASE age_certificate WHEN '18' THEN 1 ELSE 0 END) \"Teens\", SUM(CASE age_certificate WHEN 'PG' THEN 1 ELSE 0 END) \"Restricted\", SUM(CASE age_certificate WHEN 'U' THEN 1 ELSE 0 END) \"Universal\" FROM movies; Kids | School | Teens | Restricted | Universal ------+--------+-------+------------+----------- 11 | 16 | 8 | 12 | 6","title":"Case"},{"location":"sql/tables/coalesce/#coalesce","text":"COALESCE function that returns the first non-null argument. SELECT COALESCE (NULL, 2 , 1);","title":"Coalesce"},{"location":"sql/tables/coalesce/#nullif","text":"The NULLIF function returns a null value if argument_1 equals to argument_2 , otherwise it returns argument_1 SELECT NULLIF (1, 1); -- return NULL SELECT NULLIF (1, 0); -- return 1 SELECT NULLIF ('A', 'B'); -- return A","title":"NULLIF"},{"location":"sql/tables/coalesce/#cube","text":"First, specify the CUBE subclause in the the GROUP BY clause of the SELECT statement. Second, in the select list, specify the columns (dimensions or dimension columns) which you want to analyze and aggregation function expressions. Third, in the GROUP BY clause, specify the dimension columns within the parentheses of the CUBE subclause. select age_certificate, movie_length from movies group by age_certificate, cube (age_certificate, movie_length ) order by age_certificate;","title":"Cube"},{"location":"sql/tables/combining-tables/","text":"Combining Tables UNION Combines result sets from two or more SELECT statements into a single result set. The order and number of the columns in the select list of all queries must be the same -- load sample data DROP TABLE IF EXISTS top_rated_films; CREATE TABLE top_rated_films( title VARCHAR NOT NULL, release_year SMALLINT ); DROP TABLE IF EXISTS most_popular_films; CREATE TABLE most_popular_films( title VARCHAR NOT NULL, release_year SMALLINT ); INSERT INTO top_rated_films(title,release_year) VALUES ('The Shawshank Redemption',1994), ('The Godfather',1972), ('12 Angry Men',1957); INSERT INTO most_popular_films(title,release_year) VALUES ('An American Pickle',2020), ('The Godfather',1972), ('Greyhound',2020); Union Query SELECT * FROM top_rated_films UNION SELECT * FROM most_popular_films; title | release_year --------------------------+-------------- An American Pickle | 2020 Greyhound | 2020 The Shawshank Redemption | 1994 The Godfather | 1972 12 Angry Men | 1957 Union All Query This will not avoid duplicate values SELECT * FROM top_rated_films UNION ALL SELECT * FROM most_popular_films; title | release_year --------------------------+-------------- The Shawshank Redemption | 1994 -- The Godfather | 1972 12 Angry Men | 1957 An American Pickle | 2020 -- The Godfather | 1972 Greyhound | 2020 UNION with ORDER BY SELECT * FROM top_rated_films UNION ALL SELECT * FROM most_popular_films ORDER BY title; title | release_year --------------------------+-------------- 12 Angry Men | 1957 An American Pickle | 2020 Greyhound | 2020 The Godfather | 1972 The Godfather | 1972 The Shawshank Redemption | 1994 Intersect Like the UNION and EXCEPT operators, the PostgreSQL INTERSECT operator combines result sets of two or more SELECT statements into a single result set. The INTERSECT operator returns any rows that are available in both result sets. SELECT * FROM most_popular_films INTERSECT SELECT * FROM top_rated_films; title | release_year ---------------+-------------- The Godfather | 1972 Except The EXCEPT operator returns distinct rows from the first (left) query that are not in the output of the second (right) query.\\ SELECT * FROM top_rated_films EXCEPT SELECT * FROM most_popular_films; title | release_year --------------------------+-------------- The Shawshank Redemption | 1994 12 Angry Men | 1957","title":"Combining Tables"},{"location":"sql/tables/combining-tables/#combining-tables","text":"","title":"Combining Tables"},{"location":"sql/tables/combining-tables/#union","text":"Combines result sets from two or more SELECT statements into a single result set. The order and number of the columns in the select list of all queries must be the same -- load sample data DROP TABLE IF EXISTS top_rated_films; CREATE TABLE top_rated_films( title VARCHAR NOT NULL, release_year SMALLINT ); DROP TABLE IF EXISTS most_popular_films; CREATE TABLE most_popular_films( title VARCHAR NOT NULL, release_year SMALLINT ); INSERT INTO top_rated_films(title,release_year) VALUES ('The Shawshank Redemption',1994), ('The Godfather',1972), ('12 Angry Men',1957); INSERT INTO most_popular_films(title,release_year) VALUES ('An American Pickle',2020), ('The Godfather',1972), ('Greyhound',2020);","title":"UNION"},{"location":"sql/tables/combining-tables/#union-query","text":"SELECT * FROM top_rated_films UNION SELECT * FROM most_popular_films; title | release_year --------------------------+-------------- An American Pickle | 2020 Greyhound | 2020 The Shawshank Redemption | 1994 The Godfather | 1972 12 Angry Men | 1957","title":"Union Query"},{"location":"sql/tables/combining-tables/#union-all-query","text":"This will not avoid duplicate values SELECT * FROM top_rated_films UNION ALL SELECT * FROM most_popular_films; title | release_year --------------------------+-------------- The Shawshank Redemption | 1994 -- The Godfather | 1972 12 Angry Men | 1957 An American Pickle | 2020 -- The Godfather | 1972 Greyhound | 2020","title":"Union All Query"},{"location":"sql/tables/combining-tables/#union-with-order-by","text":"SELECT * FROM top_rated_films UNION ALL SELECT * FROM most_popular_films ORDER BY title; title | release_year --------------------------+-------------- 12 Angry Men | 1957 An American Pickle | 2020 Greyhound | 2020 The Godfather | 1972 The Godfather | 1972 The Shawshank Redemption | 1994","title":"UNION with ORDER BY"},{"location":"sql/tables/combining-tables/#intersect","text":"Like the UNION and EXCEPT operators, the PostgreSQL INTERSECT operator combines result sets of two or more SELECT statements into a single result set. The INTERSECT operator returns any rows that are available in both result sets. SELECT * FROM most_popular_films INTERSECT SELECT * FROM top_rated_films; title | release_year ---------------+-------------- The Godfather | 1972","title":"Intersect"},{"location":"sql/tables/combining-tables/#except","text":"The EXCEPT operator returns distinct rows from the first (left) query that are not in the output of the second (right) query.\\ SELECT * FROM top_rated_films EXCEPT SELECT * FROM most_popular_films; title | release_year --------------------------+-------------- The Shawshank Redemption | 1994 12 Angry Men | 1957","title":"Except"},{"location":"sql/tables/constraints/","text":"Constraints Constraints are like Gate Keepers. Control the type of data that goes into the table. These are used to prevent invalid data. Constraints can be added on Table Column Types of Constraints Type Note NOT NULL Field must have values UNIQUE Only unique value are allowed DEFAULT Ability to SET default PRIMARY KEY Uniquely identifies each row/record FOREIGN KEY Constraint based on Column in other table CHECK Check all values must meet specific criteria NOT NULL CREATE TABLE table_nn ( id SERIAL PRIMARY KEY, tag text NOT NULL ); INSERT INTO table_nn ( tag ) VALUES ('TAG 1'), ('TAG 2'), ('TAG 3'), (''); -- NULL value wont be accepted INSERT INTO table_nn ( tag ) VALUES (NULL); -- empty string aren't NULL values INSERT INTO table_nn ( tag ) VALUES (''); -- adding another column TEXT; ALTER TABLE table_nn ADD COLUMN is_enable TEXT; -- Updating values in new column to '' -- so that NULL constrain can be added UPDATE table_nn SET is_enable = '' WHERE is_enable IS NULL; -- adding null constraint on table ALTER TABLE public.table_nn ALTER COLUMN is_enable SET NOT NULL; UNIQUE CREATE TABLE table_unique ( id SERIAL PRIMARY KEY, emails TEXT UNIQUE ); INSERT INTO table_unique ( emailS ) VALUES ('A@B.COM'),('C@D.COM'); INSERT INTO table_unique ( emailS ) VALUES ('A@B.COM'); -- error Adding Unique Constraint on column ALTER TABLE public.table_unique ADD COLUMN is_enable text; ALTER TABLE table_unique ADD CONSTRAINT unique_is_enable UNIQUE (is_enable); ALTER TABLE public.table_unique ADD COLUMN code text, ADD COLUMN code_key text; ALTER TABLE table_unique ADD CONSTRAINT unique_code_key UNIQUE (code,code_key); DEFAULT CONSTRAINT CREATE TABLE table_default ( id SERIAL PRIMARY KEY, is_enable TEXT DEFAULT 'Y' ); INSERT INTO table_default ( id ) VALUES (1),(2); select * from table_default; id | is_enable ----+----------- 1 | Y 2 | Y ALTER TABLE public.table_default ADD COLUMN tag text; ALTER TABLE public.table_default ALTER COLUMN tag SET DEFAULT 'N'; INSERT INTO table_default ( id ) VALUES (5); select * from table_default; id | is_enable | tag ----+-----------+----- 1 | Y | 2 | Y | 5 | Y | N ALTER TABLE public.table_default ALTER COLUMN tag DROP DEFAULT; Table \"public.table_default\" Column | Type | Collation | Nullable | Default -----------+---------+-----------+----------+------------------------------------------- id | integer | | not null | nextval('table_default_id_seq'::regclass) is_enable | text | | | 'Y'::text tag | text | | | Indexes: \"table_default_pkey\" PRIMARY KEY, btree (id) Primary Key Uniquely identifies each record in a database table There can be more than one UNIQUE, but only one primary key A primary key is a field in a table, which uniquely identifies each row/column in a database table When multiple fields are used as a primary key, which may consist of single or multiple fields, also known as composite key. CREATE TABLE table_item ( item_id INTEGER PRIMARY KEY, item_name varchar(20) NOT NULL ); INSERT INTO table_item ( item_id,item_name ) VALUES (1, 'pencil'), (2, 'pen' ), (3, 'box' ); INSERT INTO table_item ( item_id,item_name ) VALUES (1, 'mobile'); -- ERROR: duplicate key value violates unique constraint \"table_item_pkey\" -- DETAIL: Key (item_id)=(1) already exists. -- SQL state: 23505 ALTER TABLE table_item DROP CONSTRAINT table_item_pkey; ALTER TABLE public.table_item ADD PRIMARY KEY (item_id); Composite Primary Key CREATE TABLE table_cpk ( id VARCHAR(20) NOT NULL, another_id VARCHAR(20) NOT NULL, grade_id VARCHAR(20) NOT NULL ); INSERT INTO table_cpk ( id, another_id, grade_id ) VALUES ('1','11','12'), ('2','21','22'), ('3','31','32'); SELECT * FROM table_cpk; -- composite key = id + another_id; ALTER TABLE public.table_cpk ADD CONSTRAINT cpk_comp_pkey PRIMARY KEY (id, another_id) Foreign Key CREATE TABLE t_suppliers ( s_id SERIAL PRIMARY KEY, s_name VARCHAR(20) NOT NULL ); CREATE TABLE t_products ( p_id SERIAL PRIMARY KEY, p_name VARCHAR(10) NOT NULL, s_id INT NOT NULL, FOREIGN KEY (s_id) REFERENCES t_suppliers (s_id) ); INSERT INTO t_suppliers ( s_name ) VALUES ('SUP 1'),('SUP 2'),('SUP 3'),('SUP 4'); INSERT INTO t_products ( P_NAME, S_ID ) VALUES ('PRO 1',1),('PRO 2',2); INSERT INTO t_products ( P_NAME, S_ID ) VALUES ('PRO 1',9); -- ERROR: insert or update on table \"t_products\" -- violates foreign key constraint \"t_products_s_id_fkey\" -- DETAIL: Key (s_id)=(9) is not present in table \"t_suppliers\". -- SQL state: 23503 CHECK Constraint CREATE TABLE table_constr ( id SERIAL PRIMARY KEY, birth_date DATE CHECK ( birth_date > '1900-01-01' ), joined_date DATE CHECK ( joined_date > birth_date ), salary NUMERIC CHECK ( salary > 0 ) ); select * from table_constr; insert into table_constr (birth_date, joined_date, salary) values ('2001-02-02','2019-01-10',100000); insert into table_constr (birth_date, joined_date, salary) values ('2001-02-02','2000-01-10',100000); insert into table_constr (birth_date, joined_date, salary) values ('2001-02-02','2000-01-10',-100000); ALTER TABLE public.table_constr ADD COLUMN prices int; ALTER TABLE table_constr ADD CONSTRAINT price_check CHECK ( prices > 0 AND salary > prices ); Example CREATE TABLE web_links ( link_id SERIAL PRIMARY KEY, link_url VARCHAR(255) NOT NULL, link_target VARCHAR(20) ); SELECT * FROM web_links; ALTER TABLE web_links ADD CONSTRAINT unique_web_url UNIQUE (link_url); INSERT INTO web_links (link_url,link_target) VALUES ('https://www.google.com/','_blank'); ALTER TABLE web_links ADD COLUMN is_enable VARCHAR(2); INSERT INTO web_links (link_url,link_target,is_enable) VALUES ('https://www.amazon.com/','_blank','Y'); ALTER TABLE web_links ADD CHECK ( is_enable IN ('Y','N') ); INSERT INTO web_links (link_url,link_target,is_enable) VALUES ('https://www.NETFLIX.com/','_blank','N'); SELECT * FROM web_links; UPDATE web_links SET is_enable = 'Y' WHERE link_id = 1","title":"Constraints"},{"location":"sql/tables/constraints/#constraints","text":"Constraints are like Gate Keepers. Control the type of data that goes into the table. These are used to prevent invalid data. Constraints can be added on Table Column","title":"Constraints"},{"location":"sql/tables/constraints/#types-of-constraints","text":"Type Note NOT NULL Field must have values UNIQUE Only unique value are allowed DEFAULT Ability to SET default PRIMARY KEY Uniquely identifies each row/record FOREIGN KEY Constraint based on Column in other table CHECK Check all values must meet specific criteria","title":"Types of Constraints"},{"location":"sql/tables/constraints/#not-null","text":"CREATE TABLE table_nn ( id SERIAL PRIMARY KEY, tag text NOT NULL ); INSERT INTO table_nn ( tag ) VALUES ('TAG 1'), ('TAG 2'), ('TAG 3'), (''); -- NULL value wont be accepted INSERT INTO table_nn ( tag ) VALUES (NULL); -- empty string aren't NULL values INSERT INTO table_nn ( tag ) VALUES (''); -- adding another column TEXT; ALTER TABLE table_nn ADD COLUMN is_enable TEXT; -- Updating values in new column to '' -- so that NULL constrain can be added UPDATE table_nn SET is_enable = '' WHERE is_enable IS NULL; -- adding null constraint on table ALTER TABLE public.table_nn ALTER COLUMN is_enable SET NOT NULL;","title":"NOT NULL"},{"location":"sql/tables/constraints/#unique","text":"CREATE TABLE table_unique ( id SERIAL PRIMARY KEY, emails TEXT UNIQUE ); INSERT INTO table_unique ( emailS ) VALUES ('A@B.COM'),('C@D.COM'); INSERT INTO table_unique ( emailS ) VALUES ('A@B.COM'); -- error","title":"UNIQUE"},{"location":"sql/tables/constraints/#adding-unique-constraint-on-column","text":"ALTER TABLE public.table_unique ADD COLUMN is_enable text; ALTER TABLE table_unique ADD CONSTRAINT unique_is_enable UNIQUE (is_enable); ALTER TABLE public.table_unique ADD COLUMN code text, ADD COLUMN code_key text; ALTER TABLE table_unique ADD CONSTRAINT unique_code_key UNIQUE (code,code_key);","title":"Adding Unique Constraint on column"},{"location":"sql/tables/constraints/#default-constraint","text":"CREATE TABLE table_default ( id SERIAL PRIMARY KEY, is_enable TEXT DEFAULT 'Y' ); INSERT INTO table_default ( id ) VALUES (1),(2); select * from table_default; id | is_enable ----+----------- 1 | Y 2 | Y ALTER TABLE public.table_default ADD COLUMN tag text; ALTER TABLE public.table_default ALTER COLUMN tag SET DEFAULT 'N'; INSERT INTO table_default ( id ) VALUES (5); select * from table_default; id | is_enable | tag ----+-----------+----- 1 | Y | 2 | Y | 5 | Y | N ALTER TABLE public.table_default ALTER COLUMN tag DROP DEFAULT; Table \"public.table_default\" Column | Type | Collation | Nullable | Default -----------+---------+-----------+----------+------------------------------------------- id | integer | | not null | nextval('table_default_id_seq'::regclass) is_enable | text | | | 'Y'::text tag | text | | | Indexes: \"table_default_pkey\" PRIMARY KEY, btree (id)","title":"DEFAULT CONSTRAINT"},{"location":"sql/tables/constraints/#primary-key","text":"Uniquely identifies each record in a database table There can be more than one UNIQUE, but only one primary key A primary key is a field in a table, which uniquely identifies each row/column in a database table When multiple fields are used as a primary key, which may consist of single or multiple fields, also known as composite key. CREATE TABLE table_item ( item_id INTEGER PRIMARY KEY, item_name varchar(20) NOT NULL ); INSERT INTO table_item ( item_id,item_name ) VALUES (1, 'pencil'), (2, 'pen' ), (3, 'box' ); INSERT INTO table_item ( item_id,item_name ) VALUES (1, 'mobile'); -- ERROR: duplicate key value violates unique constraint \"table_item_pkey\" -- DETAIL: Key (item_id)=(1) already exists. -- SQL state: 23505 ALTER TABLE table_item DROP CONSTRAINT table_item_pkey; ALTER TABLE public.table_item ADD PRIMARY KEY (item_id);","title":"Primary Key"},{"location":"sql/tables/constraints/#composite-primary-key","text":"CREATE TABLE table_cpk ( id VARCHAR(20) NOT NULL, another_id VARCHAR(20) NOT NULL, grade_id VARCHAR(20) NOT NULL ); INSERT INTO table_cpk ( id, another_id, grade_id ) VALUES ('1','11','12'), ('2','21','22'), ('3','31','32'); SELECT * FROM table_cpk; -- composite key = id + another_id; ALTER TABLE public.table_cpk ADD CONSTRAINT cpk_comp_pkey PRIMARY KEY (id, another_id)","title":"Composite Primary Key"},{"location":"sql/tables/constraints/#foreign-key","text":"CREATE TABLE t_suppliers ( s_id SERIAL PRIMARY KEY, s_name VARCHAR(20) NOT NULL ); CREATE TABLE t_products ( p_id SERIAL PRIMARY KEY, p_name VARCHAR(10) NOT NULL, s_id INT NOT NULL, FOREIGN KEY (s_id) REFERENCES t_suppliers (s_id) ); INSERT INTO t_suppliers ( s_name ) VALUES ('SUP 1'),('SUP 2'),('SUP 3'),('SUP 4'); INSERT INTO t_products ( P_NAME, S_ID ) VALUES ('PRO 1',1),('PRO 2',2); INSERT INTO t_products ( P_NAME, S_ID ) VALUES ('PRO 1',9); -- ERROR: insert or update on table \"t_products\" -- violates foreign key constraint \"t_products_s_id_fkey\" -- DETAIL: Key (s_id)=(9) is not present in table \"t_suppliers\". -- SQL state: 23503","title":"Foreign Key"},{"location":"sql/tables/constraints/#check-constraint","text":"CREATE TABLE table_constr ( id SERIAL PRIMARY KEY, birth_date DATE CHECK ( birth_date > '1900-01-01' ), joined_date DATE CHECK ( joined_date > birth_date ), salary NUMERIC CHECK ( salary > 0 ) ); select * from table_constr; insert into table_constr (birth_date, joined_date, salary) values ('2001-02-02','2019-01-10',100000); insert into table_constr (birth_date, joined_date, salary) values ('2001-02-02','2000-01-10',100000); insert into table_constr (birth_date, joined_date, salary) values ('2001-02-02','2000-01-10',-100000); ALTER TABLE public.table_constr ADD COLUMN prices int; ALTER TABLE table_constr ADD CONSTRAINT price_check CHECK ( prices > 0 AND salary > prices );","title":"CHECK Constraint"},{"location":"sql/tables/constraints/#example","text":"CREATE TABLE web_links ( link_id SERIAL PRIMARY KEY, link_url VARCHAR(255) NOT NULL, link_target VARCHAR(20) ); SELECT * FROM web_links; ALTER TABLE web_links ADD CONSTRAINT unique_web_url UNIQUE (link_url); INSERT INTO web_links (link_url,link_target) VALUES ('https://www.google.com/','_blank'); ALTER TABLE web_links ADD COLUMN is_enable VARCHAR(2); INSERT INTO web_links (link_url,link_target,is_enable) VALUES ('https://www.amazon.com/','_blank','Y'); ALTER TABLE web_links ADD CHECK ( is_enable IN ('Y','N') ); INSERT INTO web_links (link_url,link_target,is_enable) VALUES ('https://www.NETFLIX.com/','_blank','N'); SELECT * FROM web_links; UPDATE web_links SET is_enable = 'Y' WHERE link_id = 1","title":"Example"},{"location":"sql/tables/cte/","text":"Common Table Expression CTE is a temporary result take from a SQL statement A second approach to create temporary tables for query data instead of using sub queries in a FROM clause CTE's are a good alternative to sub queries. CTE can be referenced multiple times in multiple places in query statement Lifetime of CTE is equal to the lifetime of a query. Types of CTE Materialized Not materialized Syntax with cte_table ( column_list ) as ( cte_query_definition ) with num as ( select * from generate_series(1, 5) as id ) select * from num; id ---- 1 2 3 4 5 CTE with Join with cte_director as ( select movie_id, movie_name, d.director_id, first_name from movies inner join directors d on d.director_id = movies.director_id ) select * from cte_director limit 5; movie_id | movie_name | director_id | first_name ----------+------------------------+-------------+------------ 20 | Let the Right One In | 1 | Tomas 46 | There Will Be Blood | 2 | Paul 40 | The Darjeeling Limited | 3 | Wes 30 | Rushmore | 3 | Wes 15 | Grand Budapest Hotel | 3 | Wes CTE with CASE WITH cte_film AS ( SELECT movie_name, movie_length title, (CASE WHEN movie_length < 100 THEN 'Short' WHEN movie_length < 120 THEN 'Medium' ELSE 'Long' END) length FROM movies ) SELECT * FROM cte_film WHERE length = 'Long' ORDER BY title limit 5; movie_name | title | length --------------------+-------+-------- The Wizard of Oz | 120 | Long Spirited Away | 120 | Long Top Gun | 121 | Long Leon | 123 | Long Gone with the Wind | 123 | Long Complex query example WITH cte_movie_count AS ( SELECT d.director_id, SUM(COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0)) AS total_revenues FROM directors d INNER JOIN movies mv ON mv.director_id = d.director_id INNER JOIN movies_revenues r ON r.movie_id = mv.movie_id GROUP BY d.director_id ) SELECT d.director_id, d.first_name, d.last_name, cte.total_revenues FROM cte_movie_count cte INNER JOIN directors d ON d.director_id = cte.director_id LIMIT 5; CTE to perform DML Loading Sample Data create table articles ( id serial, article text ); create table deleted_articles ( id serial, article text ); insert into articles (article) values ('article 1'), ('article 2'), ('article 3'), ('article 4'), ('article 5'); Query select * from articles; id | article ----+----------- 1 | article 1 2 | article 2 3 | article 3 4 | article 4 5 | article 5 select * from deleted_articles; id | article ----+--------- (0 rows) -- deleting from one table -- returning from with cte_delete_article as ( delete from articles where id = 1 returning * ) insert into deleted_articles select * from cte_delete_article; select * from deleted_articles; -- output id | article ----+----------- 1 | article 1 (1 row)","title":"Common Table Expression"},{"location":"sql/tables/cte/#common-table-expression","text":"CTE is a temporary result take from a SQL statement A second approach to create temporary tables for query data instead of using sub queries in a FROM clause CTE's are a good alternative to sub queries. CTE can be referenced multiple times in multiple places in query statement Lifetime of CTE is equal to the lifetime of a query. Types of CTE Materialized Not materialized","title":"Common Table Expression"},{"location":"sql/tables/cte/#syntax","text":"with cte_table ( column_list ) as ( cte_query_definition ) with num as ( select * from generate_series(1, 5) as id ) select * from num; id ---- 1 2 3 4 5","title":"Syntax"},{"location":"sql/tables/cte/#cte-with-join","text":"with cte_director as ( select movie_id, movie_name, d.director_id, first_name from movies inner join directors d on d.director_id = movies.director_id ) select * from cte_director limit 5; movie_id | movie_name | director_id | first_name ----------+------------------------+-------------+------------ 20 | Let the Right One In | 1 | Tomas 46 | There Will Be Blood | 2 | Paul 40 | The Darjeeling Limited | 3 | Wes 30 | Rushmore | 3 | Wes 15 | Grand Budapest Hotel | 3 | Wes","title":"CTE with Join"},{"location":"sql/tables/cte/#cte-with-case","text":"WITH cte_film AS ( SELECT movie_name, movie_length title, (CASE WHEN movie_length < 100 THEN 'Short' WHEN movie_length < 120 THEN 'Medium' ELSE 'Long' END) length FROM movies ) SELECT * FROM cte_film WHERE length = 'Long' ORDER BY title limit 5; movie_name | title | length --------------------+-------+-------- The Wizard of Oz | 120 | Long Spirited Away | 120 | Long Top Gun | 121 | Long Leon | 123 | Long Gone with the Wind | 123 | Long","title":"CTE with CASE"},{"location":"sql/tables/cte/#complex-query-example","text":"WITH cte_movie_count AS ( SELECT d.director_id, SUM(COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0)) AS total_revenues FROM directors d INNER JOIN movies mv ON mv.director_id = d.director_id INNER JOIN movies_revenues r ON r.movie_id = mv.movie_id GROUP BY d.director_id ) SELECT d.director_id, d.first_name, d.last_name, cte.total_revenues FROM cte_movie_count cte INNER JOIN directors d ON d.director_id = cte.director_id LIMIT 5;","title":"Complex query example"},{"location":"sql/tables/cte/#cte-to-perform-dml","text":"","title":"CTE to perform DML"},{"location":"sql/tables/cte/#loading-sample-data","text":"create table articles ( id serial, article text ); create table deleted_articles ( id serial, article text ); insert into articles (article) values ('article 1'), ('article 2'), ('article 3'), ('article 4'), ('article 5');","title":"Loading Sample Data"},{"location":"sql/tables/cte/#query","text":"select * from articles; id | article ----+----------- 1 | article 1 2 | article 2 3 | article 3 4 | article 4 5 | article 5 select * from deleted_articles; id | article ----+--------- (0 rows) -- deleting from one table -- returning from with cte_delete_article as ( delete from articles where id = 1 returning * ) insert into deleted_articles select * from cte_delete_article; select * from deleted_articles; -- output id | article ----+----------- 1 | article 1 (1 row)","title":"Query"},{"location":"sql/tables/group-by-and-having/","text":"GROUP BY and HAVING GROUP BY GROUP BY clause divide the rows returned from SELECT statement into groups For each group, you can apply aggregate functions like COUNT , SUM , MIN , MAX etc. Syntax SELECT column1, AGGREGATE_FUNCTION(column2) FROM tablename GROUP BY column1; Group BY Group the data in column and pass it to aggregate function Group BY with count SELECT movie_lang, COUNT(movie_lang) as count FROM movies GROUP BY movie_lang ORDER BY count ASC; movie_lang | count ------------+------- Swedish | 1 German | 1 Korean | 1 Spanish | 1 Portuguese | 2 Japanese | 4 Chinese | 5 English | 38 Group BY with SUM SELECT age_certificate, SUM(movie_length) FROM movies GROUP BY age_certificate; age_certificate | sum -----------------+------ PG | 1462 15 | 2184 12 | 1425 18 | 994 U | 620 Group BY with MIN, MAX SELECT movie_lang, MIN(movie_length), MAX(movie_length) FROM movies GROUP BY movie_lang movie_lang | min | max ------------+-----+----- Portuguese | 140 | 145 German | 165 | 165 Chinese | 99 | 139 English | 87 | 168 Swedish | 128 | 128 Spanish | 98 | 98 Korean | 130 | 130 Japanese | 107 | 120 SELECT movie_lang, MIN(movie_length), MAX(movie_length) FROM movies GROUP BY movie_lang ORDER BY MAX(movie_length) DESC; movie_lang | min | max ------------+-----+----- English | 87 | 168 German | 165 | 165 Portuguese | 140 | 145 Chinese | 99 | 139 Korean | 130 | 130 Swedish | 128 | 128 Japanese | 107 | 120 Spanish | 98 | 98 HAVING We use HAVING clause to specify a search condition for a group or an aggregate The HAVING clause is often used with the GROUP BY clause to filter rows based on filter condition cannot use column alias with having clause because it is evaluated before the SELECT statement SELECT column1, AGGREGATE_FUNCTION(column2), FROM tablename GROUP BY column1 HAVING condition; HAVING AGGREGATE_FUNCTION(column2) = value HAVING AGGREGATE_FUNCTION(column2) >= value SELECT movie_lang, SUM(movie_length) FROM movies GROUP BY movie_lang HAVING SUM(movie_length) > 200 ORDER BY SUM(movie_length); movie_lang | sum ------------+------ Portuguese | 285 Japanese | 446 Chinese | 609 English | 4824 HAVING vs WHERE HAVING works on result group WHERE works on SELECT columns and not on the result group SELECT movie_lang, SUM(movie_length) FROM movies GROUP BY movie_lang ORDER BY 2 DESC; movie_lang | sum ------------+------ English | 4824 Chinese | 609 Japanese | 446 Portuguese | 285 German | 165 Korean | 130 Swedish | 128 Spanish | 98","title":"GROUP BY and HAVING"},{"location":"sql/tables/group-by-and-having/#group-by-and-having","text":"","title":"GROUP BY and HAVING"},{"location":"sql/tables/group-by-and-having/#group-by","text":"GROUP BY clause divide the rows returned from SELECT statement into groups For each group, you can apply aggregate functions like COUNT , SUM , MIN , MAX etc.","title":"GROUP BY"},{"location":"sql/tables/group-by-and-having/#syntax","text":"SELECT column1, AGGREGATE_FUNCTION(column2) FROM tablename GROUP BY column1;","title":"Syntax"},{"location":"sql/tables/group-by-and-having/#group-by_1","text":"Group the data in column and pass it to aggregate function","title":"Group BY"},{"location":"sql/tables/group-by-and-having/#group-by-with-count","text":"SELECT movie_lang, COUNT(movie_lang) as count FROM movies GROUP BY movie_lang ORDER BY count ASC; movie_lang | count ------------+------- Swedish | 1 German | 1 Korean | 1 Spanish | 1 Portuguese | 2 Japanese | 4 Chinese | 5 English | 38","title":"Group BY with count"},{"location":"sql/tables/group-by-and-having/#group-by-with-sum","text":"SELECT age_certificate, SUM(movie_length) FROM movies GROUP BY age_certificate; age_certificate | sum -----------------+------ PG | 1462 15 | 2184 12 | 1425 18 | 994 U | 620","title":"Group BY with SUM"},{"location":"sql/tables/group-by-and-having/#group-by-with-min-max","text":"SELECT movie_lang, MIN(movie_length), MAX(movie_length) FROM movies GROUP BY movie_lang movie_lang | min | max ------------+-----+----- Portuguese | 140 | 145 German | 165 | 165 Chinese | 99 | 139 English | 87 | 168 Swedish | 128 | 128 Spanish | 98 | 98 Korean | 130 | 130 Japanese | 107 | 120 SELECT movie_lang, MIN(movie_length), MAX(movie_length) FROM movies GROUP BY movie_lang ORDER BY MAX(movie_length) DESC; movie_lang | min | max ------------+-----+----- English | 87 | 168 German | 165 | 165 Portuguese | 140 | 145 Chinese | 99 | 139 Korean | 130 | 130 Swedish | 128 | 128 Japanese | 107 | 120 Spanish | 98 | 98","title":"Group BY with MIN, MAX"},{"location":"sql/tables/group-by-and-having/#having","text":"We use HAVING clause to specify a search condition for a group or an aggregate The HAVING clause is often used with the GROUP BY clause to filter rows based on filter condition cannot use column alias with having clause because it is evaluated before the SELECT statement SELECT column1, AGGREGATE_FUNCTION(column2), FROM tablename GROUP BY column1 HAVING condition; HAVING AGGREGATE_FUNCTION(column2) = value HAVING AGGREGATE_FUNCTION(column2) >= value SELECT movie_lang, SUM(movie_length) FROM movies GROUP BY movie_lang HAVING SUM(movie_length) > 200 ORDER BY SUM(movie_length); movie_lang | sum ------------+------ Portuguese | 285 Japanese | 446 Chinese | 609 English | 4824","title":"HAVING"},{"location":"sql/tables/group-by-and-having/#having-vs-where","text":"HAVING works on result group WHERE works on SELECT columns and not on the result group SELECT movie_lang, SUM(movie_length) FROM movies GROUP BY movie_lang ORDER BY 2 DESC; movie_lang | sum ------------+------ English | 4824 Chinese | 609 Japanese | 446 Portuguese | 285 German | 165 Korean | 130 Swedish | 128 Spanish | 98","title":"HAVING vs WHERE"},{"location":"sql/tables/miscellaneous/","text":"OPERATORS Logical SELECT 1=1, 1<1, 1>1, 1<=1, 1>=1; ?column? | ?column? | ?column? | ?column? | ?column? ----------+----------+----------+----------+---------- t | f | f | t | t SELECT 1<>1; ?column? ---------- f SELECT 1 = 1 or 1 = 2 ; ?column? ---------- t select 1 / 0; -- ERROR: division by zero Pattern Matching SELECT * FROM actors WHERE last_name LIKE '%son%'; first_name | last_name | gender | date_of_birth ------------+-------------+--------+--------------------- Woody | Harrelson | M | 1961-07-23 Samuel | Jackson | M | 1948-12-21 Lina | Leandersson | F | 1995-09-27 Jack | Nicholson | M | 1937-04-22 Mykelti | Williamson | M | 1957-03-04 Luke | Wilson | M | 1971-09-21 Owen | Wilson | M | 1968-11-18 Patrick | Wilson | M | 1973-07-03 -- ILIKE -- to ignore the case SELECT * FROM actors WHERE last_name ILIKE '__i%'; first_name | last_name | gender | date_of_birth ------------+-----------+--------+--------------------- Hiroki | Doi | M | 1999-08-10 Alec | Guiness | M | 1914-04-02 Rumi | Hiiragi | F | 1987-08-01 Miyu | Irino | M | 1988-02-19 Keira | Knightley | F | 1985-03-26 Vivien | Leigh | F | 1913-11-05 Yasmin | Paige | F | 1991-06-24 Tilda | Swinton | F | 1960-11-05 Robin | Wright | F | 1966-04-08 select 'hello' like 'hello'; select 'hello' like 'h%'; select 'hello' like '%e%'; select 'hello' like '%lo'; select 'hello' like '_ello'; select 'hello' like '__llo'; select 'hello' like '%ll_'; ?column? | ?column? | ?column? | ?column? | ?column? | ?column? | ?column? ----------+----------+----------+----------+----------+----------+---------- t | t | t | t | t | t | t -- like with length of characters select * from table where name like '____'; Tips When using AND and OR in sample SQL statement, use brackets to differentiate between statements AND operator is processed before OR operator SQL treats AND operator like multiplication and OR like divide Fetch -- OFFSET start { ROW | ROWS } -- FETCH { FIRST | NEXT } { ROW_COUNT } { ROWS|ROW } ONLY SELECT * FROM movies fetch first row only ; movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id ----------+--------------------+--------------+------------+--------------+-----------------+------------- 1 | A Clockwork Orange | 112 | English | 1972-02-02 | 18 | 13 SELECT * FROM movies offset 3 fetch next 10 row only ; movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id ----------+-------------------------------+--------------+------------+--------------+-----------------+------------- 4 | Blade Runner | 121 | English | 1982-06-25 | 15 | 27 5 | Chungking Express | 113 | Chinese | 1996-08-03 | 15 | 35 6 | City of God | 145 | Portuguese | 2003-01-17 | 18 | 20 7 | City of Men | 140 | Portuguese | 2008-02-29 | 15 | 22 8 | Cold Fish | 108 | Japanese | 2010-09-12 | 18 | 30 9 | Crouching Tiger Hidden Dragon | 139 | Chinese | 2000-07-06 | 12 | 15 10 | Eyes Wide Shut | 130 | English | 1999-07-16 | 18 | 13 11 | Forrest Gump | 119 | English | 1994-07-06 | PG | 36 12 | Gladiator | 165 | English | 2000-05-05 | 15 | 27 13 | Gone with the Wind | 123 | English | 1939-12-15 | PG | 8 IS NULL or IS NOT NULL select * from actors where date_of_birth is null; first_name | last_name | gender | date_of_birth ------------+-----------+--------+--------------- Xian | Gao | M |","title":"OPERATORS"},{"location":"sql/tables/miscellaneous/#operators","text":"","title":"OPERATORS"},{"location":"sql/tables/miscellaneous/#logical","text":"SELECT 1=1, 1<1, 1>1, 1<=1, 1>=1; ?column? | ?column? | ?column? | ?column? | ?column? ----------+----------+----------+----------+---------- t | f | f | t | t SELECT 1<>1; ?column? ---------- f SELECT 1 = 1 or 1 = 2 ; ?column? ---------- t select 1 / 0; -- ERROR: division by zero","title":"Logical"},{"location":"sql/tables/miscellaneous/#pattern-matching","text":"SELECT * FROM actors WHERE last_name LIKE '%son%'; first_name | last_name | gender | date_of_birth ------------+-------------+--------+--------------------- Woody | Harrelson | M | 1961-07-23 Samuel | Jackson | M | 1948-12-21 Lina | Leandersson | F | 1995-09-27 Jack | Nicholson | M | 1937-04-22 Mykelti | Williamson | M | 1957-03-04 Luke | Wilson | M | 1971-09-21 Owen | Wilson | M | 1968-11-18 Patrick | Wilson | M | 1973-07-03 -- ILIKE -- to ignore the case SELECT * FROM actors WHERE last_name ILIKE '__i%'; first_name | last_name | gender | date_of_birth ------------+-----------+--------+--------------------- Hiroki | Doi | M | 1999-08-10 Alec | Guiness | M | 1914-04-02 Rumi | Hiiragi | F | 1987-08-01 Miyu | Irino | M | 1988-02-19 Keira | Knightley | F | 1985-03-26 Vivien | Leigh | F | 1913-11-05 Yasmin | Paige | F | 1991-06-24 Tilda | Swinton | F | 1960-11-05 Robin | Wright | F | 1966-04-08 select 'hello' like 'hello'; select 'hello' like 'h%'; select 'hello' like '%e%'; select 'hello' like '%lo'; select 'hello' like '_ello'; select 'hello' like '__llo'; select 'hello' like '%ll_'; ?column? | ?column? | ?column? | ?column? | ?column? | ?column? | ?column? ----------+----------+----------+----------+----------+----------+---------- t | t | t | t | t | t | t -- like with length of characters select * from table where name like '____';","title":"Pattern Matching"},{"location":"sql/tables/miscellaneous/#tips","text":"When using AND and OR in sample SQL statement, use brackets to differentiate between statements AND operator is processed before OR operator SQL treats AND operator like multiplication and OR like divide","title":"Tips"},{"location":"sql/tables/miscellaneous/#fetch","text":"-- OFFSET start { ROW | ROWS } -- FETCH { FIRST | NEXT } { ROW_COUNT } { ROWS|ROW } ONLY SELECT * FROM movies fetch first row only ; movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id ----------+--------------------+--------------+------------+--------------+-----------------+------------- 1 | A Clockwork Orange | 112 | English | 1972-02-02 | 18 | 13 SELECT * FROM movies offset 3 fetch next 10 row only ; movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id ----------+-------------------------------+--------------+------------+--------------+-----------------+------------- 4 | Blade Runner | 121 | English | 1982-06-25 | 15 | 27 5 | Chungking Express | 113 | Chinese | 1996-08-03 | 15 | 35 6 | City of God | 145 | Portuguese | 2003-01-17 | 18 | 20 7 | City of Men | 140 | Portuguese | 2008-02-29 | 15 | 22 8 | Cold Fish | 108 | Japanese | 2010-09-12 | 18 | 30 9 | Crouching Tiger Hidden Dragon | 139 | Chinese | 2000-07-06 | 12 | 15 10 | Eyes Wide Shut | 130 | English | 1999-07-16 | 18 | 13 11 | Forrest Gump | 119 | English | 1994-07-06 | PG | 36 12 | Gladiator | 165 | English | 2000-05-05 | 15 | 27 13 | Gone with the Wind | 123 | English | 1939-12-15 | PG | 8","title":"Fetch"},{"location":"sql/tables/miscellaneous/#is-null-or-is-not-null","text":"select * from actors where date_of_birth is null; first_name | last_name | gender | date_of_birth ------------+-----------+--------+--------------- Xian | Gao | M |","title":"IS NULL or IS NOT NULL"},{"location":"sql/tables/order-by/","text":"ORDER BY and DISTINCT Order BY SELECT company_name, contact_name FROM customers ORDER BY company_name DESC, contact_name LIMIT 10; company_name | contact_name -----------------------------------+------------------------- Wolski Zajazd | Zbyszek Piestrzeniewicz Wilman Kala | Matti Karttunen White Clover Markets | Karl Jablonski Wellington Importadora | Paula Parente Wartian Herkku | Pirkko Koskitalo Vins et alcools Chevalier | Paul Henriot Victuailles en stock | Mary Saveley Vaffeljernet | Palle Ibsen Trails Head Gourmet Provisioners | Helvetius Nagy Tradi\u00e7\u00e3o Hipermercados | Anabela Domingues SELECT orderNumber, orderlinenumber, quantityOrdered * priceEach as final_price FROM orderdetails ORDER BY final_price DESC LIMIT 10; -- orders order_id | product_id | total_price ----------+------------+-------------------- 10981 | 38 | 15810 10865 | 38 | 15810 10353 | 38 | 10540 10417 | 38 | 10540 10889 | 38 | 10540 10424 | 38 | 10329 10897 | 29 | 9903 10372 | 38 | 8432 10816 | 38 | 7905 10540 | 38 | 7905 (10 rows) -- using alias in order by SELECT first_name, last_name as surname FROM actors ORDER BY surname DESC ; first_name | surname ------------+--------- Ziyi | Zhang Billy | Zane Sean | Young Jin-seo | Yoon Ji-tae | Yoo -- NULLS FIRST AND LAST select * from actors order by gender NULLS LAST LIMIT 5; first_name | last_name | gender | date_of_birth ------------+-----------+--------+--------------------- Malin | Akerman | F | 1978-05-12 00:00:00 Julie | Andrews | F | 1935-10-01 00:00:00 Ivana | Baquero | F | 1994-06-11 00:00:00 Lorraine | Bracco | F | 1954-10-02 00:00:00 Alice | Braga | F | 1983-04-15 00:00:00 ORDER BY on multiple columns The following statement selects the first name and last name from the customer table and sorts the rows by the first name in ascending order and last name in descending order: SELECT first_name, last_name FROM customer ORDER BY first_name ASC, last_name DESC; First Name Last Name Kelly Torres Kelly Knott In this example, the ORDER BY clause sorts rows by values in the first name column first. And then it sorts the sorted rows by values in the last name column. As you can see clearly from the output, two customers with the same first name Kelly have the last name sorted in descending order. DISTINCT SELECT DISTINCT region FROM customers WHERE country = 'USA' LIMIT 10; region -------- NM CA AK WY OR MT ID WA (8 rows) Distinct COUNT SELECT COUNT(DISTINCT region) FROM customers WHERE country = 'USA'; count ------- 8","title":"ORDER BY and DISTINCT"},{"location":"sql/tables/order-by/#order-by-and-distinct","text":"","title":"ORDER BY and DISTINCT"},{"location":"sql/tables/order-by/#order-by","text":"SELECT company_name, contact_name FROM customers ORDER BY company_name DESC, contact_name LIMIT 10; company_name | contact_name -----------------------------------+------------------------- Wolski Zajazd | Zbyszek Piestrzeniewicz Wilman Kala | Matti Karttunen White Clover Markets | Karl Jablonski Wellington Importadora | Paula Parente Wartian Herkku | Pirkko Koskitalo Vins et alcools Chevalier | Paul Henriot Victuailles en stock | Mary Saveley Vaffeljernet | Palle Ibsen Trails Head Gourmet Provisioners | Helvetius Nagy Tradi\u00e7\u00e3o Hipermercados | Anabela Domingues SELECT orderNumber, orderlinenumber, quantityOrdered * priceEach as final_price FROM orderdetails ORDER BY final_price DESC LIMIT 10; -- orders order_id | product_id | total_price ----------+------------+-------------------- 10981 | 38 | 15810 10865 | 38 | 15810 10353 | 38 | 10540 10417 | 38 | 10540 10889 | 38 | 10540 10424 | 38 | 10329 10897 | 29 | 9903 10372 | 38 | 8432 10816 | 38 | 7905 10540 | 38 | 7905 (10 rows) -- using alias in order by SELECT first_name, last_name as surname FROM actors ORDER BY surname DESC ; first_name | surname ------------+--------- Ziyi | Zhang Billy | Zane Sean | Young Jin-seo | Yoon Ji-tae | Yoo -- NULLS FIRST AND LAST select * from actors order by gender NULLS LAST LIMIT 5; first_name | last_name | gender | date_of_birth ------------+-----------+--------+--------------------- Malin | Akerman | F | 1978-05-12 00:00:00 Julie | Andrews | F | 1935-10-01 00:00:00 Ivana | Baquero | F | 1994-06-11 00:00:00 Lorraine | Bracco | F | 1954-10-02 00:00:00 Alice | Braga | F | 1983-04-15 00:00:00","title":"Order BY"},{"location":"sql/tables/order-by/#order-by-on-multiple-columns","text":"The following statement selects the first name and last name from the customer table and sorts the rows by the first name in ascending order and last name in descending order: SELECT first_name, last_name FROM customer ORDER BY first_name ASC, last_name DESC; First Name Last Name Kelly Torres Kelly Knott In this example, the ORDER BY clause sorts rows by values in the first name column first. And then it sorts the sorted rows by values in the last name column. As you can see clearly from the output, two customers with the same first name Kelly have the last name sorted in descending order.","title":"ORDER BY on multiple columns"},{"location":"sql/tables/order-by/#distinct","text":"SELECT DISTINCT region FROM customers WHERE country = 'USA' LIMIT 10; region -------- NM CA AK WY OR MT ID WA (8 rows)","title":"DISTINCT"},{"location":"sql/tables/order-by/#distinct-count","text":"SELECT COUNT(DISTINCT region) FROM customers WHERE country = 'USA'; count ------- 8","title":"Distinct COUNT"},{"location":"sql/tables/views/","text":"Views Plain View A view is a database object that is a stored query. A view is a virtual table you can create dynamically using a saved query acting as a virtual table. You can join a view to another table or view You can query a view Regular views don't store any data, but materialised view does Syntax CREATE OR REPLACE VIEW view_name AS query Example 1 CREATE OR REPLACE VIEW v_movie_quick AS SELECT movie_name, movie_length, release_date from movies mv; -- use just like a normal table select * from v_movie_quick limit 5; movie_name | movie_length | release_date --------------------+--------------+-------------- A Clockwork Orange | 112 | 1972-02-02 Apocalypse Now | 168 | 1979-08-15 Battle Royale | 111 | 2001-01-04 Blade Runner | 121 | 1982-06-25 Chungking Express | 113 | 1996-08-03 Example with JOIN CREATE OR REPLACE VIEW v_movie_d_name as SELECT movie_name, movie_length, release_date, d.first_name || ' ' || d.last_name as \"full name\" from movies mv inner join directors d on mv.director_id = d.director_id; select * from v_movie_d_name limit 5; movie_name | movie_length | release_date | full name ------------------------+--------------+--------------+----------------- Let the Right One In | 128 | 2008-10-24 | Tomas Alfredson There Will Be Blood | 168 | 2007-12-26 | Paul Anderson The Darjeeling Limited | 119 | 2007-09-29 | Wes Anderson Rushmore | 104 | 1998-11-12 | Wes Anderson Grand Budapest Hotel | 117 | 2014-07-03 | Wes Anderson Managing VIEW ALTER VIEW v_movie_d_name RENAME TO v_movie_with_names; -- ALTER VIEW select * from v_movie_with_names limit 5; -- SAME OUTPUT AS ABOVE -- dropping view DROP VIEW if exists v_movie_quick; View containing condition CREATE OR REPLACE VIEW v_movie_after_1997 as select * from movies where release_date >= '1997-12-31' and movie_lang = 'English' order by release_date desc limit 5; movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id ----------+--------------------------------------------+--------------+------------+--------------+-----------------+------------- 47 | Three Billboards Outside Ebbing, Missouri | 134 | English | 2017-11-10 | 15 | 18 15 | Grand Budapest Hotel | 117 | English | 2014-07-03 | PG | 3 22 | Life of Pi | 129 | English | 2012-11-21 | PG | 15 38 | Submarine | 115 | English | 2011-06-03 | 15 | 4 24 | Never Let Me Go | 117 | English | 2010-09-15 | 15 | 25 You cannot add, update, delete columns from a view once created for that create a new view, drop the old view and rename the new view back to original Deleting from view also deletes from the main table CREATE OR REPLACE VIEW v_movie as select * from movies; select * from v_movie limit 5; select * from movies limit 5; -- deleting from v_movie also deletes from main table delete from v_movie where movie_id = 4; Materialized View Allows you to store result of a query update data periodically :: manual used to cache result of heavy data syntax CREATE MATERIALIZED VIEW IF NOT EXISTS view_name AS query WITH [ NO ] DATA; If you want to load data into the materialised view at the creation time, you will use WITH DATA create materialized view if not exists mv_dir as select first_name, last_name from directors with no data; -- this will give error becuz no data -- is loaded at time of creation select * from mv_dir limit 5; -- refreshing view to load data in view refresh materialized view mv_dir; select * from mv_dir limit 5; -- dropping materialized view drop materialized view mv_dir; cannot change data in materialised view : insert, update and delete advantage of using materialised view : access and update materialised view without locking everyone else out disadvantage of using materialised view if you alter the base table , materialised view must also me alter : delete the old materialised view and create a new one","title":"Views"},{"location":"sql/tables/views/#views","text":"","title":"Views"},{"location":"sql/tables/views/#plain-view","text":"A view is a database object that is a stored query. A view is a virtual table you can create dynamically using a saved query acting as a virtual table. You can join a view to another table or view You can query a view Regular views don't store any data, but materialised view does","title":"Plain View"},{"location":"sql/tables/views/#syntax","text":"CREATE OR REPLACE VIEW view_name AS query","title":"Syntax"},{"location":"sql/tables/views/#example-1","text":"CREATE OR REPLACE VIEW v_movie_quick AS SELECT movie_name, movie_length, release_date from movies mv; -- use just like a normal table select * from v_movie_quick limit 5; movie_name | movie_length | release_date --------------------+--------------+-------------- A Clockwork Orange | 112 | 1972-02-02 Apocalypse Now | 168 | 1979-08-15 Battle Royale | 111 | 2001-01-04 Blade Runner | 121 | 1982-06-25 Chungking Express | 113 | 1996-08-03","title":"Example 1"},{"location":"sql/tables/views/#example-with-join","text":"CREATE OR REPLACE VIEW v_movie_d_name as SELECT movie_name, movie_length, release_date, d.first_name || ' ' || d.last_name as \"full name\" from movies mv inner join directors d on mv.director_id = d.director_id; select * from v_movie_d_name limit 5; movie_name | movie_length | release_date | full name ------------------------+--------------+--------------+----------------- Let the Right One In | 128 | 2008-10-24 | Tomas Alfredson There Will Be Blood | 168 | 2007-12-26 | Paul Anderson The Darjeeling Limited | 119 | 2007-09-29 | Wes Anderson Rushmore | 104 | 1998-11-12 | Wes Anderson Grand Budapest Hotel | 117 | 2014-07-03 | Wes Anderson","title":"Example with JOIN"},{"location":"sql/tables/views/#managing-view","text":"ALTER VIEW v_movie_d_name RENAME TO v_movie_with_names; -- ALTER VIEW select * from v_movie_with_names limit 5; -- SAME OUTPUT AS ABOVE -- dropping view DROP VIEW if exists v_movie_quick;","title":"Managing VIEW"},{"location":"sql/tables/views/#view-containing-condition","text":"CREATE OR REPLACE VIEW v_movie_after_1997 as select * from movies where release_date >= '1997-12-31' and movie_lang = 'English' order by release_date desc limit 5; movie_id | movie_name | movie_length | movie_lang | release_date | age_certificate | director_id ----------+--------------------------------------------+--------------+------------+--------------+-----------------+------------- 47 | Three Billboards Outside Ebbing, Missouri | 134 | English | 2017-11-10 | 15 | 18 15 | Grand Budapest Hotel | 117 | English | 2014-07-03 | PG | 3 22 | Life of Pi | 129 | English | 2012-11-21 | PG | 15 38 | Submarine | 115 | English | 2011-06-03 | 15 | 4 24 | Never Let Me Go | 117 | English | 2010-09-15 | 15 | 25 You cannot add, update, delete columns from a view once created for that create a new view, drop the old view and rename the new view back to original","title":"View containing condition"},{"location":"sql/tables/views/#deleting-from-view-also-deletes-from-the-main-table","text":"CREATE OR REPLACE VIEW v_movie as select * from movies; select * from v_movie limit 5; select * from movies limit 5; -- deleting from v_movie also deletes from main table delete from v_movie where movie_id = 4;","title":"Deleting from view also deletes from the main table"},{"location":"sql/tables/views/#materialized-view","text":"","title":"Materialized View"},{"location":"sql/tables/views/#allows-you-to","text":"store result of a query update data periodically :: manual used to cache result of heavy data syntax CREATE MATERIALIZED VIEW IF NOT EXISTS view_name AS query WITH [ NO ] DATA; If you want to load data into the materialised view at the creation time, you will use WITH DATA create materialized view if not exists mv_dir as select first_name, last_name from directors with no data; -- this will give error becuz no data -- is loaded at time of creation select * from mv_dir limit 5; -- refreshing view to load data in view refresh materialized view mv_dir; select * from mv_dir limit 5; -- dropping materialized view drop materialized view mv_dir; cannot change data in materialised view : insert, update and delete advantage of using materialised view : access and update materialised view without locking everyone else out disadvantage of using materialised view if you alter the base table , materialised view must also me alter : delete the old materialised view and create a new one","title":"Allows you to"},{"location":"sql/tables/where/","text":"WHERE Clause Operators Available Operator Description = Equal > Greater than < Less than >= Greater than or equal <= Less than or equal <> or != Not equal AND Logical operator AND OR Logical operator OR IN Return true if a value matches any value in a list BETWEEN Return true if a value is between a range of values LIKE Return true if a value matches a pattern IS NULL Return true if a value is NULL NOT Negate the result of other operators Where Cannot use column alias with where clause SELECT select_list FROM table_name WHERE condition ORDER BY sort_expression Where with OR select first_name, last_name, date_of_birth from actors where date_of_birth < '1990-01-01' or date_of_birth > '1980-01-01' LIMIT 10; first_name | last_name | date_of_birth ------------+-----------+--------------------- Malin | Akerman | 1978-05-12 Tim | Allen | 1953-06-13 Julie | Andrews | 1935-10-01 Ivana | Baquero | 1994-06-11 Lorraine | Bracco | 1954-10-02 Alice | Braga | 1983-04-15 Marlon | Brando | 1924-04-03 Adrien | Brody | 1973-04-14 Peter | Carlberg | 1950-12-08 Gemma | Chan | 1982-11-29 select first_name, last_name, date_of_birth from actors where date_of_birth < '1990-01-01' or date_of_birth > '1980-01-01' ORDER BY date_of_birth LIMIT 10; -- with order by clause first_name | last_name | date_of_birth ------------+-----------+--------------------- Clark | Gable | 1901-02-01 00:00:00 Scatman | Crothers | 1910-05-23 00:00:00 Vivien | Leigh | 1913-11-05 00:00:00 Alec | Guiness | 1914-04-02 00:00:00 Judy | Garland | 1922-06-10 00:00:00 Marlon | Brando | 1924-04-03 00:00:00 Dick | Van Dyke | 1925-12-13 00:00:00 Sihung | Lung | 1930-01-01 00:00:00 Ian | Holm | 1931-09-12 00:00:00 Rebecca | Pan | 1931-12-29 00:00:00 Where with AND select * from directors where date_of_birth > '1970-01-01' AND nationality = 'British'; director_id | first_name | last_name | date_of_birth | nationality -------------+------------+-----------+---------------+------------- 4 | Richard | Ayoade | 1977-06-12 | British 18 | Martin | McDonagh | 1970-03-26 | British Where with BETWEEN (NOT BETWEEN) select * from directors where date_of_birth between '1970-01-01' and '1990-01-01'; director_id | first_name | last_name | date_of_birth | nationality -------------+------------+--------------------------+---------------+------------- 2 | Paul | Anderson | 1970-06-26 | American 4 | Richard | Ayoade | 1977-06-12 | British 11 | Florian | Henckel von Donnersmarck | 1973-05-02 | German 18 | Martin | McDonagh | 1970-03-26 | British Where with LIKE SELECT first_name, last_name FROM actors WHERE actors.last_name LIKE '%son' ORDER BY first_name; first_name | last_name ------------+------------- Jack | Nicholson Lina | Leandersson Luke | Wilson Mykelti | Williamson Owen | Wilson Patrick | Wilson Samuel | Jackson Woody | Harrelson Where with IN and NOT IN select * from directors where nationality in ('American', 'Japenese'); director_id | first_name | last_name | date_of_birth | nationality -------------+------------+--------------------------+---------------+-------------- 1 | Tomas | Alfredson | 1965-04-01 | Swedish 2 | Paul | Anderson | 1970-06-26 | American 3 | Wes | Anderson | 1969-05-01 | American 4 | Richard | Ayoade | 1977-06-12 | British 5 | Luc | Besson | 1959-03-18 | French 6 | James | Cameron | 1954-08-16 | American 7 | Guillermo | del Toro | 1964-10-09 | Mexican select * from directors where nationality not in ('American', 'Japenese'); director_id | first_name | last_name | date_of_birth | nationality -------------+------------+--------------------------+---------------+-------------- 1 | Tomas | Alfredson | 1965-04-01 | Swedish 4 | Richard | Ayoade | 1977-06-12 | British 5 | Luc | Besson | 1959-03-18 | French 7 | Guillermo | del Toro | 1964-10-09 | Mexican 10 | Kinji | Fukasaku | 1930-07-03 | Japanese 11 | Florian | Henckel von Donnersmarck | 1973-05-02 | German 12 | Terry | Jones | 1942-02-01 | British","title":"WHERE Clause"},{"location":"sql/tables/where/#where-clause","text":"Operators Available Operator Description = Equal > Greater than < Less than >= Greater than or equal <= Less than or equal <> or != Not equal AND Logical operator AND OR Logical operator OR IN Return true if a value matches any value in a list BETWEEN Return true if a value is between a range of values LIKE Return true if a value matches a pattern IS NULL Return true if a value is NULL NOT Negate the result of other operators","title":"WHERE Clause"},{"location":"sql/tables/where/#where","text":"Cannot use column alias with where clause SELECT select_list FROM table_name WHERE condition ORDER BY sort_expression","title":"Where"},{"location":"sql/tables/where/#where-with-or","text":"select first_name, last_name, date_of_birth from actors where date_of_birth < '1990-01-01' or date_of_birth > '1980-01-01' LIMIT 10; first_name | last_name | date_of_birth ------------+-----------+--------------------- Malin | Akerman | 1978-05-12 Tim | Allen | 1953-06-13 Julie | Andrews | 1935-10-01 Ivana | Baquero | 1994-06-11 Lorraine | Bracco | 1954-10-02 Alice | Braga | 1983-04-15 Marlon | Brando | 1924-04-03 Adrien | Brody | 1973-04-14 Peter | Carlberg | 1950-12-08 Gemma | Chan | 1982-11-29 select first_name, last_name, date_of_birth from actors where date_of_birth < '1990-01-01' or date_of_birth > '1980-01-01' ORDER BY date_of_birth LIMIT 10; -- with order by clause first_name | last_name | date_of_birth ------------+-----------+--------------------- Clark | Gable | 1901-02-01 00:00:00 Scatman | Crothers | 1910-05-23 00:00:00 Vivien | Leigh | 1913-11-05 00:00:00 Alec | Guiness | 1914-04-02 00:00:00 Judy | Garland | 1922-06-10 00:00:00 Marlon | Brando | 1924-04-03 00:00:00 Dick | Van Dyke | 1925-12-13 00:00:00 Sihung | Lung | 1930-01-01 00:00:00 Ian | Holm | 1931-09-12 00:00:00 Rebecca | Pan | 1931-12-29 00:00:00","title":"Where with OR"},{"location":"sql/tables/where/#where-with-and","text":"select * from directors where date_of_birth > '1970-01-01' AND nationality = 'British'; director_id | first_name | last_name | date_of_birth | nationality -------------+------------+-----------+---------------+------------- 4 | Richard | Ayoade | 1977-06-12 | British 18 | Martin | McDonagh | 1970-03-26 | British","title":"Where with AND"},{"location":"sql/tables/where/#where-with-between-not-between","text":"select * from directors where date_of_birth between '1970-01-01' and '1990-01-01'; director_id | first_name | last_name | date_of_birth | nationality -------------+------------+--------------------------+---------------+------------- 2 | Paul | Anderson | 1970-06-26 | American 4 | Richard | Ayoade | 1977-06-12 | British 11 | Florian | Henckel von Donnersmarck | 1973-05-02 | German 18 | Martin | McDonagh | 1970-03-26 | British","title":"Where with BETWEEN (NOT BETWEEN)"},{"location":"sql/tables/where/#where-with-like","text":"SELECT first_name, last_name FROM actors WHERE actors.last_name LIKE '%son' ORDER BY first_name; first_name | last_name ------------+------------- Jack | Nicholson Lina | Leandersson Luke | Wilson Mykelti | Williamson Owen | Wilson Patrick | Wilson Samuel | Jackson Woody | Harrelson","title":"Where with LIKE"},{"location":"sql/tables/where/#where-with-in-and-not-in","text":"select * from directors where nationality in ('American', 'Japenese'); director_id | first_name | last_name | date_of_birth | nationality -------------+------------+--------------------------+---------------+-------------- 1 | Tomas | Alfredson | 1965-04-01 | Swedish 2 | Paul | Anderson | 1970-06-26 | American 3 | Wes | Anderson | 1969-05-01 | American 4 | Richard | Ayoade | 1977-06-12 | British 5 | Luc | Besson | 1959-03-18 | French 6 | James | Cameron | 1954-08-16 | American 7 | Guillermo | del Toro | 1964-10-09 | Mexican select * from directors where nationality not in ('American', 'Japenese'); director_id | first_name | last_name | date_of_birth | nationality -------------+------------+--------------------------+---------------+-------------- 1 | Tomas | Alfredson | 1965-04-01 | Swedish 4 | Richard | Ayoade | 1977-06-12 | British 5 | Luc | Besson | 1959-03-18 | French 7 | Guillermo | del Toro | 1964-10-09 | Mexican 10 | Kinji | Fukasaku | 1930-07-03 | Japanese 11 | Florian | Henckel von Donnersmarck | 1973-05-02 | German 12 | Terry | Jones | 1942-02-01 | British","title":"Where with IN and NOT IN"},{"location":"sql/working-with-database/","text":"Database Creating a database Dropping Database Commands create database testdb; drop database if exists testdb; Connect to DB -- using psql \\c test See list of tables -- using psql \\d #to see tables \\d <name of table> #to see details about table","title":"Database"},{"location":"sql/working-with-database/#database","text":"","title":"Database"},{"location":"sql/working-with-database/#creating-a-database","text":"","title":"Creating a database"},{"location":"sql/working-with-database/#dropping-database","text":"","title":"Dropping Database"},{"location":"sql/working-with-database/#commands","text":"create database testdb; drop database if exists testdb;","title":"Commands"},{"location":"sql/working-with-database/#connect-to-db","text":"-- using psql \\c test","title":"Connect to DB"},{"location":"sql/working-with-database/#see-list-of-tables","text":"-- using psql \\d #to see tables \\d <name of table> #to see details about table","title":"See list of tables"},{"location":"sql/working-with-database/schema/","text":"Schema PostgreSQL schema's should be unique and different from each other Allows you to organise database objects Schema allow multiple users to interact with one database without interfering with each other Allow access and limit database objects to be accessed by the user. -- syntax to create new schema CREATE SCHEMA sales; -- syntax to create new schema CREATE SCHEMA hr; -- syntax to rename existing schema ALTER SCHEMA sales RENAME TO marketing; -- drop schema, DO THIS CAREFULLY !! DROP SCHEMA hr; -- specify the schema for the table explicitly select * from hr.public.jobs; -- creating a sample table CREATE TABLE temporders ( id SERIAL PRIMARY KEY ); -- moving the table from one schema to another ALTER TABLE public.temporders SET SCHEMA marketing; -- show current schema select current_schema(); -- get default search path where the query will start looking show search_path; search_path ----------------- \"$user\", public -- order to search path is important SET search_path to '$user', marketing, public; PG_CATALOG PostgreSQL stores the metadata information about the database and cluster in the schema pg_catalog . This information is partially used by PostgreSQL itself to keep track things itself, but it also presented so external people/processes can understand whats inside the database. pg_catalog schema contains system tables and all the built-in types, functions and operators. pg_catalog is effectively part of the search path. Although if it is not named explicitly in the path then it is implicitly searched before searching the path's schema. select * from information_schema.schemata; catalog_name | schema_name | schema_owner --------------+--------------------+-------------- learning | information_schema | postgres learning | public | postgres learning | pg_catalog | postgres learning | pg_toast | postgres","title":"Schema"},{"location":"sql/working-with-database/schema/#schema","text":"PostgreSQL schema's should be unique and different from each other Allows you to organise database objects Schema allow multiple users to interact with one database without interfering with each other Allow access and limit database objects to be accessed by the user. -- syntax to create new schema CREATE SCHEMA sales; -- syntax to create new schema CREATE SCHEMA hr; -- syntax to rename existing schema ALTER SCHEMA sales RENAME TO marketing; -- drop schema, DO THIS CAREFULLY !! DROP SCHEMA hr; -- specify the schema for the table explicitly select * from hr.public.jobs; -- creating a sample table CREATE TABLE temporders ( id SERIAL PRIMARY KEY ); -- moving the table from one schema to another ALTER TABLE public.temporders SET SCHEMA marketing; -- show current schema select current_schema(); -- get default search path where the query will start looking show search_path; search_path ----------------- \"$user\", public -- order to search path is important SET search_path to '$user', marketing, public;","title":"Schema"},{"location":"sql/working-with-database/schema/#pg_catalog","text":"PostgreSQL stores the metadata information about the database and cluster in the schema pg_catalog . This information is partially used by PostgreSQL itself to keep track things itself, but it also presented so external people/processes can understand whats inside the database. pg_catalog schema contains system tables and all the built-in types, functions and operators. pg_catalog is effectively part of the search path. Although if it is not named explicitly in the path then it is implicitly searched before searching the path's schema. select * from information_schema.schemata; catalog_name | schema_name | schema_owner --------------+--------------------+-------------- learning | information_schema | postgres learning | public | postgres learning | pg_catalog | postgres learning | pg_toast | postgres","title":"PG_CATALOG"}]}